[
  {
    "objectID": "portfolio/fleayi-adult.html",
    "href": "portfolio/fleayi-adult.html",
    "title": "Amphibian disease resistance",
    "section": "",
    "text": "Matthijs Hollanders et al. conducted a four-year mark-recapture study on Fleay’s barred frogs (Mixophyes fleayi) across three sites in northern New South Wales, Australia, to assess their current susceptibility to the amphibian disease chytridiomycosis.\nThe research was recently published as an open access article in Ecological Applications, where they argue that this species shows hallmarks of the evolution of pathogen resistance. Using disease-structured multistate models incorporating individual pathogen loads measured during recapture events, they show that although some individuals likely succumb to the disease, overall mortality was low and frogs were much more likely to clear their infections than to gain them. Good news!\nThis research was featured in The Conversation and ABC News."
  },
  {
    "objectID": "portfolio/dmso.html",
    "href": "portfolio/dmso.html",
    "title": "Controlling for DMSO",
    "section": "",
    "text": "Kate Summer et al. recently published an open access manuscript in Biofilm, for which Quantecol conducted the statistical analysis.\nThe authors make a strong case for controlling for the effects of the solvent dimethyl-sulfoxide (DMSO) in biofilm studies. In addition to a literature review of 76 published studies, Summer conducted experiments on the bacteria Streptococcus pneumoniae and Pseudomonas aeruginosa to determine to what extent DMSO affected biofilm formation. We analysed the microbial activity and biofilm inhibition in these experiments using hormetic dose-response models."
  },
  {
    "objectID": "blog/hmm-in-stan.html",
    "href": "blog/hmm-in-stan.html",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "",
    "text": "The development of Bayesian statistical software had considerable influence on statistical ecology because many ecological models are complex and require custom likelihoods that were not necessarily available in off-the-shelf software such as R or Program Mark (Cooch and White 2008, Kéry and Royle 2015, Kéry and Royle 2020). One of the reasons why ecological models are so complex is because the data are often messy. For instance, in something like a randomised control trial, careful design means you can make stronger assumptions such as that measurements are obtained without error. However, ecological data are noisy and we frequently require complicated observation models that account for imperfect detection of species or individuals. As a result, a large class of ecological models can be formulated as hidden Markov models (HMMs), where time-series data are modeled with a (partially or completely) unobserved ecological model and an observation model conditioned on it. Examples of ecological HMMs include mark-recapture, occupancy, and \\(N\\)-mixture models, including their more complex multistate variants.\nHMMs are straightforward to model with non-gradient based Markov chain Monte Carlo (MCMC) methods because the latent ecological states can be sampled like parameters. Formulating these models became accessible to ecologists because of the simplicity afforded by statistical software like BUGS, JAGS, and NIMBLE (de Valpine et al. 2017). Consider the simple mark-recapture model where individuals \\(i \\in 1 : I\\) first captured at time \\(t = f_i\\) survive between times \\(t \\in f_i: T\\) with survival probability \\(\\phi\\) and are recaptured on each occasion with detection probability \\(p\\). The state-space formulation of this model from \\(t_{(f_i + 1):T}\\) with vague priors for \\(\\phi\\) and \\(p\\) is as follows:\n\\[\n\\begin{aligned}\n  z_{i,t} &\\sim \\textrm{Bernoulli} \\left( z_{i,t-1} \\times \\phi \\right) \\\\\n  y_{i,t} &\\sim \\textrm{Bernoulli} \\left( z_{i,t} \\times p \\right) \\\\\n  \\phi, p &\\sim \\textrm{Beta} \\left( 1, 1 \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(z_{i,t}\\) are the partially observed ecological states (where \\(z=1\\) is an alive individual and \\(z=0\\) is a dead individual) and \\(y_{i,t}\\) are the observed data (where \\(y=1\\) is an observed individual and \\(y=0\\) is an unobserved individual). Using NIMBLE 1.1.0 in R 4.3.2 (R Core Team 2023), coding this model is straightforward and follows the algebra above closely.\n\n\nCode\nlibrary(nimble)\ncmr_code &lt;- nimbleCode({\n  # likelihood\n  for (i in 1:n_ind) {\n    # initial state is known\n    z[i, first[i]] &lt;- y[i, first[i]]\n    # subsequent surveys\n    for (t in (first[i] + 1):n_surv) {\n      z[i, t] ~ dbern(z[i, t - 1] * phi)\n      y[i, t] ~ dbern(z[i, t] * p)\n    } # t\n  } # i\n  # priors\n  phi ~ dbeta(1, 1)\n  p ~ dbeta(1, 1)\n})\n\n\nAfter simulating some data, creating a NIMBLE model from the code, and configuring the MCMC, we can see that each unknown z[i, t] gets its own binary MCMC sampler.\n\n\nCode\n# metadata\nn_ind &lt;- 100\nn_surv &lt;- 8\n\n# parameters\nphi &lt;- 0.6\np &lt;- 0.7\n\n# containers\nz &lt;- y &lt;- z_known &lt;- matrix(NA, n_ind, n_surv)\nfirst &lt;- sample(1:(n_surv - 1), n_ind, replace = T) |&gt; sort()\nlast &lt;- numeric(n_ind)\n\n# simulation\nfor (i in 1:n_ind) {\n  z[i, first[i]] &lt;- y[i, first[i]] &lt;- 1\n  for (t in (first[i] + 1):n_surv) {\n    z[i, t] &lt;- rbinom(1, 1, z[i, t - 1] * phi)\n    y[i, t] &lt;- rbinom(1, 1, z[i, t] * p)\n  } # t\n  last[i] &lt;- max(which(y[i, ] == 1))\n  z_known[i, first[i]:last[i]] &lt;- 1\n} # i\n\n# create NIMBLE model object\nRmodel &lt;- nimbleModel(cmr_code, \n                      constants = list(n_ind = n_ind, n_surv = n_surv, first = first),\n                      data = list(y = y, z = z_known))\n\n# configure MCMC\nconf &lt;- configureMCMC(Rmodel)\n\n\n===== Monitors =====\nthin = 1: p, phi\n===== Samplers =====\nRW sampler (2)\n  - phi\n  - p\nbinary sampler (273)\n  - z[]  (273 elements)\n\n\nNote that because we know with certainty that individuals were alive between the first and last survey they were captured, we can actually ease computation by supplying those z[i, first[i]:last[i]] as known. We’ll run this model to confirm that it’s able to recover our input parameters using MCMCvis (Youngflesh 2018).\n\n\nCode\n# compile and build MCMC\nCmodel &lt;- compileNimble(Rmodel)\nCmcmc &lt;- buildMCMC(conf) |&gt; compileNimble(project = Cmodel, resetFunctions = T)\n\n# fit model\nn_iter &lt;- 1e3 ; n_chains &lt;- 4\nfit_cmr_nimble &lt;- runMCMC(Cmcmc, niter = n_iter * 2, nburnin = n_iter, nchains = n_chains)\n\n\n\n\nCode\n# summarise\nlibrary(MCMCvis)\nsummary_cmr_nimble &lt;- MCMCsummary(fit_cmr_nimble, params = c(\"phi\", \"p\")) |&gt;\n  as_tibble(rownames = \"variable\") |&gt;\n  mutate(truth = c(phi, p))\nsummary_cmr_nimble\n\n\n# A tibble: 2 × 9\n  variable  mean     sd `2.5%` `50%` `97.5%`  Rhat n.eff truth\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 phi      0.590 0.0442  0.503 0.592   0.673  1.01   426   0.6\n2 p        0.680 0.0660  0.547 0.681   0.799  1.02   306   0.7\n\n\nAs flexible as Bayesian software like BUGS and JAGS are, most of them do not use Hamiltonian Monte Carlo (HMC, Neal 1994) to generate samples from the posterior distribution, but see nimbleHMC for a recent implementation in NIMBLE. Without going into detail myself (but see Betancourt (2018)), HMC is generally considered a superior algorithm and moreover gives warnings when something goes wrong in the sampling, alerting the user to potential issues in the estimation. As a result, HMC should generally be preferred by practitioners but it has one trait that can be challenging to ecologists in particular: HMC requires that all parameters are continuous, meaning that our typical state-space formulations of HMMs cannot be fit with HMC due to the presence of latent discrete parameters like our ecological states z[i, t] above. The remainder of this post will focus on dealing with fitting complex ecological models using Stan (Carpenter et al. 2017), a probabilistic programming language which implements a state-of-the-art No U-Turn Sampler (NUTS, Hoﬀman and Gelman 2014). I present a unified approach that is applicable to simple and complex models alike, highlighted with three examples of increasing complexity."
  },
  {
    "objectID": "blog/hmm-in-stan.html#introduction",
    "href": "blog/hmm-in-stan.html#introduction",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "",
    "text": "The development of Bayesian statistical software had considerable influence on statistical ecology because many ecological models are complex and require custom likelihoods that were not necessarily available in off-the-shelf software such as R or Program Mark (Cooch and White 2008, Kéry and Royle 2015, Kéry and Royle 2020). One of the reasons why ecological models are so complex is because the data are often messy. For instance, in something like a randomised control trial, careful design means you can make stronger assumptions such as that measurements are obtained without error. However, ecological data are noisy and we frequently require complicated observation models that account for imperfect detection of species or individuals. As a result, a large class of ecological models can be formulated as hidden Markov models (HMMs), where time-series data are modeled with a (partially or completely) unobserved ecological model and an observation model conditioned on it. Examples of ecological HMMs include mark-recapture, occupancy, and \\(N\\)-mixture models, including their more complex multistate variants.\nHMMs are straightforward to model with non-gradient based Markov chain Monte Carlo (MCMC) methods because the latent ecological states can be sampled like parameters. Formulating these models became accessible to ecologists because of the simplicity afforded by statistical software like BUGS, JAGS, and NIMBLE (de Valpine et al. 2017). Consider the simple mark-recapture model where individuals \\(i \\in 1 : I\\) first captured at time \\(t = f_i\\) survive between times \\(t \\in f_i: T\\) with survival probability \\(\\phi\\) and are recaptured on each occasion with detection probability \\(p\\). The state-space formulation of this model from \\(t_{(f_i + 1):T}\\) with vague priors for \\(\\phi\\) and \\(p\\) is as follows:\n\\[\n\\begin{aligned}\n  z_{i,t} &\\sim \\textrm{Bernoulli} \\left( z_{i,t-1} \\times \\phi \\right) \\\\\n  y_{i,t} &\\sim \\textrm{Bernoulli} \\left( z_{i,t} \\times p \\right) \\\\\n  \\phi, p &\\sim \\textrm{Beta} \\left( 1, 1 \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(z_{i,t}\\) are the partially observed ecological states (where \\(z=1\\) is an alive individual and \\(z=0\\) is a dead individual) and \\(y_{i,t}\\) are the observed data (where \\(y=1\\) is an observed individual and \\(y=0\\) is an unobserved individual). Using NIMBLE 1.1.0 in R 4.3.2 (R Core Team 2023), coding this model is straightforward and follows the algebra above closely.\n\n\nCode\nlibrary(nimble)\ncmr_code &lt;- nimbleCode({\n  # likelihood\n  for (i in 1:n_ind) {\n    # initial state is known\n    z[i, first[i]] &lt;- y[i, first[i]]\n    # subsequent surveys\n    for (t in (first[i] + 1):n_surv) {\n      z[i, t] ~ dbern(z[i, t - 1] * phi)\n      y[i, t] ~ dbern(z[i, t] * p)\n    } # t\n  } # i\n  # priors\n  phi ~ dbeta(1, 1)\n  p ~ dbeta(1, 1)\n})\n\n\nAfter simulating some data, creating a NIMBLE model from the code, and configuring the MCMC, we can see that each unknown z[i, t] gets its own binary MCMC sampler.\n\n\nCode\n# metadata\nn_ind &lt;- 100\nn_surv &lt;- 8\n\n# parameters\nphi &lt;- 0.6\np &lt;- 0.7\n\n# containers\nz &lt;- y &lt;- z_known &lt;- matrix(NA, n_ind, n_surv)\nfirst &lt;- sample(1:(n_surv - 1), n_ind, replace = T) |&gt; sort()\nlast &lt;- numeric(n_ind)\n\n# simulation\nfor (i in 1:n_ind) {\n  z[i, first[i]] &lt;- y[i, first[i]] &lt;- 1\n  for (t in (first[i] + 1):n_surv) {\n    z[i, t] &lt;- rbinom(1, 1, z[i, t - 1] * phi)\n    y[i, t] &lt;- rbinom(1, 1, z[i, t] * p)\n  } # t\n  last[i] &lt;- max(which(y[i, ] == 1))\n  z_known[i, first[i]:last[i]] &lt;- 1\n} # i\n\n# create NIMBLE model object\nRmodel &lt;- nimbleModel(cmr_code, \n                      constants = list(n_ind = n_ind, n_surv = n_surv, first = first),\n                      data = list(y = y, z = z_known))\n\n# configure MCMC\nconf &lt;- configureMCMC(Rmodel)\n\n\n===== Monitors =====\nthin = 1: p, phi\n===== Samplers =====\nRW sampler (2)\n  - phi\n  - p\nbinary sampler (273)\n  - z[]  (273 elements)\n\n\nNote that because we know with certainty that individuals were alive between the first and last survey they were captured, we can actually ease computation by supplying those z[i, first[i]:last[i]] as known. We’ll run this model to confirm that it’s able to recover our input parameters using MCMCvis (Youngflesh 2018).\n\n\nCode\n# compile and build MCMC\nCmodel &lt;- compileNimble(Rmodel)\nCmcmc &lt;- buildMCMC(conf) |&gt; compileNimble(project = Cmodel, resetFunctions = T)\n\n# fit model\nn_iter &lt;- 1e3 ; n_chains &lt;- 4\nfit_cmr_nimble &lt;- runMCMC(Cmcmc, niter = n_iter * 2, nburnin = n_iter, nchains = n_chains)\n\n\n\n\nCode\n# summarise\nlibrary(MCMCvis)\nsummary_cmr_nimble &lt;- MCMCsummary(fit_cmr_nimble, params = c(\"phi\", \"p\")) |&gt;\n  as_tibble(rownames = \"variable\") |&gt;\n  mutate(truth = c(phi, p))\nsummary_cmr_nimble\n\n\n# A tibble: 2 × 9\n  variable  mean     sd `2.5%` `50%` `97.5%`  Rhat n.eff truth\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 phi      0.590 0.0442  0.503 0.592   0.673  1.01   426   0.6\n2 p        0.680 0.0660  0.547 0.681   0.799  1.02   306   0.7\n\n\nAs flexible as Bayesian software like BUGS and JAGS are, most of them do not use Hamiltonian Monte Carlo (HMC, Neal 1994) to generate samples from the posterior distribution, but see nimbleHMC for a recent implementation in NIMBLE. Without going into detail myself (but see Betancourt (2018)), HMC is generally considered a superior algorithm and moreover gives warnings when something goes wrong in the sampling, alerting the user to potential issues in the estimation. As a result, HMC should generally be preferred by practitioners but it has one trait that can be challenging to ecologists in particular: HMC requires that all parameters are continuous, meaning that our typical state-space formulations of HMMs cannot be fit with HMC due to the presence of latent discrete parameters like our ecological states z[i, t] above. The remainder of this post will focus on dealing with fitting complex ecological models using Stan (Carpenter et al. 2017), a probabilistic programming language which implements a state-of-the-art No U-Turn Sampler (NUTS, Hoﬀman and Gelman 2014). I present a unified approach that is applicable to simple and complex models alike, highlighted with three examples of increasing complexity."
  },
  {
    "objectID": "blog/hmm-in-stan.html#marginalisation",
    "href": "blog/hmm-in-stan.html#marginalisation",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Marginalisation",
    "text": "Marginalisation\nSince HMC cannot sample discrete parameters, we have to reformulate our models without the latent states through a process called marginalisation. Marginalisation essentially just counts the mutually exclusive ways an observation can be made and sums these probabilities, as \\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B)\\). For instance, in the above mark-recapture example, consider an individual was captured on occasion \\(t_1\\) and recaptured on occasion \\(t_2\\). The only way this could have happened is that the individual survived the interval \\(t_{1:2}\\) with probability \\(\\phi\\) and was recaptured on occasion \\(t_2\\) with probability \\(p\\), making the marginal likelihood of that datapoint \\(\\phi \\times p\\). However, if it was not observed at time \\(t_{2}\\), two things are possible: either the animal survived but was not recaptured with probability \\(\\phi \\times (1 - p)\\) or the individual died with probability \\(1 - \\phi\\) and was thus not recaptured. Summing these probabilities gives us the marginal likelihood for that datapoint, which is \\(\\phi \\times (1 - p) + (1 - \\phi)\\). Models can be marginalised in many different ways, but in this post I’m going to focus on the forward algorithm. Although it takes some getting used to, the forward algorithm facilitates fitting a wide variety of (increasingly complex) HMMs.\n\nForward algorithm\nFirst, I’m going to restructure our mark-recapture model with the introduction of transition probability matrices (TPMs). Formulating ecological models this way means we can apply the same strategy for a wide variety of models. In the mark-recapture example, we still have two latent ecological states: (1) dead and (2) alive. The transitions from one state to the other is given by the following TPM, where the states of departure (time \\(t - 1\\)) are in the rows and states of arrival (time \\(t\\)) are in the columns (note that the rows of TPMs must sum to 1):\n\\[\n\\mathrm{TPM}_z = \\begin{bmatrix}\n  1 & 0 \\\\\n  1 - \\phi & \\phi          \n\\end{bmatrix}\n\\tag{2}\\]\nHere, the dead state is an absorbing state and individuals remain in the alive states with probability \\(\\phi\\). The observation process still deals with two observed states, (1) not detected and (2) detected, and is also formulated as a \\(2 \\times 2\\) TPM, where the ecological states are in the rows (states of departure) and the observed states in the columns (states of arrival):\n\\[\n\\mathrm{TPM}_y = \\begin{bmatrix}\n  1 & 0 \\\\\n  1 - p & p\n\\end{bmatrix}\n\\tag{3}\\]\nThe forward algorithm works by computing the marginal likelihood of an individual being in each state \\(s \\in 1 : S\\) at time \\(t\\) given the observations up to time \\(t\\), stored in a \\(T \\times S\\) matrix we’ll call \\(\\mathrm{MPS}\\) (Marginal Probability per State). From here on, the notation for subsetting row \\(i\\) of a matrix \\(M\\) is \\(M_i\\) and \\(M_{:j}\\) for subsetting column \\(j\\). The forward algorithm starts from initial state probabilities, which in the mark-recapture example are known, given that we know with certainty that every individual is alive at first capture:\n\\[\n\\mathrm{MPS}_{f_i} = \\left[ 0, 1 \\right]\n\\]\nFor subsequent occasions after an individual’s first capture \\(t_{(f_i+1):T}\\), the marginal probabilities of being in state \\(s\\) up to time \\(t\\) given the observations that came before \\(t\\) are computed as:\n\\[\n\\mathrm{MPS}_{t} = \\mathrm{MPS}_{t-1} \\times \\mathrm{TPM}_z \\odot {\\mathrm{TPM}_y}_{[:y_{i, t}]}^\\intercal\n\\]\nThat is, matrix multiplying the marginal probabilities at time \\(t-1\\) with the ecological TPM and then element-wise multiplying (where \\(\\odot\\) denotes the Hadamard or element-wise product) the resulting row-vector with the transposed relevant column of the observation TPM, where the first column is used if the individual was not observed (\\(y_{i,t} = 1\\)) and the second column if it was observed (\\(y_{i,t} = 2\\)). The forward algorithm continues until survey \\(T\\) after which the log density is incremented with the log of the sum of \\(\\mathrm{MPS}_T\\), the probabilities of being in each state at the end of the study period."
  },
  {
    "objectID": "blog/hmm-in-stan.html#example-1-basic-mark-recapture-as-a-hmm",
    "href": "blog/hmm-in-stan.html#example-1-basic-mark-recapture-as-a-hmm",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Example 1: Basic mark-recapture as a HMM",
    "text": "Example 1: Basic mark-recapture as a HMM\nAs a first example of structuring ecological models as HMMs with matrix multiplication, I have translated the initial mark-recapture model for use in Stan. I wrote two versions, one with the probabilities which most closely resembles the above algebra and one with log probabilities, which is what Stan naturally works with. Doing so involves swapping the multiplication of probabilities with summing the log probabilities, changing log(sum()) to log_sum_exp(), and performing matrix multiplication on the log scale with a custom function log_product_exp(), which I found on Stack Overflow (note that the function is overloaded to account for the dimensions of your two matrices). I have a few comments:\n\nFor computational efficiency with column-major ordering in Stan (also true for R, BUGS, and NIMBLE, for that matter), the above linear algebra operations are happening in reverse with a transposed \\(\\mathrm{TPM}_z\\) and \\(\\mathrm{MPS}\\). The ecological TPM is still constructed as above but transposed afterwards.\nI am getting into the habit of initialising \\(\\mathrm{MPS}_t\\) with the probabilities associated with the observation process (that is, the lowest hierarchical level of the HMM), as this is the only part of the model that is directly conditioned on the observed data. These marginal detection probabilities can then be incremented with the ecological process, where the .*= operator in Stan element-wise multiplies mps[:, t] with the expression on the right of the operator. Structuring even the simplest model this way makes understanding the more complicated ones coming up more manageable.\nJust like in the NIMBLE version, we don’t have to worry about the probabilities associated with being dead between the first and last capture. Note that for the occasions between first and last capture, I only compute the marginal probabilities of the alive state, fixing those of the dead state to 0 (or negative_infinity() for log).\nEven though these models don’t have latent ecological states as discrete parameters, they can still be derived using the Viterbi algorithm. This is performed in the generated quantities block, and for details I recommend studying the Stan User’s Guide. Note that in the simple mark-recapture example we already know that individuals were alive in state 2 between the first and last occasion of capture, so the Viterbi algorithm is only implemented from (last[i] + 1):n_surv.\nIn all models, I also compute the log likelihood in the vector log_lik for potential use in model checking using the loo package (Vehtari et al. 2023).\nMost parts of the model block are repeated in generated quantities. Although this makes the whole program more verbose, it is more efficient to compute derived quantities in the generated quantities block as they are only computed once per HMC iteration, whereas the transformed parameters and the model blocks get updated with each gradient evaluation, of which there can be many per iteration.\n\n\nStan programs\n\nProbabilitiesLog Probabilities\n\n\n\n\nCode\ndata {\n  int&lt;lower=1&gt; n_ind, n_surv;\n  array[n_ind] int&lt;lower=1, upper=n_surv-1&gt; first;\n  array[n_ind] int&lt;lower=first, upper=n_surv&gt; last;\n  array[n_ind, n_surv] int&lt;lower=1, upper=2&gt; y;\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; phi, p;\n}\n\nmodel {\n  // TPM containers\n  matrix[2, 2] tpm_z, tpm_y;\n  \n  // ecological TPM (transpose once)\n  tpm_z[1, 1] = 1;\n  tpm_z[1, 2] = 0;\n  tpm_z[2, 1] = 1 - phi;\n  tpm_z[2, 2] = phi;\n  tpm_z = tpm_z';\n  \n  // observation TPM\n  tpm_y[1, 1] = 1;\n  tpm_y[1, 2] = 0;\n  tpm_y[2, 1] = 1 - p;\n  tpm_y[2, 2] = p;\n  \n  // marginal probabilities per state\n  matrix[2, n_surv] mps;\n  \n  // likelihood\n  for (i in 1:n_ind) {\n    \n    // initial state probabilities\n    mps[:, first[i]] = [ 0, 1 ]';\n    \n    // from first to last capture, only alive states conditioned on data\n    if (first[i] &lt; last[i]) {\n      for (t in (first[i] + 1):last[i]) {\n        \n        // initialise marginal probability with observation process\n        mps[2, t] = tpm_y[2, y[i, t]];\n        \n        // increment with ecological process\n        mps[2, t] *= tpm_z[2, 2] * mps[2, t - 1];\n      } // t\n    }\n    \n    // ensure dead state at last capture is impossible\n    mps[1, last[i]] = 0;\n    \n    // if individual was not captured on last survey\n    if (last[i] &lt; n_surv) {\n      \n      // after last capture, condition both states on data\n      for (t in (last[i] + 1):n_surv) {\n        \n        // initialise marginal probabilities with observation process\n        mps[:, t] = tpm_y[:, y[i, t]];\n        \n        // increment with ecological process (matrix multiplication)\n        mps[:, t] .*= tpm_z * mps[:, t - 1];\n      } // t\n    }\n    \n    // increment log density\n    target += log(sum(mps[:, n_surv]));\n    \n  } // i\n  \n  // priors\n  target += beta_lupdf(phi | 1, 1);\n  target += beta_lupdf(p | 1, 1);\n}\n\ngenerated quantities {\n  array[n_ind, n_surv] int z;\n  vector[n_ind] log_lik;\n  {\n    matrix[2, 2] tpm_z, tpm_y;\n    tpm_z[1, 1] = 1;\n    tpm_z[1, 2] = 0;\n    tpm_z[2, 1] = 1 - phi;\n    tpm_z[2, 2] = phi;\n    tpm_z = tpm_z';\n    tpm_y[1, 1] = 1;\n    tpm_y[1, 2] = 0;\n    tpm_y[2, 1] = 1 - p;\n    tpm_y[2, 2] = p;\n    array[2, n_surv] int back_ptr;\n    matrix[2, n_surv] mps, best_p;\n    real tmp;\n    int tt;\n    \n    for (i in 1:n_ind) {\n    \n      // log-lik\n      mps[:, first[i]] = [ 0, 1 ]';\n      if (first[i] &lt; last[i]) {\n        for (t in (first[i] + 1):last[i]) {\n          mps[2, t] = tpm_y[2, y[i, t]];\n          mps[2, t] *= tpm_z[2, 2] * mps[2, t - 1];\n        } // t\n      }\n      mps[1, last[i]] = 0;\n      \n      // Viterbi\n      best_p[:, last[i]] = [ 0, 1 ]';\n      if (last[i] &lt; n_surv) {\n        best_p[:, (last[i] + 1):n_surv] = rep_matrix(0, 2, n_surv - last[i]);\n        for (t in (last[i] + 1):n_surv) {\n          // marginal observation probabilities\n          mps[:, t] = tpm_y[:, y[i, t]];\n          for (s_a in 1:2) {  // state of arrival\n            for (s_d in 1:2) { // state of departure\n              tmp = best_p[s_d, t - 1] * tpm_z[s_a, s_d] * mps[s_a, t];\n              if (tmp &gt; best_p[s_a, t]) {\n                back_ptr[s_a, t] = s_d;\n                best_p[s_a, t] = tmp;\n              }\n            } // s_d\n          } // s_a\n          \n          // increment with ecological process for log-lik\n          mps[:, t] .*= tpm_z * mps[:, t - 1];\n        } // t\n      }\n      log_lik[i] = log(sum(mps[:, n_surv]));\n      \n      // ecological states\n      z[i, first[i]:last[i]] = rep_array(2, last[i] - first[i] + 1);\n      if (last[i] &lt; n_surv) {\n        tmp = max(best_p[:, n_surv]);\n        for (s_a in 1:2) {\n          if (best_p[s_a, n_surv] == tmp) {\n            z[i, n_surv] = s_a;\n          }\n        } // s_a\n        if (last[i] &lt; (n_surv - 1)) {\n          for (t in (last[i] + 1):(n_surv - 1)) {\n            tt = n_surv - t + last[i] + 1;\n            z[i, tt - 1] = back_ptr[z[i, tt], tt];\n          } // t\n        }\n      }\n    } // i\n  }\n}\n\n\n\n\n\n\nCode\nfunctions{\n  /**\n   * Return the natural logarithm of the product of the element-wise exponentiation of the specified matrices\n   *\n   * @param a  First matrix or (row_)vector\n   * @param b  Second matrix or (row_)vector\n   *\n   * @return   log(exp(a) * exp(b))\n   */\n  matrix log_product_exp(matrix a, matrix b) {\n    int x = rows(a);\n    int y = cols(b);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    matrix[x, y] c;\n    for (j in 1:y) {\n      for (i in 1:x) {\n        c[i, j] = log_sum_exp(a_tr[:, i] + b[:, j]);\n      }\n    }\n    return c;\n  }\n  vector log_product_exp(matrix a, vector b) {\n    int x = rows(a);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    vector[x] c;\n    for (i in 1:x) {\n      c[i] = log_sum_exp(a_tr[:, i] + b);\n    }\n    return c;\n  }\n  row_vector log_product_exp(row_vector a, matrix b) {\n    int y = cols(b);\n    vector[size(a)] a_tr = a';\n    row_vector[y] c;\n    for (j in 1:y) {\n      c[j] = log_sum_exp(a_tr + b[:, j]);\n    }\n    return c;\n  }\n  real log_product_exp(row_vector a, vector b) {\n    real c = log_sum_exp(a' + b);\n    return c;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; n_ind, n_surv;\n  array[n_ind] int&lt;lower=1, upper=n_surv-1&gt; first;\n  array[n_ind] int&lt;lower=first, upper=n_surv&gt; last;\n  array[n_ind, n_surv] int&lt;lower=1, upper=2&gt; y;\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; phi, p;\n}\n\nmodel {\n  // log TPM containers\n  matrix[2, 2] ltpm_z, ltpm_y;\n  \n  // ecological ltpm\n  ltpm_z[1, 1] = 0;\n  ltpm_z[1, 2] = negative_infinity();\n  ltpm_z[2, 1] = log1m(phi);\n  ltpm_z[2, 2] = log(phi);\n  ltpm_z = ltpm_z';\n  \n  // observation ltpm\n  ltpm_y[1, 1] = 0;\n  ltpm_y[1, 2] = negative_infinity();\n  ltpm_y[2, 1] = log1m(p);\n  ltpm_y[2, 2] = log(p);\n  \n  // marginal log probabilities per state\n  matrix[2, n_surv] lmps;\n  \n  // likelihood\n  for (i in 1:n_ind) {\n    \n    // initial state log probabilities\n    lmps[:, first[i]] = [ negative_infinity(), 0 ]';\n    \n    // from first to last capture, only alive states conditioned on data\n    if (first[i] &lt; last[i]) {\n      for (t in (first[i] + 1):last[i]) {\n        \n        // initialise marginal log probability with observation process\n        lmps[2, t] = ltpm_y[2, y[i, t]];\n        \n        // increment with ecological process\n        lmps[2, t] += ltpm_z[2, 2] + lmps[2, t - 1];\n      } // t\n    }\n    \n    // ensure dead state at last capture is impossible\n    lmps[1, last[i]] = negative_infinity();\n    \n    // if individual was not captured on last survey\n    if (last[i] &lt; n_surv) {\n      \n      // after last capture, condition both states on data\n      for (t in (last[i] + 1):n_surv) {\n        \n        // initialise marginal log probabilities with observation process\n        lmps[:, t] = ltpm_y[:, y[i, t]];\n        \n        // increment with ecological process (log matrix multiplication)\n        lmps[:, t] += log_product_exp(ltpm_z, lmps[:, t - 1]);\n      } // t\n    }\n    \n    // increment log density\n    target += log_sum_exp(lmps[:, n_surv]);\n    \n  } // i\n  \n  // priors\n  target += beta_lupdf(phi | 1, 1);\n  target += beta_lupdf(p | 1, 1);\n}\n\ngenerated quantities {\n  array[n_ind, n_surv] int z;\n  vector[n_ind] log_lik;\n  {\n    matrix[2, 2] ltpm_z, ltpm_y, trm;\n    ltpm_z[1, 1] = 0;\n    ltpm_z[1, 2] = negative_infinity();\n    ltpm_z[2, 1] = log1m(phi);\n    ltpm_z[2, 2] = log(phi);\n    ltpm_z = ltpm_z';\n    ltpm_y[1, 1] = 0;\n    ltpm_y[1, 2] = negative_infinity();\n    ltpm_y[2, 1] = log1m(p);\n    ltpm_y[2, 2] = log(p);\n    array[2, n_surv] int back_ptr;\n    matrix[2, n_surv] lmps, best_lp;\n    real tmp;\n    int tt;\n    \n    for (i in 1:n_ind) {\n      \n      // log-lik\n      lmps[:, first[i]] = [ negative_infinity(), 0 ]';\n      if (first[i] &lt; last[i]) {\n        for (t in (first[i] + 1):last[i]) {\n          lmps[2, t] = ltpm_y[2, y[i, t]];\n          lmps[2, t] += ltpm_z[2, 2] + lmps[2, t - 1];\n        } // t\n      }\n      lmps[1, last[i]] = negative_infinity();\n      \n      // Viterbi\n      best_lp[:, last[i]] = [ negative_infinity(), 0 ]';\n      if (last[i] &lt; n_surv) {\n        best_lp[:, (last[i] + 1):n_surv] = rep_matrix(negative_infinity(), 2, n_surv - last[i]);\n        for (t in (last[i] + 1):n_surv) {\n          // marginal observation log probabilities\n          lmps[:, t] = ltpm_y[:, y[i, t]];\n          for (s_a in 1:2) {  // state of arrival\n            for (s_d in 1:2) { // state of departure\n              tmp = best_lp[s_d, t - 1] + ltpm_z[s_a, s_d] + lmps[s_a, t];\n              if (tmp &gt; best_lp[s_a, t]) {\n                back_ptr[s_a, t] = s_d;\n                best_lp[s_a, t] = tmp;\n              }\n            } // s_d\n          } // s_a\n          \n          // increment with ecological process for log-lik\n          lmps[:, t] += log_product_exp(ltpm_z, lmps[:, t - 1]);\n        } // t\n      }\n      log_lik[i] = log_sum_exp(lmps[:, n_surv]);\n      \n      // ecological states\n      z[i, first[i]:last[i]] = rep_array(2, last[i] - first[i] + 1);\n      if (last[i] &lt; n_surv) {\n        tmp = max(best_lp[:, n_surv]);\n        for (s_a in 1:2) {\n          if (best_lp[s_a, n_surv] == tmp) {\n            z[i, n_surv] = s_a;\n          }\n        } // s_a\n        if (last[i] &lt; (n_surv - 1)) {\n          for (t in (last[i] + 1):(n_surv - 1)) {\n            tt = n_surv - t + last[i] + 1;\n            z[i, tt - 1] = back_ptr[z[i, tt], tt];\n          } // t\n        }\n      }\n    } // i\n  }\n}\n\n\n\n\n\n\n\nWhy we like HMC\nUsing CmdStanR 0.7.1 to call CmdStan 2.34.1 (Gabry et al. 2023), we’ll just run the log probability model for the same number of iterations as the NIMBLE version, and we see that there were no issues with sampling (Stan would tell us otherwise) and the effective sample sizes (ESS) were several times higher than for the NIMBLE model. This is why we want to use HMC.\n\n\nCode\n# data for Stan\ncmr_data &lt;- list(n_ind = n_ind, n_surv = n_surv, \n                 first = first, last = last, \n                 y = y + 1) |&gt;\n  # no NAs in Stan\n  sapply(\\(x) replace(x, is.na(x), 1))\n\n# run HMC\nfit_cmr &lt;- cmr_lp$sample(data = cmr_data, refresh = 0,\n                         chains = n_chains, parallel_chains = n_chains, iter_warmup = n_iter, iter_sampling = n_iter)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 2.0 seconds.\nChain 2 finished in 2.0 seconds.\nChain 3 finished in 2.0 seconds.\nChain 4 finished in 2.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.0 seconds.\nTotal execution time: 2.3 seconds.\n\n\nCode\n# summary\nlibrary(tidyverse)\nfit_cmr$summary(c(\"phi\", \"p\")) |&gt; \n  select(variable, median, contains(\"q\"), contains(\"ess\")) |&gt;\n  mutate(ess_nimble = summary_cmr_nimble$n.eff,\n         truth = c(phi, p))\n\n\n# A tibble: 2 × 8\n  variable median    q5   q95 ess_bulk ess_tail ess_nimble truth\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 phi       0.589 0.517 0.661    2332.    2443.        426   0.6\n2 p         0.687 0.576 0.787    2197.    2403.        306   0.7\n\n\nAnd just to confirm, we check if the Viterbi algorithm was able to closely recover the latent alive states.\n\n\nCode\n# generate estimated z matrix\nz_est &lt;- fit_cmr$summary(\"z\")$median |&gt;\n  matrix(n_ind, n_surv) |&gt; \n  {\\(z) ifelse(z &lt; 1, NA, ifelse(z == 1, 0, 1))}()\n\n# what proportion of latent z states correspond with the truth?\nn_unknown &lt;- n_surv - last\ncorrect &lt;- numeric(n_ind)\nfor (i in 1:n_ind) {\n  if (n_unknown[i] &gt; 0) {\n    unknowns &lt;- (last[i] + 1):n_surv\n    correct[i] &lt;- sum((z_est == z)[i, unknowns])\n  }\n}\nsum(correct) / sum(n_unknown)\n\n\n[1] 0.875"
  },
  {
    "objectID": "blog/hmm-in-stan.html#example-2-dynamic-multistate-occupancy-model",
    "href": "blog/hmm-in-stan.html#example-2-dynamic-multistate-occupancy-model",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Example 2: Dynamic multistate occupancy model",
    "text": "Example 2: Dynamic multistate occupancy model\nIn order to highlight the generality of the above modeling approach, we’re going to switch gears to a dynamic occupancy model. These models are also just HMMs, with some trivial differences to the basic mark-recapture model that can seem daunting at first:\n\nThere can be more than two states.\nThe initial state probabilities here are unknown, that is, we don’t know the occupancy state of a site when it’s first surveyed.\nState transitions can happen in more than one direction, whereas dead individuals in mark-recapture can never transition to alive.\n\nConsider a study area containing sites \\(i \\in 1 : I\\) surveyed across \\(t \\in 1 : T\\) years. We are investigating whether sites are occupied by tiger snakes (Notechis scutatus) in each year, but want to differentiate between sites that simply support tiger snakes or those that support breeding. Our ecological process thus has three possible states: (1) unoccupied by tiger snakes, (2) occupied by tiger snakes but not for breeding, or (3) occupied by breeding tiger snakes. The observation process also has three possible states: (1) no snakes found, (2) snakes found but no evidence of breeding, and (3) snakes found with evidence of breeding, as determined by the discovery of gravid females. Instead of directly formulating the ecological TPM here, we’ll do it in continuous time instead using a transition rate matrix (TRM).\n\n\n\nA rainforest tiger snake (Notechis scutatus) from northern New South Wales, Australia.\n\n\n\n\n\n\n\n\nEcology occurs in continuous time\n\n\n\nTPMs assume equal intervals between surveys, but this is the exception rather than the norm for ecological data. Although things like survival probabilities can easily be exponentiated to the time interval, this doesn’t work for processes that are not absorbing states, i.e., snakes moving in and out of sites. Unlike individual survival, sites are free to transition between states from year to year. Even with simpler models, however, it often makes more sense to model the survival process with mortality hazard rates (where the survival probability \\(S\\) is related to the mortality hazard rate \\(h\\) as \\(S = \\exp(-h)\\)) for a number of reasons (Ergon et al. 2018). The multistate extension of this is straightforward, where we instead take the matrix exponential of a TRM (Glennie et al. 2022). By constructing our transition matrices as TRMs, where the off-diagonals contain instantaneous hazard rates and the diagonals contain negative sums of these off-diagonals (ensuring rows sum to 0, not 1), we can generate time-specific TPMs by taking the matrix exponential of the TRM multiplied by the time interval between occasions (\\(\\tau_t\\)), thereby essentially marginalising over possible state transitions that occurred during survey intervals:\n\\[\n\\mathrm{TPM}_t = \\exp \\left( \\mathrm{TRM} \\times \\tau_t \\right)\n\\tag{4}\\]\n\n\nA TRM for the tiger snake example might look something like this, with again states of departure (year \\(t-1\\)) in the rows and states of arrival (year \\(t\\)) in the columns:\n\\[\n\\mathrm{TRM}_z = \\begin{bmatrix}\n  -(\\gamma_1 + \\gamma_2) & \\gamma_1 & \\gamma_2 \\\\\n  \\epsilon_1 & -(\\epsilon_1 + \\psi_1) & \\psi_1 \\\\\n  \\epsilon_2 & \\psi_2 & -(\\epsilon_2 + \\psi_2)\n\\end{bmatrix}\n\\tag{5}\\] with the following parameters:\n\n\\(\\gamma_1\\): the “non-breeding” colonisation rate, or the rate at which a site becomes occupied with non-breeding snakes at time \\(t\\) when the site was not occupied at time \\(t-1\\).\n\\(\\gamma_2\\): the “breeding” colonisation rate, or the rate at which a site becomes occupied with breeding snakes at time \\(t\\) when the site was not occupied at time \\(t-1\\).\n\\(\\epsilon_1\\): the “non-breeding” emigration rate, or the rate at which a site becomes onoccupied at time \\(t\\) when it was occupied with non-breeding snakes at time \\(t-1\\).\n\\(\\epsilon_2\\): the “breeding” emigration rate, or the rate at which a site becomes onoccupied at time \\(t\\) when it was occupied with breeding snakes at time \\(t-1\\).\n\\(\\psi_1\\): the breeding rate, or the rate at which a site becomes used for breeding at time \\(t\\) when the site was occupied but not used for breeding at time \\(t-1\\).\n\\(\\psi_2\\): the breeding cessation rate, or the rate at which a site stops being used for breeding at time \\(t\\) when the site was used for breeding at time \\(t-1\\).\n\nNote that even if these parameters aren’t exactly the quantities of interest, derivatives of these parameters are easily derived as generated quantities after parameter estimation. Of course, these parameters can be modeled more complexly with linear or non-linear effects at the level of years, sites, or both, with the only changes being that the TRMs are constructed at those levels (for instance, by creating site- and year-level TRMs where the parameters populating the TRM are allowed to vary at that level—see Example 3 for an application).\nIn addition to the TRM/TPM, the ecological process is initiated on the first occasion with an initial state vector, giving the probabilities of being in each state at \\(t_1\\), which have to sum to 1:\n\\[\n  \\boldsymbol{\\eta} = \\left[ \\eta_1 , \\eta_2 , \\eta_3 \\right]\n\\tag{6}\\]\n\nSimulating continuous time\nBelow, I’ll simulate the ecological data by specifying a TRM and using the expm package to take the matrix exponential to generate the TPM (Goulet et al. 2021). In order to simulate ecological states z[i, t] we use the categorical distribution, which is just the Bernoulli distribution generalised to multiple outcomes. Note that because we’re simulating the ecological process from year to year, the time intervals \\(\\tau\\) are just 1; however, in the Stan program below, we will account for missing years using tau.\n\n\nCode\n# metadata\nn_site &lt;- 100\nn_year &lt;- 10\nn_sec &lt;- 4\n\n# parameters\neta &lt;- c(NA, 0.3, 0.4)       # unoccupied/non-breeding/breeding initial probabilities\neta[1] &lt;- 1 - sum(eta[2:3])  # ensure probabilities sum to 1 (simplex) \ngamma &lt;- c(0.3, 0.2)         # non-breeding/breeding colonisation rates\nepsilon &lt;- c(0.4, 0.1)       # non-breeding/breeding emigration rates\npsi &lt;- c(0.4, 0.3)           # breeding/breeding cessation rates\n\n# function for random categorical draw\nrcat &lt;- function(n, prob) {\n  return(\n    rmultinom(n, size = 1, prob) |&gt; \n      apply(2, \\(x) which(x == 1))\n  )\n}\n\n# TRM\ntrm &lt;- matrix(c(-(gamma[1] + gamma[2]), gamma[1], gamma[2],\n                epsilon[1], -(epsilon[1] + psi[1]), psi[1],\n                epsilon[2], psi[2], -(epsilon[2] + psi[2])),\n              3, byrow = T)\ntrm\n\n\n     [,1] [,2] [,3]\n[1,] -0.5  0.3  0.2\n[2,]  0.4 -0.8  0.4\n[3,]  0.1  0.3 -0.4\n\n\nCode\n# here's the TPM\nlibrary(expm)\ntpm_z &lt;- expm(trm)\ntpm_z\n\n\n      [,1]  [,2]  [,3]\n[1,] 0.650 0.182 0.168\n[2,] 0.231 0.515 0.254\n[3,] 0.101 0.182 0.717\n\n\nCode\n# containers\nz &lt;- matrix(NA, n_site, n_year)\n\n# ecological process simulation\nfor (i in 1:n_site) {\n  \n  # initial states\n  z[i, 1] &lt;- rcat(1, eta)\n  \n  # subsequent years\n  for (t in 2:n_year) {\n    z[i, t] &lt;- rcat(1, tpm_z[z[i, t - 1], ])\n  } # t\n} # i\n\n# some ecological states\nhead(z, 10)\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    1    1    1    1    1    1    1    1    1     1\n [2,]    3    3    3    3    3    3    2    2    3     3\n [3,]    1    3    3    2    1    1    1    2    2     2\n [4,]    2    2    2    2    2    3    3    3    3     2\n [5,]    2    3    3    3    3    3    3    3    2     2\n [6,]    2    3    2    3    2    1    3    2    2     1\n [7,]    2    3    3    3    3    3    1    1    1     3\n [8,]    2    2    1    1    1    1    3    2    2     1\n [9,]    2    2    2    2    3    2    2    2    2     1\n[10,]    2    3    3    2    1    1    3    3    2     2\n\n\n\n\nObservation process\nAlthough possible to construct in continuous time, often our observation model makes more sense in discrete time, especially if specific surveys were conducted instead of with something like an automated recording device. For instance, with dynamic occupancy models, an investigator would conduct multiple “secondary” surveys within a season or year in order to disentangle the ecological process from the observation process.\n\n\n\n\n\n\nRobust Design\n\n\n\nWith multistate models where individuals or sites can transition between ecological states, in order to reliably estimate transitions rates and detection probabilities we need to conduct multiple consecutive surveys within periods of assumed closure, yielding secondary surveys nested within primary occasions. In the absence of this, there are parameter identifiability issues for the state transition rates and state-specific detection probabilities. This survey strategy was originally called the “robust design” (Pollock 1982) and the necessity of applying it for multistate models more generally has gone largely unappreciated by ecologists.\n\n\nAs in the mark-recapture example, we construct an observation TPM where the ecological states are in the rows (states of departure) and observed states in the columns (states of arrival), recognising that there’s uncertainty in being able to detect breeding:\n\\[\n\\mathrm{TPM}_y = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  1 - p_1 & p_1 & 0 \\\\\n  1 - p_2 & p_2 \\times (1 - \\delta) & p_2 \\times \\delta\n\\end{bmatrix}\n\\tag{7}\\] with the following parameters:\n\n\\(p_{1:2}\\): the state-specific detection probabilities, or the probabilities of detecting tiger snakes if they are present but not breeding (\\(p_1\\)) or present and breeding (\\(p_2\\)).\n\\(\\delta\\): the breeding detection probability, or the probability of detecting tiger snake breeding conditional on it occurring at a site. Note that in this example, breeding is detected when gravid females are found. Note that false-positive detections of any kind (erroneous snake identification, etc.) are not considered, but will be in Example 3.\n\nI’ll simulate some observational data below where the investigators aimed to conduct 4 secondary surveys per year (primary occasion), all conducted during the tiger snake breeding season, with some secondary surveys missed, and some years missed altogether.\n\n\nCode\n# parameters\np &lt;- c(0.4, 0.5)  # non-breeding/breeding detection probabilities\ndelta &lt;- 0.6      # breeding detection probability\n\n# TPM\ntpm_y &lt;- matrix(c(1, 0, 0,\n                  1 - p[1], p[1], 0,\n                  1 - p[2], p[2] * (1 - delta), p[2] * delta),\n                3, byrow = T)\n\n# containers\nn_year_i &lt;- numeric(n_site)\nyears &lt;- n_sec_it &lt;- matrix(NA, n_site, n_year)\nsecs &lt;- array(NA, c(n_site, n_year, n_sec))\ny &lt;- array(NA, c(n_site, n_year, n_sec))\n\n# simulation\nfor (i in 1:n_site) {\n  \n  # number of years and sequence of years surveyed (always surveyed in first year)\n  n_year_i[i] &lt;- 1 + rbinom(1, n_year - 1, 0.8)\n  years[i, 1:n_year_i[i]] &lt;- c(1, sample(2:n_year, n_year_i[i] - 1) |&gt; sort())\n  \n  for (t in years[i, 1:n_year_i[i]]) {\n    \n    # number of secondaries per site and year and sequence of secondaries surveyed\n    n_sec_it[i, t] &lt;- 1 + rbinom(1, n_sec - 1, 0.8)\n    secs[i, t, 1:n_sec_it[i, t]] &lt;- sample(1:n_sec, n_sec_it[i, t]) |&gt; sort()\n    \n    # observation process\n    y[i, t, secs[i, t, 1:n_sec_it[i, t]]] &lt;- rcat(n_sec_it[i, t], tpm_y[z[i, t], ])\n  } # t\n} # i\n\n\nI recognise that some of the indexing here is daunting but it just makes sure that our data is contained in the right parts of the detection arrays and that Stan isn’t going to have to deal with NAs in the data, which is less straightfoward than it is in JAGS and NIMBLE.\n\n\n\n\n\n\nIndexing\n\n\n\nIn JAGS and NIMBLE we don’t really have to worry about NAs because MCMC samplers are assigned to unobserved data, but that is inefficient. Things aren’t so simple in Stan and we’re better off just pointing the model to where the observations sit in data arrays. In the above example, n_year_i is an n_site length vector holding the number of surveyed years per site. years is an n_site * n_year matrix with the sequence of years observed, which for n_year_i[i] = 6 might looks like years[i, ] = [1, 3, 4, 6, 7, 8, NA, NA, NA, NA]. The NAs will have to be replaced with some other value by the time they get fed to Stan, but you don’t have to worry about them because they won’t get used anyway. n_sec is the maximum number of secondary surveys, and n_sec_it[i, t] holds the number of secondary surveys conducted per site and year. Just like years, secs holds the sequence of specific secondaries that were surveyed for that site and year. Putting it together, in each value t of years[i, 1:n_year_i[i]], y[i, t, secs[i, t, 1:n_sec_it[i, t]]] selects the observations of a site only for the secondaries in the years that were surveyed. In the Stan program below, this subset of y is used to select the observations to extract the associated probabilities out of the observation log TPM.\n\n\n\n\nStan program\nBelow is the Stan program for the above model written with log probabilities. In many ways it is similar to the mark-recapture example, with the following noteworthy changes:\n\nThe site- and year-specific time intervals tau[i, t] between surveyed years are computed in the transformed data block. Additionally, a logical q is computed indicating whether or not sites were surveyed in each year.\nAll entries in the \\(3 \\times T\\) log MPS lmps are initialised with the observation process. Note that to account for the replicate secondary surveys, the relevant log probabilities of each secondary are subsetted from the observation TPM for each possible ecological state. For example, consider a detection vector of site \\(i\\), year \\(t\\), and secondaries \\(1:K_{i,t}\\) of y[i, t, secs[i, t, 1:n_sec_it[i, t]]] = [2, 3, 1, 2], showing that snakes were only observed during secondary 1, 2, and 4, and that a gravid female was only observed on the second occasion. The detection vector corresponding to ecological state 3 (present and breeding) would be \\(\\mathrm{TPM}_{3,[2, 3, 1, 3]} = \\left[ p_2 \\times (1 - \\delta), p_2 \\times \\delta, p_2 \\times (1 - \\delta), 1 - p_2 \\right]^\\intercal\\). The marginal probability of the observations is the product of these probabilities, or the sum of the log probabilities. Note that the detection vectors corresponding to the other states will be 0, because it’s impossible to observe breeding snakes for both unoccupied sites and sites occupied by non-breeders.\nBecause the TPM varies by site and year due to the unequal time intervals tau[i, t], the log TPM of the ecological process gets overwritten in each site- and year-level loop in the model block. Note that in the generated quantities block a different approach is used, where the same yearly TRM is used but lmps is only incremented with observation log probabilities in years that are surveyed for each site.\nAs in Example 1, the log likelihood and Viterbi algorithm are performed in the generated quantities block, but the Viterbi algorithm has been modified for this specific model.\n\n\n\nCode\nfunctions{\n  /**\n   * Return the natural logarithm of the product of the element-wise exponentiation of the specified matrices\n   *\n   * @param a  First matrix or (row_)vector\n   * @param b  Second matrix or (row_)vector\n   *\n   * @return   log(exp(a) * exp(b))\n   */\n  matrix log_product_exp(matrix a, matrix b) {\n    int x = rows(a);\n    int y = cols(b);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    matrix[x, y] c;\n    for (j in 1:y) {\n      for (i in 1:x) {\n        c[i, j] = log_sum_exp(a_tr[:, i] + b[:, j]);\n      }\n    }\n    return c;\n  }\n  vector log_product_exp(matrix a, vector b) {\n    int x = rows(a);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    vector[x] c;\n    for (i in 1:x) {\n      c[i] = log_sum_exp(a_tr[:, i] + b);\n    }\n    return c;\n  }\n  row_vector log_product_exp(row_vector a, matrix b) {\n    int y = cols(b);\n    vector[size(a)] a_tr = a';\n    row_vector[y] c;\n    for (j in 1:y) {\n      c[j] = log_sum_exp(a_tr + b[:, j]);\n    }\n    return c;\n  }\n  real log_product_exp(row_vector a, vector b) {\n    real c = log_sum_exp(a' + b);\n    return c;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; n_site, n_year, n_sec;\n  array[n_site] int&lt;lower=1, upper=n_year&gt; n_year_i;\n  array[n_site, n_year] int&lt;lower=1, upper=n_year&gt; years;\n  array[n_site, n_year] int&lt;lower=1, upper=n_sec&gt; n_sec_it;\n  array[n_site, n_year, n_sec] int&lt;lower=1, upper=n_sec&gt; secs;\n  array[n_site, n_year, n_sec] int&lt;lower=1, upper=3&gt; y;\n}\n\ntransformed data {\n  array[n_site, n_year - 1] int tau;\n  array[n_site, n_year] int&lt;lower=0, upper=1&gt; q = rep_array(0, n_site, n_year);\n  for (i in 1:n_site) {\n    for (t in 2:n_year_i[i]) {\n      tau[i, t - 1] = years[i, t] - years[i, t - 1];\n    }\n    // surveys in that year?\n    for (t in 1:n_year) {\n      for (t_i in years[i, 1:n_year_i[i]]) {\n        if (t == t_i) {\n          q[i, t] = 1;\n        }\n      } // t_i\n    } // t\n  } // i\n}\n\nparameters {\n  simplex[3] eta;\n  vector&lt;lower=0&gt;[2] gamma, epsilon, psi;\n  vector&lt;lower=0, upper=1&gt;[2] p;\n  real&lt;lower=0, upper=1&gt; delta;\n}\n\nmodel {\n  // TRM and log TPM containers\n  matrix[3, 3] trm, ltpm_z, ltpm_y;\n  \n  // pre-compute log initial state vector\n  vector[3] log_eta = log(eta);\n  \n  // ecological TRM (transpose once)\n  trm[1, 1] = -(gamma[1] + gamma[2]);\n  trm[1, 2] = gamma[1];\n  trm[1, 3] = gamma[2];\n  trm[2, 1] = epsilon[1];\n  trm[2, 2] = -(epsilon[1] + psi[1]);\n  trm[2, 3] = psi[1];\n  trm[3, 1] = epsilon[2];\n  trm[3, 2] = psi[2];\n  trm[3, 3] = -(epsilon[2] + psi[2]);\n  trm = trm';\n  \n  // pre-compute log detection probabilities\n  vector[2] log_p = log(p);\n  vector[2] log1m_p = log1m(p);\n  \n  // observation log TPM\n  ltpm_y[1, 1] = 0;\n  ltpm_y[1, 2] = negative_infinity();\n  ltpm_y[1, 3] = negative_infinity();\n  ltpm_y[2, 1] = log1m_p[1];\n  ltpm_y[2, 2] = log_p[1];\n  ltpm_y[2, 3] = negative_infinity();\n  ltpm_y[3, 1] = log1m_p[2];\n  ltpm_y[3, 2] = log_p[2] + log1m(delta);\n  ltpm_y[3, 3] = log_p[2] + log(delta);\n  \n  // marginal log probabilities per state\n  matrix[3, n_year] lmps;\n  int tt;\n  \n  // likelihood\n  for (i in 1:n_site) {\n    \n    // initialise marginal log probabilities with observation model in each year\n    for (t in 1:n_year_i[i]) {\n      tt = years[i, t];  // map to correct year\n      for (s in 1:3) {\n        lmps[s, t] = sum(ltpm_y[s, y[i, tt, secs[i, tt, 1:n_sec_it[i, tt]]]]);\n      } // s\n    } // t\n    \n    // increment initial state log probabilities\n    lmps[:, 1] += log_eta;\n    \n    // for subsequent years\n    for (t in 2:n_year_i[i]) {\n      \n      // compute log TPM from TRM and tau\n      ltpm_z = log(matrix_exp(trm * tau[i, t - 1]));\n      \n      // increment marginal log detection probabilities with ecological process\n      lmps[:, t] += log_product_exp(ltpm_z, lmps[:, t - 1]);\n    }\n    \n    // increment log density\n    target += log_sum_exp(lmps[:, n_year_i[i]]);\n    \n  } // i\n  \n  // priors\n  target += dirichlet_lupdf(eta | ones_vector(3));\n  target += exponential_lupdf(gamma | 1);\n  target += exponential_lupdf(epsilon | 1);\n  target += exponential_lupdf(psi | 1);\n  target += beta_lupdf(p | 1, 1);\n  target += beta_lupdf(delta | 1, 1);\n}\n\ngenerated quantities {\n  array[n_site, n_year] int z;\n  vector[n_site] log_lik;\n  {\n    matrix[3, 3] trm, ltpm_z, ltpm_y;\n    vector[3] log_eta = log(eta);\n    trm[1, 1] = -(gamma[1] + gamma[2]);\n    trm[1, 2] = gamma[1];\n    trm[1, 3] = gamma[2];\n    trm[2, 1] = epsilon[1];\n    trm[2, 2] = -(epsilon[1] + psi[1]);\n    trm[2, 3] = psi[1];\n    trm[3, 1] = epsilon[2];\n    trm[3, 2] = psi[2];\n    trm[3, 3] = -(epsilon[2] + psi[2]);\n    trm = trm';\n    ltpm_z = matrix_exp(trm); // only one needed\n    vector[2] log_p = log(p);\n    vector[2] log1m_p = log1m(p);\n    ltpm_y[1, 1] = 0;\n    ltpm_y[1, 2] = negative_infinity();\n    ltpm_y[1, 3] = negative_infinity();\n    ltpm_y[2, 1] = log1m_p[1];\n    ltpm_y[2, 2] = log_p[1];\n    ltpm_y[2, 3] = negative_infinity();\n    ltpm_y[3, 1] = log1m_p[2];\n    ltpm_y[3, 2] = log_p[2] + log1m(delta);\n    ltpm_y[3, 3] = log_p[2] + log(delta);\n    array[3, n_year] int back_ptr;\n    matrix[3, n_year] lmps, best_lp;\n    real tmp;\n    int tt;\n    \n    for (i in 1:n_site) {\n      \n      // initialise probabilities\n      lmps = rep_matrix(0, 3, n_year);\n      best_lp = rep_matrix(negative_infinity(), 3, n_year);\n      \n      // first year and observation log probabilities\n      lmps[:, 1] = log_eta;\n      for (t in 1:n_year) {\n        if (q[i, t]) {\n          for (s in 1:3) {\n            lmps[s, t] += sum(ltpm_y[s, y[i, t, secs[i, t, 1:n_sec_it[i, t]]]]);\n          } // s\n        }\n      } // t\n      \n      // Viterbi\n      best_lp[:, 1] = lmps[:, 1];\n      for (t in 2:n_year) {\n        for (s_a in 1:3) {  // state of arrival\n          for (s_d in 1:3) { // state of departure\n            if (q[i, t]) {\n              tmp = best_lp[s_d, t - 1] + ltpm_z[s_a, s_d] + lmps[s_a, t];\n            } else {\n              // ignore observation process if not surveyed\n              tmp = best_lp[s_d, t - 1] + ltpm_z[s_a, s_d];\n            }\n            if (tmp &gt; best_lp[s_a, t]) {\n              back_ptr[s_a, t] = s_d;\n              best_lp[s_a, t] = tmp;\n            }\n          } // s_d\n        } // s_a\n        \n        // increment with ecological process for log-lik\n        lmps[:, t] += log_product_exp(ltpm_z, lmps[:, t - 1]);\n      } // t\n      log_lik[i] += log_sum_exp(lmps[:, n_year_i[i]]);\n      \n      // ecological states\n      tmp = max(best_lp[:, n_year]);\n      for (s_a in 1:3) {\n        if (best_lp[s_a, n_year] == tmp) {\n          z[i, n_year] = s_a;\n        }\n      } // s_a\n      for (t in 1:(n_year - 1)) {\n        tt = n_year - t + 1;\n        z[i, tt - 1] = back_ptr[z[i, tt], tt];\n      } // t\n    } // i\n  }\n}\n\n\nWe’ll run Stan and visually summarise the parameter estimates, with the input plotted alongside in green. It looks like we did alright, but there is a lot of uncertainty, largely because sites can move between 3 states between each year. For this reason, dynamic occupancy models are notoriously data-hungry. Often the initial state parameters are particularly difficult to estimate and are sometimes best parameterised as the steady state vector of the TPM. I haven’t quite worked out how to implement this efficiently in Stan.\n\n\nCode\n# data for Stan\nocc_data &lt;- list(n_site = n_site, n_year = n_year, n_sec = n_sec,\n                 n_year_i = n_year_i, years = years, n_sec_it = n_sec_it,\n                 secs = secs, y = y) |&gt; \n  sapply(\\(x) replace(x, is.na(x), 1))\n\n# run HMC\nfit_dyn &lt;- occ_dyn_ms$sample(data = occ_data, refresh = 0, init = 0.1,\n                             chains = n_chains, parallel_chains = n_chains, iter_sampling = n_iter)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 151.8 seconds.\nChain 3 finished in 156.7 seconds.\nChain 4 finished in 159.2 seconds.\nChain 2 finished in 160.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 156.9 seconds.\nTotal execution time: 160.3 seconds.\n\n\n\n\nResults\n\n\nCode\n# visualise estimates\nfit_dyn$summary(c(\"eta\", \"gamma\", \"epsilon\", \"psi\", \"p\", \"delta\")) |&gt;\n  mutate(truth = c(eta, gamma, epsilon, psi, p, delta),\n         parameter = c(rep(\"eta\", 3), \n                       rep(c(\"gamma\", \"epsilon\", \"psi\", \"p\"), each = 2), \n                       \"delta\") |&gt;\n           factor(levels = c(\"eta\", \"gamma\", \"epsilon\", \"psi\", \"p\", \"delta\")),\n         variable = factor(variable) |&gt; fct_reorder(as.numeric(parameter)),\n         process = c(rep(\"Ecological Process\", 3 + 3 * 2), \n                     rep(\"Observation Process\", 3)) |&gt;\n           factor()) |&gt;\n  ggplot(aes(median, fct_rev(variable))) +\n  facet_wrap(~ process, scales = \"free_y\", ncol = 1) +\n  geom_pointrange(aes(xmin = q5, xmax = q95), position = position_nudge(y = 1/10)) +\n  geom_point(aes(truth), colour = green, position = position_nudge(y = -1/10)) +\n  scale_x_continuous(breaks = seq(0.2, 1, 0.2), expand = c(0, 0)) +\n  scale_y_discrete(labels = ggplot2:::parse_safe) +\n  labs(x = \"Posterior\", y = \"Parameter\")\n\n\n\n\n\nLet’s also check how many of our ecological ecological states were correctly recovered. Since only state 3 (snakes present and breeding) can be directly observed with certainty, we’ll only check the ecological states for those times where no breeding was observed. We certainly didn’t do as well as in the mark-recapture example, which isn’t too surprising with a small number of individuals and the complexity of the model.\n\n\nCode\n# maximum observed state\nz_obs &lt;- apply(y, 1:2, \\(y) ifelse(sum(is.na(y)) == n_sec, \n                                   NA, max(y, na.rm = T)))\n\n# estimated ecological states\nz_est &lt;- fit_dyn$summary(\"z\")$median |&gt;\n  matrix(n_site, n_year)\n\n# proportion of ecological states correctly estimated\nunknowns &lt;- z_obs &lt; 3 | is.na(z_obs)\nsum((z_est == z)[unknowns]) / sum(unknowns)\n\n\n[1] 0.704\n\n\n\n\n\n\n\n\nSteady state distribution\n\n\n\nChatGPT tells me that to get the steady state distribution from a TPM, you perform the eigendecomposition of the TPM and the eigenvector corresponding to the eigenvalue of 1 is the steady state distribution. Stan does have eigendecomposition, so I’m sure there’s a way to parameterise the initial states nicely, but it gets more complicated with time- and/or individual-varying parameters."
  },
  {
    "objectID": "blog/hmm-in-stan.html#example-3-disease-structured-mark-recapture-with-state-misclassification",
    "href": "blog/hmm-in-stan.html#example-3-disease-structured-mark-recapture-with-state-misclassification",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Example 3: Disease-structured mark-recapture with state misclassification",
    "text": "Example 3: Disease-structured mark-recapture with state misclassification\nThe main reason I dove so deep into HMMs is because during my PhD I ran into a problem with my mark-recapture study on Fleay’s barred frogs (Mixophyes fleayi). For two years I conducted mark-recapture surveys along Gondwana rainforest streams in northern New South Wales, Australia, swabbing every frog I found to test for the presence of the pathogenic amphibian chytrid fungus (Batrachochytrium dendrobatidis, Bd) (Scheele et al. 2019) to hopefully infer something about (1) the effect of Bd on frog mortality rates and (2) the infection dynamics.\n\n\n\nAdult male Fleay’s or silverblue-eyed barred frog (Mixophyes fleayi) from Border Ranges National Park, New South Wales, Australia.\n\n\nThe ecological TRM of such a disease-structured multistate model is pretty straightforward, with three possible states: (1) alive and uninfected, (2) alive and infected, and (3) dead, which is an absorbing state:\n\\[\n\\mathrm{TRM}_z = \\begin{bmatrix}\n  -(\\psi_1 + \\phi_1) & \\psi_1 & \\phi_1 \\\\\n  \\psi_2 & -(\\psi_2 + \\phi_2) & \\phi_2 \\\\\n  0 & 0 & 0\n\\end{bmatrix}\n\\tag{8}\\]\nHere, \\(\\phi_{1:2}\\) are the mortality hazard rates of uninfected and infected frogs, respectively, and \\(\\psi_{1:2}\\) are the rates of gaining and clearing infections, respectively.\n\nNothing is certain\nWhile I was running qPCRs in the lab on some swabs collected during high frequency surveys (1 week intervals) with large numbers of recaptures, I noticed some frogs were apparently frequently clearing and re-gaining infections. For instance, a frog captured on 5 successive weeks may have looked something like this: \\(\\left[ 2, 2, 1, 2, 1 \\right]\\), implying the infection was lost twice and gained once across 4 weeks. This seemed unlikely, and it struck me as more probable that I was simply getting false negatives. At the same time, I couldn’t rule out false positives, either, given that contamination is always a risk. The best way to account for this uncertainty is to model it.\nI realised that if you conduct robust design sampling and collect a swab with each capture, you could essentially model the swabbing process like an occupancy model within a mark-recapture model. That is, imagine surveying frogs \\(i \\in 1:I\\) during \\(t \\in 1:T\\) primary occasions (say every 2 months). During each primary occasion, you conduct multiple secondary surveys \\(k \\in 1:K\\) within a short amount of time where you’re willing to assume the population is closed, that is, no animals dying, coming in, changing their infection state, etc. Every time you capture a frog, including those captured multiple times within a primary, you collect a swab. There are 3 possible observed states (\\(o\\)) per secondary: (1) frog captured with swab Bd–, (2) frog captured with swab Bd+, and (3) frog not captured. The TPM of this observation process for repeat secondaries with false negatives and false positives is as follows, with ecological states in the rows and observed states in the columns:\n\\[\n\\mathrm{TPM}_o = \\begin{bmatrix}\n  p_1 \\times ( 1 - \\lambda_1) & p_2 \\times \\lambda_1 & 1 - p_1 \\\\\n  p_2 \\times (1 - \\delta_1) & p_2 \\times \\delta_1 & 1 - p_2 \\\\\n  0 & 0 & 1\n\\end{bmatrix}\n\\tag{9}\\] Where \\(p_{1:2}\\) are the detection probabilities of uninfected and infected frogs, respectively, \\(\\lambda_1\\) is the probability of getting a Bd+ swab from an uninfected frog (false positive), and \\(\\delta_2\\) is the probability of detecting Bd on a swab from an infected frog (where \\(1 - \\delta_1\\) is the false negative probability). The Bd detection parameters \\(\\delta_1\\) and \\(\\lambda_1\\) are identified if at least some frogs are recaptured multiple times within a primary occasion.\n\n\nDouble-decking the HMM\nThis is already a lot, but it’s really no different from Example 2: a continuous time ecological process, simpler this time because the dead state is absorbing, and a discrete time observation process, albeit with an extra false positive parameter. However, the reality of the data was slightly more complex. It is conventional to run qPCR several times per swab to accurately quantify the DNA load present in a sample. But this is analogous to the swabbing process described above. Presumably, it’s possible to get false negatives and false positives in the qPCR diagnostic process as well. There are thus three diagnostic states (\\(y\\), data): (1) qPCR replicate Bd–, (2) qPCR replicate Bd+, or (3) no qPCR performed, because no frog was captured and thus no swab was collected. Typically, people will just average their measured DNA loads across runs and apply an inclusion criterion, such as that samples are considered infected when 2/3 qPCR replicates return Bd DNA. But we can do better, we can just model it. Here’s the TPM of the diagnostic process, with observed states in the rows and diagnostic states in the columns:\n\\[\n\\mathrm{TPM}_y = \\begin{bmatrix}\n  1 - \\lambda_2 & \\lambda_2 & 0 \\\\\n  1 - \\delta_2 & \\delta_2 & 0 \\\\\n  0 & 0 & 1\n\\end{bmatrix}\n\\tag{10}\\] where \\(\\lambda_2\\) is another false positive parameter and \\(\\delta_2\\) is the detection probability in the qPCR process.\n\n\nAlways try to use all of the data\nAnother element of this model involved incorporating infection intensity into the model. qPCR doesn’t just tell you whether a sample is infected, but it tells you how many DNA copies are present in the sample. So with multiple qPCR runs (\\(x\\)) \\(l \\in 1:L\\) collected from multiple samples (\\(n\\)), we can actually estimate the latent infection intensity on the individual \\(m\\) and samples:\n\\[\n\\begin{aligned}\n  m_{i,t} &\\sim \\mathrm{Lognormal} \\left( \\mu, \\sigma_1 \\right) \\\\\n  n_{i,t,k} &\\sim \\mathrm{Lognormal} \\left( m_{i,t}, \\sigma_2 \\right) \\\\\n  x_{i,t,k,l} &\\sim \\mathrm{Lognormal} \\left( n_{i,t,k}, \\sigma_3 \\right)\n\\end{aligned}\n\\tag{11}\\] where lognormals are used to ensure the infection intensities are positive, \\(\\mu\\) is the log population average and \\(\\sigma_{1:3}\\) are the population standard deviation and the errors of the swabbing and qPCR processes, respectively. We can incorporate the infection intensities to the ecological and observation models by modeling \\(\\phi_2\\), the mortality rates of infected frogs, and \\(\\delta_{1:2}\\), the Bd detection probabilities, as functions of the relevant infection intensities, for example like this:\n\\[\n\\begin{aligned}\n  {\\phi_2}_{i, t} &= \\exp \\left( \\log {\\phi_2}_\\alpha + {\\phi_2}_\\beta \\times m_{i,t} \\right) \\\\\n  {\\delta_1}_{i,t} &= 1 - (1 - r_1)^{m_{i,t}} \\\\\n  {\\delta_2}_{i,t,k} &= 1 - (1 - r_2)^{n_{i,t,k}}\n\\end{aligned}\n\\tag{12}\\] Here, the first equation is a simple GLM of the mortality hazard rate as a function of time-varying individual infection intensity, and the Bd detection probabilities are modeled as a function of \\(r_{1:2}\\), or the probabilities of detecting one log gene copy of DNA on a swab or qPCR replicate.\nDr. Andy Royle and myself published this model in Methods in Ecology and Evolution where we initially implemented the model in NIMBLE (Hollanders and Royle 2022). But of course, I was keen to try to marginalise out the latent ecological and observed states to fit this model in Stan (note that the ecological states are completely unobserved, as there’s no way of knowing whether an infected qPCR run actually came from an infected sample or frog!).\n\n\nStan implementation\nThis model took me a lot of time to get going in Stan, but I got there in the end. I won’t go into detail about explaining everything, but I’ll mention a few key things here:\n\nI implemented this model with threading (within-chain parallelisation), where the work from the different HMC chains gets partitioned across (potentially) all of the cores on your computer. With this particular individual-level likelihood, this should have considerable speed benefits. Basically, I put as much of the model block inside the partial_sum_lpmf() function which then gets called by reduce_sum(). See the Stan User’s Guide for more information. In this example, a lot of things could have been pre-computed instead of within each individual loop, but the current formulation is ready more more individual-level complexity.\nOne thing I got hung up on for a while was how the observation and diagnostic processes should be coded. The trick is to start with the diagnostic process, that is, the lowest level of the hierarchy that is directly conditioned on the data (as we have been doing). For each secondary survey \\(k\\) in primary \\(t\\) that an individual was captured (using the indicator q created in the transformed data block), you sum the diagnostic probability vectors associated with the observed data (diagnostic states) for each possible (partially latent) observed state. Because these probabilities differ for each observed state, we need another object to hold the intermediate quantities, which I’ve called omega. The diagnostic log probability vectors are then (log) matrix multiplied with the observation (log) TPM. If the frog was not captured during that secondary, lmps is simply incremented with the observation TPM probabilities associated with not being detected. From here, lmps is incremented with the ecological process as in the previous examples.\nBecause we only use the rows of the observation and diagnostic TPMs that are associated with alive and observed frogs, respectively, I don’t write out the full TPMs in the code.\nStan has some issue related to evaluating the gradient of 0 * log(0). I don’t know what that means exactly, but I know I was having errors with the model unless I formulated the model in a particular way (I think that’s why I had to subset \\(\\mathrm{TPM}_z\\) (ltpm_z) to not include the dead state, but I’m not sure). Specifically, as in Example 3, I only increment lmps with the alive states until the last primary of capture. After that occasion, the dead state is incremented for the first time. For subsequent occasions, the dead state is incremented separately from the alive states. In the generated quantities for the Viterbi algorithm, I do need the full ltpm_z, as I’m interested in propagating the probabilities of the dead state.\nThe inclusion of false positives means the observation and diagnostic processes are examples of finite mixtures that may have multimodality in certain model constructions (Royle and Link 2006). Here, the true pathogen detection probabilities \\(\\delta\\) are modeled as a function of infection intensity, which may be enough to avoid the multimodality. Just to be sure, however, I’ve constrained \\(\\lambda_{1:2}\\) to be less than \\(r_{1:2}\\) in the program and further given fairly strong \\(\\mathrm{Beta} (1, 10)\\) priors for \\(\\lambda\\).\nThe individual infection intensities m[i, t] are parameterised as centered lognormal distribution and the sample infection intensities n[i, t, k] as non-centered lognormal, as this seemed to give the best HMC performance.\n\n\nSimulation\nBelow I simulate some data where the rates of gaining and clearing infection are affected by temperature (negative and positive, respectively), which seems to be the case for the frog-Bd system. To account for the varying infection dynamics in the state probabilities at first capture, I’ll use the steady state distribution for each primary which is computed using the infection dynamics probabilities, given by \\({\\psi_p}_{1:2} = 1 - \\exp(-\\psi_{1:2})\\). Then, the expected probability of being infected at first capture in each primary is \\(\\eta = \\frac{{\\psi_p}_1}{{\\psi_p}_1 + {\\psi_p}_2}\\).\n\n\nCode\n# metadata\nn_ind &lt;- 100\nn_prim &lt;- 8\nn_sec &lt;- 3\nn_diag &lt;- 3\ntau &lt;- rlnorm(n_prim - 1, log(1), 0.5)             # unequal primary occasion intervals\ntemp &lt;- rnorm(n_prim - 1) |&gt; sort(decreasing = T)  # it's getting colder\n\n# parameters\nphi_a &lt;- c(0.1, 0.1)       # mortality rates of uninfected/infected frogs (intercepts)\nphi_b &lt;- 0.3               # effect of one log Bd gene copy on mortality\npsi_a &lt;- c(0.7, 0.4)       # rates of gaining/clearing infections (intercepts)\npsi_b &lt;- c(-0.4, 0.3)      # effect of temperature on rates of gaining/clearing infections\np_a &lt;- c(0.7, 0.6)         # detection probabilities of uninfected/infected frogs\nr &lt;- c(0.4, 0.6)           # pathogen detection probabilities (per log Bd gene copy)\nlambda &lt;- c(0.05, 0.10)    # sampling/diagnostic false positive probabilities\nmu &lt;- 1.0                  # population log average infection intensity\nsigma &lt;- c(0.3, 0.2, 0.1)  # population and sampling/diagnostic SDs\n\n# TPM containers\ntrm_z &lt;- tpm_z &lt;- array(NA, c(3, 3, n_ind, n_prim - 1))\ntpm_o &lt;- array(NA, c(3, 3, n_ind, n_prim, n_sec))\ntpm_y &lt;- array(NA, c(3, 3, n_ind, n_prim, n_sec))\n\n# parameter containers\nphi &lt;- array(phi_a, c(2, n_ind, n_prim - 1))\npsi &lt;- exp(matrix(log(psi_a), 2, n_prim - 1) + matrix(psi_b, 2, n_prim - 1) * matrix(temp, 2, n_prim - 1, byrow = T))\npsi_p &lt;- 1 - exp(-psi)\neta &lt;- psi_p[1, ] / apply(psi_p, 2, sum)\np &lt;- array(p_a, c(2, n_ind, n_prim, n_sec))\nm &lt;- delta1 &lt;- array(NA, c(n_ind, n_prim))\nn &lt;- delta2 &lt;- array(NA, c(n_ind, n_prim, n_sec))\n\n# primary occasions of first capture\nfirst &lt;- sort(sample(1:(n_prim - 1), n_ind, replace = T))\n\n# TPMs\nfor (i in 1:n_ind) {\n  \n  # fix detection probabilities in first secondary of first primary to ensure capture\n  p[, i, first[i], 1] &lt;- 1\n  \n  for (t in first[i]:n_prim) {\n    \n    # individual infection intensity\n    m[i, t] &lt;- rlnorm(1, mu, sigma[1])\n    \n    # sample pathogen detection\n    delta1[i, t] &lt;- 1 - (1 - r[1]) ^ m[i, t]\n    \n    # sample infection intensities\n    n[i, t, ] &lt;- rlnorm(n_sec, log(m[i, t]), sigma[2])\n    \n    # diagnostic pathogen detection\n    delta2[i, t, ] &lt;- 1 - (1 - r[2]) ^ n[i, t, ]\n    \n    for (k in 1:n_sec) {\n      \n      # observation TPM\n      tpm_o[, , i, t, k] &lt;- matrix(c(p[1, i, t, k] * (1 - lambda[1]), p[1, i, t, k] * lambda[1], 1 - p[1, i, t, k],\n                                     p[2, i, t, k] * (1 - delta1[i, t]), p[2, i, t, k] * delta1[i, t], 1 - p[2, i, t, k], \n                                     0, 0, 1), \n                                   3, byrow = T)\n      \n      # diagnostic TPM\n      tpm_y[, , i, t, k] &lt;- matrix(c(1 - lambda[2], lambda[2], 0, \n                                     1 - delta2[i, t, k], delta2[i, t, k], 0, \n                                     0, 0, 1), \n                                   3, byrow = T)\n\n    } # k\n  } # t\n  \n  for (t in first[i]:(n_prim - 1)) {\n    \n    # mortality rate as a function of infection intensity\n    phi[2, i, t] &lt;- exp(log(phi_a[2]) + phi_b * m[i, t])\n    \n    # ecological TRM/TPM\n    trm_z[, , i, t] &lt;- matrix(c(-(psi[1, t] + phi[1, i, t]), psi[1, t], phi[1, i, t], \n                                psi[2, t], -(psi[2, t] + phi[2, i, t]), phi[2, i, t], \n                                0, 0, 0), \n                              3, byrow = T)\n    tpm_z[, , i, t] &lt;- expm(trm_z[, , i, t] * tau[t])\n    \n  } # t\n} # i\n\n# containers\nz &lt;- array(NA, c(n_ind, n_prim))\no &lt;- array(NA, c(n_ind, n_prim, n_sec))\ny &lt;- x &lt;- array(NA, c(n_ind, n_prim, n_sec, n_diag))\n\n# simulation\nfor (i in 1:n_ind) {\n  \n  # ecological process\n  z[i, first[i]] &lt;- rcat(1, c(1 - eta[first[i]], eta[first[i]]))\n  for (t in (first[i] + 1):n_prim) {\n    z[i, t] &lt;- rcat(1, tpm_z[z[i, t - 1], , i, t - 1])\n  } # t\n  \n  for (t in first[i]:n_prim) {\n    for (k in 1:n_sec) {\n      \n      # observation process\n      o[i, t, k] &lt;- rcat(1, tpm_o[z[i, t], , i, t, k])\n      \n      # diagnostic process\n      y[i, t, k, ] &lt;- rcat(n_diag, tpm_y[o[i, t, k], , i, t, k])\n      \n      # diagnostic run infection intensity\n      for (l in 1:n_diag) {\n        if (y[i, t, k, l] == 2) {\n          x[i, t, k, l] &lt;- rlnorm(1, log(n[i, t, k]), sigma[3])\n        }\n      } # l\n    } # k\n  } # t\n} # i\n\n\n\n\nStan program\nHere’s the Stan program.\n\n\nCode\nfunctions {\n  /**\n   * Return partial sum over individuals applying the forward algorithm for multievent mark-recapture model formulated as hidden Markov model\n   *\n   * @param seq_ind    Sequence of individuals from 1 to n_ind\n   * @param start      Start of seq_ind for partial sum\n   * @param end        End of seq_ind for partial sum\n   * @param y          Detection history\n   * @param q          Logical indicating whether individual was detected in each secondary\n   * @param n_prim     Number of primary occasions\n   * @param n_sec      Number of secondary occasions per primary\n   * @param first      Primary occasion of first capture per individual\n   * @param last       Primary occasion of last capture per individual\n   * @param first_sec  Secondary of first capture in primary of first capture\n   * @param tau        Time intervals between primary occasions\n   * @param temp       Average temperatures between primary occasions\n   * @param phi_a      Mortality rates of uninfected and infected individuals (intercepts)\n   * @param phi_b      Effect of one log gene copy of pathogen on mortality rate\n   * @param psi_a      Rates of gaining and clearing infections (intercepts)\n   * @param psi_b      Effects of temperature on rates of gaining and clearing infections\n   * @param p_a        Detection probabilities of uninfected and infected individuals\n   * @param r          Probabilities of detecting one log gene copy of pathogen on individuals (sampling process) and samples (diagnostic process)\n   * @param lambda     False positive probabilities in sampling and diagnostic processes\n   * @param m          Individual infection intensities per primary\n   * @param n_z        z-scores of sample infection intensities per secondary\n   * @param sigma      Vector of infection intensity SDs\n   *\n   * @return           Log probability for subset of individuals\n   */\n  real partial_sum_lpmf(\n    data array[] int seq_ind, data int start, data int end, data array[,,,] int y, data array[,,] int q, \n    data int n_prim, data int n_sec, data array[] int first, data array[] int last, data array[] int first_sec, data vector tau, data vector temp, \n    vector phi_a, real phi_b, vector psi_a, vector psi_b, vector p_a, vector r, vector lambda, matrix m, array[] matrix n_z, vector sigma) {\n      \n      // number of individuals in partial sum and initialise partial target\n      int n_ind = end - start + 1;\n      real ptarget = 0;\n      \n      // containers\n      array[2] vector[n_prim - 1] phi, psi, psi_p;\n      vector[n_prim - 1] eta, log_eta, log1m_eta;\n      array[2] matrix[n_sec, n_prim] p;\n      matrix[n_ind, n_prim] log_m = log(m[:, start:end])';\n      array[n_ind] matrix[n_sec, n_prim] n;\n      tuple(vector[n_prim], matrix[n_sec, n_prim]) delta;\n      \n      // transform intercepts\n      vector[2] log_phi_a = log(phi_a);\n      vector[2] log_psi_a = log(psi_a);\n      \n      // transition rate/log probability matrices (TRM/LTPM)\n      array[n_prim - 1] matrix[3, 3] trm;\n      array[n_prim - 1] matrix[3, 2] ltpm_z;\n      array[n_prim, n_sec] matrix[2, 3] ltpm_o;\n      array[n_prim, n_sec] matrix[2, 2] ltpm_y;\n      \n      // log probabilities\n      array[n_prim] matrix[2, n_sec] omega;\n      matrix[3, n_prim] lmps;\n      \n      // for each individual\n      for (i in 1:n_ind) {\n        \n        // index that maps to original n_ind\n        int ii = i + start - 1;\n        \n        // ecological process parameters\n        phi[1] = rep_vector(phi_a[1], n_prim - 1);\n        phi[2] = exp(log_phi_a[2] + phi_b * m[1:(n_prim - 1), ii]);\n        for (s in 1:2) {\n          psi[s] = exp(log_psi_a[s] + psi_b[s] * temp);\n          psi_p[s] = 1 - exp(-psi[s]);\n        }\n        eta = psi_p[1] ./ (psi_p[1] + psi_p[2]);\n        log_eta = log(eta);\n        log1m_eta = log1m(eta);\n        \n        // primary occasion intervals\n        for (t in first[ii]:(n_prim - 1)) {\n          \n          // ecological TRM\n          trm[t][1, 1] = -(psi[1][t] + phi[1][t]);\n          trm[t][1, 2] = psi[1][t];\n          trm[t][1, 3] = phi[1][t];\n          trm[t][2, 1] = psi[2][t];\n          trm[t][2, 2] = -(psi[2][t] + phi[2][t]);\n          trm[t][2, 3] = phi[2][t];\n          trm[t][3] = zeros_row_vector(3);\n          trm[t] = trm[t]';\n          \n          // ecological log TPM\n          ltpm_z[t] = log(matrix_exp(trm[t] * tau[t])[:, 1:2]);\n          \n        } // t\n        \n        // sample infection intensities\n        ptarget += std_normal_lupdf(to_vector(n_z[ii]));\n        n[i] = exp(rep_matrix(log_m[i], n_sec) + n_z[ii] * sigma[2]);\n        \n        // individual and sample infection detection probabilities\n        delta.1 = 1 - pow(1 - r[1], m[:, ii]);\n        delta.2 = 1 - pow(1 - r[2], n[i]);\n        \n        // detection probabilities (fixed at 1 for secondary of first capture)\n        for (s in 1:2) {\n          p[s] = rep_matrix(p_a[s], n_sec, n_prim);\n          p[s][first_sec[ii], first[ii]] = 1;\n        } // s\n        \n        // initialise log MPS\n        lmps = rep_matrix(0, 3, n_prim);\n        \n        // for each primary occasion\n        for (t in first[ii]:n_prim) {\n          \n          // for each secondary occasion\n          for (k in 1:n_sec) {\n            \n            // observation log TPM\n            ltpm_o[t, k][1, 1] = log(p[1][k, t]) + log1m(lambda[1]);\n            ltpm_o[t, k][1, 2] = log(p[1][k, t]) + log(lambda[1]);\n            ltpm_o[t, k][1, 3] = log1m(p[1][k, t]);\n            ltpm_o[t, k][2, 1] = log(p[2][k, t]) + log1m(delta.1[t]);\n            ltpm_o[t, k][2, 2] = log(p[2][k, t]) + log(delta.1[t]);\n            ltpm_o[t, k][2, 3] = log1m(p[2][k, t]);\n            \n            // diagnostic log TPM\n            ltpm_y[t, k][1, 1] = log1m(lambda[2]);\n            ltpm_y[t, k][1, 2] = log(lambda[2]);\n            ltpm_y[t, k][2, 1] = log1m(delta.2[k, t]);\n            ltpm_y[t, k][2, 2] = log(delta.2[k, t]);\n            \n            // if captured during secondary\n            if (q[ii, t, k]) {\n              \n              // diagnostic probabilities (y[ii, t, k] is an array of n_diag integers)\n              for (o in 1:2) {\n                omega[t][o, k] = sum(ltpm_y[t, k][o, y[ii, t, k]]);\n              }\n              \n              // accrue log marginal probabilities with observation process\n              lmps[1:2, t] += log_product_exp(ltpm_o[t, k][:, 1:2], omega[t][:, k]);\n              \n              // if not captured, observation process only\n            } else {\n              lmps[1:2, t] += ltpm_o[t, k][:, 3];\n            }\n          } // k\n        } // t\n        \n        // marginal log probabilities of alive states at first capture\n        lmps[1:2, first[ii]] += [ log1m_eta[first[i]], log_eta[first[i]] ]';\n        \n        // marginal log probabilities of alive states between first and last primary with captures\n        if (first[ii] &lt; last[ii]) {\n          for (t in (first[ii] + 1):last[ii]) {\n            lmps[1:2, t] += log_product_exp(ltpm_z[t - 1][1:2], lmps[1:2, t - 1]);\n          } // t\n        }\n        \n        // if last captured in the last primary\n        if (last[ii] == n_prim) {\n          \n          // increment target with marginal log probabilities of alive states\n          ptarget += log_sum_exp(lmps[1:2, n_prim]);\n        } else {\n          \n          // marginal log probabilities of all ecological states in primary after last capture\n          lmps[1:2, last[ii] + 1] += log_product_exp(ltpm_z[last[ii]][1:2], lmps[1:2, last[ii]]);\n          lmps[3, last[ii] + 1] = log_product_exp(ltpm_z[last[ii]][3], lmps[1:2, last[ii]]);\n          \n          // if there are more primaries\n          if ((last[ii] + 1) &lt; n_prim) {\n            \n            // marginal log probabilities of all ecological states until last primary\n            for (t in (last[ii] + 2):n_prim) {\n              lmps[1:2, t] += log_product_exp(ltpm_z[t - 1][1:2], lmps[1:2, t - 1]);\n              lmps[3, t] = log_sum_exp(log_product_exp(ltpm_z[t - 1][3], lmps[1:2, t - 1]), lmps[3, t - 1]);\n            } // t\n          }\n          \n          // increment target with marginal log probabilities of all ecological states\n          ptarget += log_sum_exp(lmps[:, n_prim]);\n        }\n      } // i\n      \n      return ptarget;\n    }\n    \n  /**\n   * Return the natural logarithm of the product of the element-wise exponentiation of the specified matrices\n   *\n   * @param a  First matrix or (row_)vector\n   * @param b  Second matrix or (row_)vector\n   *\n   * @return   log(exp(a) * exp(b))\n   */\n  matrix log_product_exp(matrix a, matrix b) {\n    int x = rows(a);\n    int y = cols(b);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    matrix[x, y] c;\n    for (j in 1:y) {\n      for (i in 1:x) {\n        c[i, j] = log_sum_exp(a_tr[:, i] + b[:, j]);\n      }\n    }\n    return c;\n  }\n  vector log_product_exp(matrix a, vector b) {\n    int x = rows(a);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    vector[x] c;\n    for (i in 1:x) {\n      c[i] = log_sum_exp(a_tr[:, i] + b);\n    }\n    return c;\n  }\n  row_vector log_product_exp(row_vector a, matrix b) {\n    int y = cols(b);\n    vector[size(a)] a_tr = a';\n    row_vector[y] c;\n    for (j in 1:y) {\n      c[j] = log_sum_exp(a_tr + b[:, j]);\n    }\n    return c;\n  }\n  real log_product_exp(row_vector a, vector b) {\n    real c = log_sum_exp(a' + b);\n    return c;\n  }\n}\n\ndata {\n  int grainsize, n_ind, n_prim, n_sec, n_diag, n_x;\n  array[n_ind] int&lt;lower=1, upper=n_prim&gt; first, last, first_sec;\n  vector&lt;lower=0&gt;[n_prim - 1] tau;\n  vector[n_prim - 1] temp;\n  array[n_ind, n_prim, n_sec, n_diag] int&lt;lower=1, upper=3&gt; y;\n  array[n_x] int&lt;lower=1&gt; ind, prim, sec;\n  vector[n_x] x;\n}\n\ntransformed data {\n  array[n_ind] int seq_ind = linspaced_int_array(n_ind, 1, n_ind);\n  array[n_ind, n_prim, n_sec] int&lt;lower=0, upper=1&gt; q = rep_array(0, n_ind, n_prim, n_sec);\n  for (i in 1:n_ind) {\n    for (t in first[i]:n_prim) {\n      for (k in 1:n_sec) {\n        if (min(y[i, t, k]) &lt; 3) {\n          q[i, t, k] = 1;\n        }\n      }\n    }\n  }\n}\n\nparameters {\n  vector&lt;lower=0&gt;[2] phi_a, psi_a;\n  real phi_b;\n  vector[2] psi_b;\n  vector&lt;lower=0, upper=1&gt;[2] p_a, r;\n  vector&lt;lower=0, upper=r&gt;[2] lambda;\n  real mu;\n  matrix&lt;lower=0&gt;[n_prim, n_ind] m;\n  array[n_ind] matrix[n_sec, n_prim] n_z;\n  vector&lt;lower=0&gt;[3] sigma;            \n}\n\nmodel {\n  // log sample infection intensities\n  array[n_ind] matrix[n_sec, n_prim] log_n;\n  matrix[n_ind, n_prim] log_m = log(m)';\n  for (i in 1:n_ind) {\n    log_n[i] = rep_matrix(log_m[i], n_sec) + n_z[i] * sigma[2];\n  }\n  \n  // likelihood of diagnostic infection intensities\n  for (j in 1:n_x) {\n    target += lognormal_lupdf(x[j] | log_n[ind[j]][sec[j], prim[j]], sigma[3]);\n  }\n  \n  // likelihood of CMR\n  target += reduce_sum(partial_sum_lupmf, seq_ind, grainsize, y, q, \n                       n_prim, n_sec, first, last, first_sec, tau, temp, \n                       phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, m, n_z, sigma);\n                       \n  // priors\n  target += exponential_lupdf(phi_a | 1);\n  target += std_normal_lupdf(phi_b);\n  target += exponential_lupdf(psi_a | 1);\n  target += std_normal_lupdf(psi_b);\n  target += beta_lupdf(p_a | 1, 1);\n  target += beta_lupdf(r | 1, 1);\n  target += beta_lupdf(lambda | 1, 10);\n  target += normal_lupdf(mu | 0, 1);\n  target += exponential_lupdf(sigma | 1);\n  target += lognormal_lupdf(to_vector(m) | mu, sigma[1]);\n}\n\ngenerated quantities {\n  // containers\n  array[n_ind, n_prim] int z;\n  vector[n_ind] log_lik;\n  {\n    array[2] vector[n_prim - 1] phi, psi, psi_p;\n    vector[n_prim - 1] eta, log_eta, log1m_eta;\n    array[2] matrix[n_sec, n_prim] p;\n    matrix[n_ind, n_prim] log_m = log(m)';\n    array[n_ind] matrix[n_sec, n_prim] n;\n    tuple(vector[n_prim], matrix[n_sec, n_prim]) delta;\n    vector[2] log_phi_a = log(phi_a);\n    vector[2] log_psi_a = log(psi_a);\n    array[n_prim - 1] matrix[3, 3] trm;\n    array[n_prim - 1] matrix[3, 3] ltpm_z;\n    array[n_prim, n_sec] matrix[2, 3] ltpm_o;\n    array[n_prim, n_sec] matrix[2, 2] ltpm_y;\n    array[n_prim] matrix[2, n_sec] omega;\n    array[3, n_prim] int back_ptr;\n    matrix[3, n_prim] lmps, best_lp;\n    real tmp;\n    int tt;\n    \n    for (i in 1:n_ind) {\n    \n      phi[1] = rep_vector(phi_a[1], n_prim - 1);\n      phi[2] = exp(log_phi_a[2] + phi_b * m[1:(n_prim - 1), i]);\n      for (s in 1:2) {\n        psi[s] = exp(log_psi_a[s] + psi_b[s] * temp);\n        psi_p[s] = 1 - exp(-psi[s]);\n      }\n      eta = psi_p[1] ./ (psi_p[1] + psi_p[2]);\n      log_eta = log(eta);\n      log1m_eta = log1m(eta);\n      for (t in first[i]:(n_prim - 1)) {\n        trm[t][1, 1] = -(psi[1][t] + phi[1][t]);\n        trm[t][1, 2] = psi[1][t];\n        trm[t][1, 3] = phi[1][t];\n        trm[t][2, 1] = psi[2][t];\n        trm[t][2, 2] = -(psi[2][t] + phi[2][t]);\n        trm[t][2, 3] = phi[2][t];\n        trm[t][3] = zeros_row_vector(3);\n        trm[t] = trm[t]';\n        // dead state remains for Viterbi algorithm\n        ltpm_z[t] = log(matrix_exp(trm[t] * tau[t]));\n      } // t\n      n[i] = exp(rep_matrix(log_m[i], n_sec) + n_z[i] * sigma[2]);\n      delta.1 = 1 - pow(1 - r[1], m[:, i]);\n      delta.2 = 1 - pow(1 - r[2], n[i]);\n      for (s in 1:2) {\n        p[s] = rep_matrix(p_a[s], n_sec, n_prim);\n        p[s][first_sec[i], first[i]] = 1;\n      } // s\n      lmps = rep_matrix(0, 3, n_prim);\n      best_lp = rep_matrix(negative_infinity(), 3, n_prim);\n      \n      for (t in first[i]:n_prim) {\n        for (k in 1:n_sec) {\n          ltpm_o[t, k][1, 1] = log(p[1][k, t]) + log1m(lambda[1]);\n          ltpm_o[t, k][1, 2] = log(p[1][k, t]) + log(lambda[1]);\n          ltpm_o[t, k][1, 3] = log1m(p[1][k, t]);\n          ltpm_o[t, k][2, 1] = log(p[2][k, t]) + log1m(delta.1[t]);\n          ltpm_o[t, k][2, 2] = log(p[2][k, t]) + log(delta.1[t]);\n          ltpm_o[t, k][2, 3] = log1m(p[2][k, t]);\n          ltpm_y[t, k][1, 1] = log1m(lambda[2]);\n          ltpm_y[t, k][1, 2] = log(lambda[2]);\n          ltpm_y[t, k][2, 1] = log1m(delta.2[k, t]);\n          ltpm_y[t, k][2, 2] = log(delta.2[k, t]);\n          // observation log probabilities\n          if (q[i, t, k]) {\n            for (o in 1:2) {\n              omega[t][o, k] = sum(ltpm_y[t, k][o, y[i, t, k]]);\n            }\n            lmps[1:2, t] += log_product_exp(ltpm_o[t, k][:, 1:2], omega[t][:, k]);\n          } else {\n            lmps[1:2, t] += ltpm_o[t, k][:, 3];\n          }\n        } // k\n      } // t\n      \n      // first primary\n      lmps[1:2, first[i]] = [ log1m_eta[first[i]], log_eta[first[i]] ]' + lmps[1:2, first[i]];\n      best_lp[1:2, first[i]] = lmps[1:2, first[i]];\n      \n      // Viterbi\n      if (first[i] &lt; last[i]) {\n        for (t in (first[i] + 1):last[i]) {\n          for (s_a in 1:2) {  // state of arrival\n            for (s_d in 1:2) {  // state of departure\n              tmp = best_lp[s_d, t - 1] + ltpm_z[t - 1][s_a, s_d] + lmps[s_a, t];\n              if (tmp &gt; best_lp[s_a, t]) {\n                back_ptr[s_a, t] = s_d;\n                best_lp[s_a, t] = tmp;\n              }\n            } // s_d\n          } // s_a\n          // increment with ecological process for log-lik\n          lmps[1:2, t] += log_product_exp(ltpm_z[t - 1][1:2, 1:2], lmps[1:2, t - 1]);\n        } // t\n      }\n      if (last[i] &lt; n_prim) {\n        for (t in (last[i] + 1):n_prim) {\n          for (s_a in 1:3) {  // state of arrival\n            for (s_d in 1:3) {  // state of departure\n              tmp = best_lp[s_d, t - 1] + ltpm_z[t - 1][s_a, s_d] + lmps[s_a, t];\n              if (tmp &gt; best_lp[s_a, t]) {\n                back_ptr[s_a, t] = s_d;\n                best_lp[s_a, t] = tmp;\n              }\n            } // s_d\n          } // s_a\n          // increment with ecological process for log-lik\n          lmps[:, t] += log_product_exp(ltpm_z[t - 1], lmps[:, t - 1]);\n        } // t\n        log_lik[i] = log_sum_exp(lmps[:, n_prim]);\n      }\n      \n      // ecological states\n      tmp = max(best_lp[:, n_prim]);\n      for (s_a in 1:3) { // state of arrival\n        if (best_lp[s_a, n_prim] == tmp) {\n          z[i, n_prim] = s_a;\n        }\n      } // s_a\n      for (t in first[i]:(n_prim - 1)) {\n        tt = n_prim - t + first[i];\n        z[i, tt - 1] = back_ptr[z[i, tt], tt];\n      } // t\n    } // i\n  }\n}\n\n\nNext I prepare the data for Stan and run HMC with threading enabled for more computational power. On this M2 MacBook Pro I have 10 cores, meaning if I just use 2 HMC chains I can spread the work across 5 cores each. The grainsize arguments recommends how to partition the work in the partial sum function across cores.\n\n\nCode\n# convert diagnostic infection intensities to long format\nx_long &lt;- reshape2::melt(x, value.name = \"x\", varnames = c(\"ind\", \"prim\", \"sec\", \"diag\")) |&gt; \n  as_tibble() |&gt;\n  drop_na() |&gt;\n  arrange(ind)\n\n# data for Stan\nn_chains &lt;- 2\nn_cores &lt;- parallel::detectCores()\nn_threads &lt;- floor(n_cores / n_chains)\nfirst_sec &lt;- rep(1, n_ind)  # simulated to be first captured in first secondary\nlast &lt;- apply(y, 1:2, min) |&gt; apply(1, \\(y) max(which(y &lt; 3)))\ngoat_data &lt;- list(grainsize = floor(n_ind / n_threads / 4),\n                  n_ind = n_ind, n_prim = n_prim, n_sec = n_sec, n_diag = n_diag,\n                  first = first, last = last, first_sec = first_sec, tau = tau, temp = temp,\n                  y = ifelse(is.na(y), 3, y),\n                  n_x = nrow(x_long), ind = x_long$ind, prim = x_long$prim, sec = x_long$sec,\n                  x = x_long$x)\n\n# run HMC\nfit_goat &lt;- hmm_goat$sample(data = goat_data, refresh = 0, init = 0.1, \n                            threads_per_chain = n_threads, chains = n_chains, parallel_chains = n_chains, iter_sampling = n_iter)\n\n\nRunning MCMC with 2 parallel chains...\n\nChain 1 finished in 512.2 seconds.\nChain 2 finished in 662.3 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 587.2 seconds.\nTotal execution time: 662.4 seconds.\n\n\n\n\n\nResults\nWe’ll visually check the parameter estimates with the simulation input and see that it was recovered well. For what it’s worth, in the paper we showed that the infection dynamics get overestimated significantly (5-fold in our application) when not accounting for state misclassification.\n\n\nCode\n# check estimates\nfit_goat$summary(c(\"phi_a\", \"phi_b\", \"psi_a\", \"psi_b\", \"p_a\", \"r\", \"lambda\", \"mu\", \"sigma\")) |&gt;\n  mutate(truth = c(phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, mu, sigma),\n         parameter = c(rep(\"phi\", 3), \n                       rep(c(\"psi\", \"psi\", \"p\", \"r\", \"lambda\"), each = 2), \n                       \"mu\", rep(\"sigma\", 3)) |&gt;\n           factor(levels = c(\"phi\", \"psi\", \"p\", \"r\", \"lambda\", \"mu\", \"sigma\")),\n         variable = factor(variable) |&gt;\n           str_remove(c(\"_a\")) |&gt; \n           str_replace(\"phi\\\\[2\\\\]\", \"phi\\\\[2\\\\[alpha\\\\]\\\\]\") |&gt;\n           str_replace(\"phi_b\", \"phi\\\\[2\\\\[beta\\\\]\\\\]\") |&gt;\n           str_replace(\"psi\\\\[1\\\\]\", \"psi\\\\[1\\\\[alpha\\\\]\\\\]\") |&gt;\n           str_replace(\"psi\\\\[2\\\\]\", \"psi\\\\[2\\\\[alpha\\\\]\\\\]\") |&gt;\n           str_replace(\"psi_b\\\\[1\\\\]\", \"psi\\\\[1\\\\[beta\\\\]\\\\]\") |&gt;\n           str_replace(\"psi_b\\\\[2\\\\]\", \"psi\\\\[2\\\\[beta\\\\]\\\\]\") |&gt;\n           fct_reorder(as.numeric(parameter)),\n         process = c(rep(\"Ecological Process\", 3 + 2 + 2),\n                     rep(\"Observation and Diagnostic Processes\", 2 * 3),\n                     rep(\"Infection Intensities\", 1 + 3)) |&gt;\n           factor(levels = c(\"Ecological Process\", \"Observation and Diagnostic Processes\", \"Infection Intensities\"))) |&gt;\n  ggplot(aes(median, fct_rev(variable))) +\n  facet_wrap(~ process, scales = \"free_y\", ncol = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", colour = \"#333333\")  +\n  geom_pointrange(aes(xmin = q5, xmax = q95), position = position_nudge(y = 1/10)) +\n  geom_point(aes(truth), colour = green, position = position_nudge(y = -1/10)) +\n  scale_x_continuous(breaks = seq(-1, 1, 0.5), expand = c(0, 0)) +\n  scale_y_discrete(labels = ggplot2:::parse_safe) +\n  labs(x = \"Posterior\", y = \"Parameter\")\n\n\n\n\n\nAgain, we might check to see how well we recovered our latent ecological states, which were completely unobservable from the data.\n\n\nCode\nz_est &lt;- fit_goat$summary(\"z\")$median |&gt;\n  matrix(n_ind, n_prim) |&gt;\n  {\\(z) ifelse(z &lt; 1, NA, z)}()\nsum(z_est == z, na.rm = T) / sum(!is.na(z))\n\n\n[1] 0.946\n\n\nOne of the main reasons to account for imperfect pathogen detection is to get more realistic estimates of infection prevalence in the population. We can use the full posterior distribution of the latent ecological states from the Viterbi algorithm to estimate the infection prevalence per primary occasion, which here was simulated to be increasing due to dropping temperatures going into austral winter plotted for some fictional dates to show the unequal primary occasion intervals.\n\n\nCode\n# dates\nstart &lt;- ymd(\"2020-12-01\")\ntime_unit &lt;- 21\ndates &lt;- seq.Date(start, start + days(1 + round(sum(tau) * time_unit)), by = 1)\nprim &lt;- dates[c(1, sapply(1:(n_prim - 1), \\(t) round(sum(tau[1:t] * time_unit))))]\n\n# plot\nfit_goat$draws(\"z\", format = \"draws_df\") |&gt;\n  pivot_longer(contains(\"z\"), values_to = \"z\") |&gt;\n  mutate(z = if_else(z &lt; 1, NA, z),\n         ind = rep(1:n_ind, n() / n_ind) |&gt; factor(),\n         prim = rep(prim, each = n_ind) |&gt; rep(n() / n_ind / n_prim)) |&gt;\n  mutate(alive = sum(z == 1 | z == 2, na.rm = T),\n         infected = sum(z == 2, na.rm = T),\n         prevalence = infected / alive,\n         .by = c(.draw, prim)) |&gt;\n  summarise(mean = mean(prevalence),\n            q5 = quantile(prevalence, 0.05),\n            q95 = quantile(prevalence, 0.95),\n            .by = prim) |&gt;\n  ggplot(aes(prim, mean)) +\n  geom_pointrange(aes(ymin = q5, ymax = q95), position = position_nudge(x = 2)) +\n  geom_point(aes(y = infected / alive),\n             data = tibble(prim = prim,\n                           alive = apply(z, 2, \\(z) sum(z == 1 | z == 2, na.rm = T)),\n                           infected = apply(z, 2, \\(z) sum(z == 2, na.rm = T))),\n             colour = green, position = position_nudge(x = -2)) +\n  scale_y_continuous(breaks = seq(0.2, 1, 0.2), limits = c(0, 1.001), expand = c(0, 0)) +\n  labs(x = \"Primary Occasion\", y = \"Infection Prevalence\")\n\n\n\n\n\nThanks for reading if you made it this far. I hope this guide will be useful for ecologists (and others) wanting to transition to gradient-based sampling methods for complex ecological models. I’m sure I’ve some errors somewhere, so if someone finds any or has any questions, please don’t hesitate to reach out."
  },
  {
    "objectID": "about/people/matthijs.html",
    "href": "about/people/matthijs.html",
    "title": "Dr. Matthijs Hollanders",
    "section": "",
    "text": "Matthijs received Bachelor and Master’s degrees in Biology from Wageningen University & Research, The Netherlands, and was awarded his doctorate from Southern Cross University, Australia, in 2023. A passion for quantitative methods was sparked during his PhD at SCU where he investigated contemporary amphibian responses to the disease chytridiomycosis. Intrigued by the often noisy ecological datasets, he strives to account for uncertainty in datasets to make robust, honest scientific inference."
  },
  {
    "objectID": "about/contact.html",
    "href": "about/contact.html",
    "title": "Quantecol",
    "section": "",
    "text": "For inquiries about support with your projects, please get in touch via e-mail or call us at 0456 659 313."
  },
  {
    "objectID": "about/contact.html#contact",
    "href": "about/contact.html#contact",
    "title": "Quantecol",
    "section": "",
    "text": "For inquiries about support with your projects, please get in touch via e-mail or call us at 0456 659 313."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Quantecol",
    "section": "",
    "text": "We provide custom services tailored to the requirements of the client. This ranges from brief consultations about methodology or analysis, to conducting full data analyses and scientific reporting. We offer:\n\nMethodological guidance\nConsultations about study design\nValidation of statistical analysis\nTutorials and workshops for specific statistical methods\nCollaboration on research projects\nData and output visualisation\n\nWe prefer to take a Bayesian approach because of the flexibility in building custom models, straightforward quantification of uncertainty, and ability to incorporate domain expertise when available. Our capabilities include, but are not limited to, the following statistical models:\n\n(Generalised) linear models\nPower analysis and simulations\nMultivariate analysis\nDose-response models\nOrdinal regression\nHidden Markov models\nItem-response models\nGaussian processes\nEcological models\n\nMark-recapture and multievent\n(Dynamic) occupancy, including co-occurrence\n(Dynamic) N- and multinomial-mixture\nSpatial mark-recapture\nDistance sampling\nSpecies distribution models\nContinuous time models\n\n\nIf you are interested in working with Quantecol, please get in touch."
  },
  {
    "objectID": "services.html#services",
    "href": "services.html#services",
    "title": "Quantecol",
    "section": "",
    "text": "We provide custom services tailored to the requirements of the client. This ranges from brief consultations about methodology or analysis, to conducting full data analyses and scientific reporting. We offer:\n\nMethodological guidance\nConsultations about study design\nValidation of statistical analysis\nTutorials and workshops for specific statistical methods\nCollaboration on research projects\nData and output visualisation\n\nWe prefer to take a Bayesian approach because of the flexibility in building custom models, straightforward quantification of uncertainty, and ability to incorporate domain expertise when available. Our capabilities include, but are not limited to, the following statistical models:\n\n(Generalised) linear models\nPower analysis and simulations\nMultivariate analysis\nDose-response models\nOrdinal regression\nHidden Markov models\nItem-response models\nGaussian processes\nEcological models\n\nMark-recapture and multievent\n(Dynamic) occupancy, including co-occurrence\n(Dynamic) N- and multinomial-mixture\nSpatial mark-recapture\nDistance sampling\nSpecies distribution models\nContinuous time models\n\n\nIf you are interested in working with Quantecol, please get in touch."
  },
  {
    "objectID": "about/vision.html",
    "href": "about/vision.html",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol is a statistical consulting firm started in 2022 with the vision of providing clients with robust study designs and rigorous data analysis. We understand that every dataset is unique, often requiring custom model configurations to disentangle the signals form the noise. Our view is that ethical scientific behaviour involves honest reporting of uncertainty to buffer against spurious conclusions, while simultaneously extracting as much as possible from our data. Our aim is to work collaboratively with a range of researchers to communicate research output with a high degree of confidence."
  },
  {
    "objectID": "about/vision.html#our-vision",
    "href": "about/vision.html#our-vision",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol is a statistical consulting firm started in 2022 with the vision of providing clients with robust study designs and rigorous data analysis. We understand that every dataset is unique, often requiring custom model configurations to disentangle the signals form the noise. Our view is that ethical scientific behaviour involves honest reporting of uncertainty to buffer against spurious conclusions, while simultaneously extracting as much as possible from our data. Our aim is to work collaboratively with a range of researchers to communicate research output with a high degree of confidence."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol offers a range of methodological and analytical services for efficient, effective, and honest scientific inference.\n\n\nData forms the foundation of scientific studies but not all data are created equal. Statistical models make non-trivial assumptions about how the data were collected which influence the validity of your conclusions. We can provide you with study designs tailored to your system, with recommendations for appropriate sample sizes, surveying schemes, and more.\n\n\n\nYour datasets often contain a wealth of information but complex models are frequently required to reveal these signals. We can help you with this process, ranging from tutorials and workshops for applying statistical methodology, to conducting the entire analysis for you in a clear, collaborative fashion.\n\nmodel {\n  // likelihood of diagnostic infection intensities\n  for (j in 1:n_x) {\n    target += lognormal_lupdf(x[j] | log_n[ind[j]][sec[j], prim[j]], sigma[3]);\n  }\n  \n  // likelihood of multievent model (threading)\n  target += reduce_sum(partial_sum_lupmf, seq_ind, grainsize, y, q, \n                       n_prim, n_sec, first, last, first_sec, tau, temp, \n                       phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, m, n_z, sigma);\n}\n\n\n\n\nCommunication of key results derived from our data is what advances scientific fields. We believe that honest reporting involves an appropriate display of uncertainty and does not hide the raw data. The services we offer are predicated on transparency and scientific integrity."
  },
  {
    "objectID": "index.html#what-we-do",
    "href": "index.html#what-we-do",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol offers a range of methodological and analytical services for efficient, effective, and honest scientific inference.\n\n\nData forms the foundation of scientific studies but not all data are created equal. Statistical models make non-trivial assumptions about how the data were collected which influence the validity of your conclusions. We can provide you with study designs tailored to your system, with recommendations for appropriate sample sizes, surveying schemes, and more.\n\n\n\nYour datasets often contain a wealth of information but complex models are frequently required to reveal these signals. We can help you with this process, ranging from tutorials and workshops for applying statistical methodology, to conducting the entire analysis for you in a clear, collaborative fashion.\n\nmodel {\n  // likelihood of diagnostic infection intensities\n  for (j in 1:n_x) {\n    target += lognormal_lupdf(x[j] | log_n[ind[j]][sec[j], prim[j]], sigma[3]);\n  }\n  \n  // likelihood of multievent model (threading)\n  target += reduce_sum(partial_sum_lupmf, seq_ind, grainsize, y, q, \n                       n_prim, n_sec, first, last, first_sec, tau, temp, \n                       phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, m, n_z, sigma);\n}\n\n\n\n\nCommunication of key results derived from our data is what advances scientific fields. We believe that honest reporting involves an appropriate display of uncertainty and does not hide the raw data. The services we offer are predicated on transparency and scientific integrity."
  },
  {
    "objectID": "portfolio/christmas-island.html",
    "href": "portfolio/christmas-island.html",
    "title": "Reintroducing Extinct in the Wild reptiles",
    "section": "",
    "text": "Dr. Jon-Paul Emery and colleagues conducted experimental releases of the Extinct in the Wild (EW) blue-tailed skinks (Cryptoblepharus egeriae) and Lister’s geckos (Lepidodactylus listeri) on Christmas Island. These endemic reptiles disappeared from the wild following the establishment of the invasive common wolf snake (Lycodon capucinus) on the island in the 1980s. In 2018–2019, 170 skinks and 160 geckos were released into a predator-free area after which they were monitored with robust design mark-recapture surveys for one year post-release.\nQuantecol got involved with the analysis of the survey data, where we built a continuous time Jolly-Seber model to estimate per-capita recruitment, mortality rates of juvenile and adult skinks and geckos, and population sizes. Blue-tailed skinks flourished with high survival and recruitment rates leading to an increasing population size. Unfortuantely, Lister’s geckos did not fare so well with a steadily decreasing population size and no evidence of recruitment.\nThe work was published today as an Open Access article in Animal Conservation and represents an important contribution to conservation science."
  },
  {
    "objectID": "portfolio/multievent.html",
    "href": "portfolio/multievent.html",
    "title": "Novel multievent mark-recapture model",
    "section": "",
    "text": "Matthijs Hollanders and Dr. J. Andrew Royle recently developed a novel multievent mark-recapture model that accounts for infection state assignment errors. By estimating the false-negative and false-positive probabilities in the disease detection protocols, these errors can be propagated and accounted for while estimating the ecological process of interest. They used simulations and a case study with Fleay’s barred frog (Mixophyes fleayi) infected with the amphibian chytrid fungus Batrachochytrium dendrobatidis as a case study.\nThey found that infection prevalence was underestimated by \\(\\frac{1}{3}\\) while the rates of gaining and clearing infections were overestimated by factors of 4–5 when state misclassification was not accounted for. This was largely due to the limited ability of swab samples to detect low-level infections of the chytrid fungus.\nThe research was published as an Open Access article in Methods in Ecology and Evolution, and includes code to simulate and analyse your own datasets in the Supporting Information and on GitHub."
  }
]