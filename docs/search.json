[
  {
    "objectID": "portfolio/fleayi-adult.html",
    "href": "portfolio/fleayi-adult.html",
    "title": "Amphibian disease resistance",
    "section": "",
    "text": "Dr. Matthijs Hollanders et al. conducted a four-year mark-recapture study on Fleay’s barred frogs (Mixophyes fleayi) across three sites in northern New South Wales, Australia, to assess their current susceptibility to the amphibian disease chytridiomycosis.\nThe research was recently published as an open access article in Ecological Applications, where they argue that this species shows hallmarks of the evolution of pathogen resistance. Using disease-structured multistate models incorporating individual pathogen loads measured during recapture events, they show that although some individuals likely succumb to the disease, overall mortality was low and frogs were much more likely to clear their infections than to gain them. Good news!\nThis research was featured in The Conversation and ABC News."
  },
  {
    "objectID": "portfolio/multievent.html",
    "href": "portfolio/multievent.html",
    "title": "Novel multievent mark-recapture model",
    "section": "",
    "text": "Dr. Matthijs Hollanders and Dr. J. Andrew Royle recently developed a novel multievent mark-recapture model that accounts for infection state assignment errors. By estimating the false-negative and false-positive probabilities in the disease detection protocols, these errors can be propagated and accounted for while estimating the ecological process of interest. They used simulations and a case study with Fleay’s barred frog (Mixophyes fleayi) infected with the amphibian chytrid fungus Batrachochytrium dendrobatidis as a case study.\nThey found that infection prevalence was underestimated by \\(\\frac{1}{3}\\) while the rates of gaining and clearing infections were overestimated by factors of 4–5 when state misclassification was not accounted for. This was largely due to the limited ability of swab samples to detect low-level infections of the chytrid fungus.\nThe research was published as an Open Access article in Methods in Ecology and Evolution, and includes code to simulate and analyse your own datasets in the Supporting Information and on GitHub."
  },
  {
    "objectID": "portfolio/christmas-island.html",
    "href": "portfolio/christmas-island.html",
    "title": "Reintroducing Extinct in the Wild reptiles",
    "section": "",
    "text": "Dr. Jon-Paul Emery and colleagues conducted experimental releases of the Extinct in the Wild (EW) blue-tailed skinks (Cryptoblepharus egeriae) and Lister’s geckos (Lepidodactylus listeri) on Christmas Island. These endemic reptiles disappeared from the wild following the establishment of the invasive common wolf snake (Lycodon capucinus) on the island in the 1980s. In 2018–2019, 170 skinks and 160 geckos were released into a predator-free area after which they were monitored with robust design mark-recapture surveys for one year post-release.\nDr. Matthijs Hollanders from Quantecol got involved with the analysis of the survey data, where we built a continuous time Jolly-Seber model to estimate per-capita recruitment, mortality rates of juvenile and adult skinks and geckos, and population sizes. Blue-tailed skinks flourished with high survival and recruitment rates leading to an increasing population size. Unfortuantely, Lister’s geckos did not fare so well with a steadily decreasing population size and no evidence of recruitment.\nThe work was published today as an Open Access article in Animal Conservation and represents an important contribution to conservation science."
  },
  {
    "objectID": "blog/marginalisation.html",
    "href": "blog/marginalisation.html",
    "title": "Marginalising discrete variables in ecological models",
    "section": "",
    "text": "Bayesian methods are popular for ecological modeling because of the flexibility afforded by software like JAGS, NIMBLE, and Stan. JAGS and NIMBLE are derivatives of the BUGS (Bayesian inference Using Gibbs Sampling) language and, unlike Stan, permit discrete parameters. Discrete parameters are common in conditional likelihood parameterisations of many ecological models, such as the alive state in mark-recapture, occupancy in occupancy models, and abundance in N-mixture models. All of these models can be expressed in the marginal likelihood form where the likelihood is expressed without the discrete parameters, effectively integrating over the possible values of each parameter.\n\n\n\n\n\n\nMarginal and conditional likelihoods\n\n\n\nWhen we speak about conditional likelihoods in the context of ecological models, we express the likelihood of the data \\(y\\) as a function of parameters \\(\\theta\\) and (partially) hidden discrete states \\(Z\\), denoted \\(p_c\\left(y \\mid \\theta, Z\\right)\\). The marginal likelihood expresses the likelihood conditional on the parameters without the discrete states, thus marginalising them from the model, \\(p_m\\left(y \\mid \\theta\\right)\\)\n\n\nMarginalisation leads to faster estimation and, ironically, better exploration of the posterior distributions of the underlying discrete states compared to sampling them directly. The downside is that these models can be more difficult to code or understand, because the marginal likelihood parameterisations don’t reflect the data-generating process as closely as the conditional parameterisations. Ironically, all of these ecological models were first constructed in the marginalised form when frequentist methods dominated the field. Although the conditional parameterisations facilitated the on-boarding of many ecologists that were less stats-savvy (like myself), generally the marginalised forms should be preferred. And when using gradient-based MCMC methods such as the No U-Turn Sampler (NUTS) implemented in Stan, you don’t have a choice.\nWhen I became interested in statistics during my PhD I was using JAGS and subsequently NIMBLE. I always knew I wanted to learn Stan but I found the marginalisation of discrete parameters daunting. Once I decided to bite the bullet, I quickly realised it wasn’t so hard, and it culminated in my first blog post about implementing hidden Markov models in Stan. In this post, I’ll focus on two types of popular models that are generally not constructed as hidden Markov models: occupancy and N-mixture models. I’ll show how to quickly marginalise the latent states, how to recover posterior distributions for them after estimation, and explore different parameterisations and model types."
  },
  {
    "objectID": "blog/marginalisation.html#general-approach",
    "href": "blog/marginalisation.html#general-approach",
    "title": "Marginalising discrete variables in ecological models",
    "section": "General approach",
    "text": "General approach\nThe first approach I outline follows Equation 4 and computes the site-level log-likelihood in a for-loop. A few things to note about this and some upcoming programs:\n\nI’m using uniform priors for probabilities.\nWhen marginalising out discrete parameters, since we’re summing over the mutually exclusive probabilities, we have to keep track of our normalising constants. Therefore, we need to use the _lpmf instead of the _lupmf distributions. This doesn’t make a difference for the Bernoulli distribution because it doesn’t have constant terms, but it does for other examples.\nAlthough I’ve just simulated data with fixed occupancy and detection probabilities, these models facilitate site-level effects on occupancy and site-by-occasion effects on detection. Therefore, in the model block, I created a vector of occupancy probabilities and a matrix of detection probabilities (using the “intercepts” psi_a and p_a) to guide practitioners using this as a template for their own models.\nIn the model block, since we specify a log probability density in Stan, I pre-compute \\(\\log\\left(\\psi\\right)\\) and \\(\\log\\left(1 - \\psi\\right)\\), as this vectorised operation is more efficient than doing it for each site in the for-loop.\nIn the same vein, summing probabilities on the second line of Equation 3 equates to using the log_sum_exp() function in Stan when working with log probabilities.\nIn the generated quantities block I recover the posterior distributions for the latent occupancy states. I initialise an array of ones (all sites occupied), but then for sites without detections I use inverse transform sampling to sample from the normalised state-specific log probabilities.4 Since this sampling step uses the categorical_logit_rng() function which yields values 1 or 2 (associated with the respective log probabilities), I subtract 1 from the outcome to yield occupancy states 0 or 1.\n\n\n\nCode\n# load packages\nif (!require(pacman)) install.packages(\"pacman\")\npacman::p_load(cmdstanr, loo, tidyverse, tidybayes, posterior)\nchains &lt;- 8 ; iter_warmup &lt;- 200 ; iter_sampling &lt;- 500\noptions(mc.cores = chains)\n\n\n\n\nCode\ndata {\n  int&lt;lower=1&gt; I, J_max;  // number of sites and maximum surveys\n  array[I] int&lt;lower=1, upper=J_max&gt; J; // number of surveys\n  array[I, J_max] int&lt;lower=0, upper=1&gt; y;  // detection history\n}\n\ntransformed data {\n  array[I] int Q;  // number of detections\n  for (i in 1:I) {\n    Q[i] = sum(y[i, 1:J[i]]);\n  }\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; psi_a, p_a;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(psi_a | 1, 1);\n  target += beta_lupdf(p_a | 1, 1);\n  \n  // site-level occupancy and site-by-occasion level detection\n  vector[I] psi = rep_vector(psi_a, I);\n  matrix[I, J_max] p = rep_matrix(p_a, I, J_max);\n  \n  // log-transform occupancy probabilities\n  vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n  \n  // likelihood\n  for (i in 1:I) {\n    if (Q[i]) {\n      target += log_psi[i] + bernoulli_lpmf(y[i, 1:J[i]] | p[i, 1:J[i]]);\n    } else {\n      target += log_sum_exp(log1m_psi[i], \n                            log_psi[i] + bernoulli_lpmf(y[i, 1:J[i]] | p[i, 1:J[i]]));\n    }\n  }\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I] int z = ones_int_array(I);\n  {\n    vector[I] psi = rep_vector(psi_a, I);\n    matrix[I, J_max] p = rep_matrix(p_a, I, J_max);\n    vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n    vector[2] lp;\n    for (i in 1:I) {\n      if (Q[i]) {\n        log_lik[i] = log_psi[i] + bernoulli_lpmf(y[i, 1:J[i]] | p[i, 1:J[i]]);\n      } else {\n        lp = [ log1m_psi[i], \n               log_psi[i] + bernoulli_lpmf(y[i, 1:J[i]] | p[i, 1:J[i]]) ]';\n        log_lik[i] = log_sum_exp(lp);\n        z[i] = categorical_logit_rng(lp - log_lik[i]) - 1;\n      }\n    }\n  }\n}\n\n\nLet’s run the model using cmdstanr (Gabry et al. 2024). This samples well and fast, so we’ll run many iterations to compare speed.\n\n\nCode\nfit_occ &lt;- occ$sample(\n  data = list(I = I, J_max = J_max, J = J, y = y), \n  refresh = 0, chains = chains, \n  iter_warmup = 1e3, iter_sampling = 1e4\n)\n\n\nRunning MCMC with 8 parallel chains...\n\nChain 1 finished in 2.5 seconds.\nChain 4 finished in 2.5 seconds.\nChain 2 finished in 2.6 seconds.\nChain 3 finished in 2.6 seconds.\nChain 5 finished in 2.6 seconds.\nChain 8 finished in 2.6 seconds.\nChain 6 finished in 2.7 seconds.\nChain 7 finished in 2.9 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 2.6 seconds.\nTotal execution time: 3.0 seconds.\n\n\nCode\nfit_occ$summary(c(\"psi_a\", \"p_a\")) |&gt; \n  select(variable, median, contains(\"q\"), rhat, contains(\"ess\")) |&gt; \n  mutate(truth = c(psi, p))\n\n\n# A tibble: 2 × 8\n  variable median    q5   q95  rhat ess_bulk ess_tail truth\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 psi_a     0.656 0.520 0.834  1.00   25468.   21238.   0.6\n2 p_a       0.346 0.261 0.437  1.00   23766.   29475.   0.4\n\n\nCode\nfit_occ$loo()\n\n\n\nComputed from 80000 by 100 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -158.8 10.4\np_loo         1.9  0.2\nlooic       317.7 20.8\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.3, 0.8]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nWe can also get the proportion of sites that had the latent occupancy states estimated correctly, using the posterior median of each site’s occupancy state. Since we know \\(z=1\\) for sites with detections, we’ll only compute it for ambiguous sites.\n\n\nCode\nfit_occ |&gt; \n  spread_rvars(z[i]) |&gt; \n  mutate(truth = .env$z, \n         Q = rowSums(y)) |&gt; \n  filter(!Q) |&gt; \n  summarise(proportion_true = mean(median(z) == truth))\n\n\n# A tibble: 1 × 1\n  proportion_true\n            &lt;dbl&gt;\n1           0.811"
  },
  {
    "objectID": "blog/marginalisation.html#multinomial-likelihood",
    "href": "blog/marginalisation.html#multinomial-likelihood",
    "title": "Marginalising discrete variables in ecological models",
    "section": "Multinomial likelihood",
    "text": "Multinomial likelihood\nThere’s quicker ways to fit the same model, specifically with this intercepts-only model with the same number of surveys per site. This uses the multinomial likelihood by specifying cell probabilities for the possible detection histories, for which I’ll show the Stan program below. This approach leverages the definition that the number of surveys with detections follows a \\(\\textrm{Binomial}\\left(J, p\\right)\\) distribution and that sites have a finite number of possible detection histories.5 The cell probabilities correspond to the \\(J+1\\) possible outcomes of having made \\(y = 0, 1, \\dots, J\\) detections with corresponding likelihood \\(\\textrm{Binomial}\\left(y \\mid J, p\\right)\\), where only the sites with \\(y=0\\) detections need to have the probability of not being occupied incremented. We can use the multinomial_logit_lupmf function to increment the log density after normalising the cell probabilities on the log scale.\n\n\nCode\ndata {\n  int&lt;lower=1&gt; I, J;  // number of sites and surveys\n  array[I, J] int&lt;lower=0, upper=1&gt; y;  // detection history\n}\n\ntransformed data {\n  array[I] int Q;  // number of detections\n  for (i in 1:I) {\n    Q[i] = sum(y[i]);\n  }\n  int K = J + 1;  // number of possible detection histories\n  array[K] int P = zeros_int_array(K);  // category counts\n  for (i in 1:I) {\n    P[Q[i] + 1] += 1;\n  }\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; psi_a, p_a;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(psi_a | 1, 1);\n  target += beta_lupdf(p_a | 1, 1);\n  \n  // log probabilities with detections\n  vector[K] lp = rep_vector(log(psi_a), K);\n  for (k in 1:K) {\n    lp[k] += binomial_lpmf(k - 1 | J, p_a);\n  }\n  \n  // log probability without detections\n  lp[1] = log_sum_exp(log1m(psi_a), lp[1]);\n  \n  // increment target\n  target += multinomial_logit_lupmf(P | lp - log_sum_exp(lp));\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I] int z = ones_int_array(I);\n  {\n    real log_psi = log(psi_a), log1m_psi = log1m(psi_a);\n    row_vector[2] lp;\n    for (i in 1:I) {\n      if (Q[i]) {\n        log_lik[i] = log_psi + bernoulli_lpmf(y[i] | p_a);\n      } else {\n        lp = [ log1m_psi, log_psi + bernoulli_lpmf(y[i] | p_a) ];\n        log_lik[i] = log_sum_exp(lp);\n        z[i] = categorical_logit_rng(lp' - log_lik[i]) - 1;\n      }\n    }\n  }\n}\n\n\nThis runs even faster and of course recovers the same posterior. We still need the same computations as the first model in the generated quantities block if we want the site-level log-likelihood values and latent states.\n\n\nCode\nfit_occ_multi &lt;- occ_multi$sample(\n  data = list(I = I, J = J_max, y = y), \n  refresh = 0, chains = chains, \n  iter_warmup = 1e3, iter_sampling = 1e4\n)\n\n\nRunning MCMC with 8 parallel chains...\n\n\nChain 1 finished in 0.7 seconds.\nChain 2 finished in 0.7 seconds.\nChain 3 finished in 0.7 seconds.\nChain 4 finished in 0.8 seconds.\nChain 5 finished in 0.7 seconds.\nChain 6 finished in 0.7 seconds.\nChain 7 finished in 0.7 seconds.\nChain 8 finished in 0.7 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 0.7 seconds.\nTotal execution time: 0.9 seconds.\n\n\nCode\nloo_compare(fit_occ$loo(), fit_occ_multi$loo())\n\n\n       elpd_diff se_diff\nmodel2 0.0       0.0    \nmodel1 0.0       0.0    \n\n\nThe approaches outlined so far works well for single season occupancy. For any flavour of multi-season (dynamic) occupancy models, you’re much better off implementing the scaleable forward algorithm I describe in detail in my first post."
  },
  {
    "objectID": "blog/marginalisation.html#combining-occupancy-and-abundance",
    "href": "blog/marginalisation.html#combining-occupancy-and-abundance",
    "title": "Marginalising discrete variables in ecological models",
    "section": "Combining occupancy and abundance",
    "text": "Combining occupancy and abundance\nWhat if we’re counting individuals at sites but not all sites are occupied? This can be described by the following generative model, which is essentially an N-mixture model nested in an occupancy model, or a zero-inflated N-mixture model:\n\\[\n\\begin{aligned}\n  z_i &\\sim \\textrm{Bernoulli} \\left(\\psi\\right) \\\\\n  N_i &\\sim \\textrm{Poisson}\\left(z_i \\cdot \\lambda\\right) \\\\\n  y_{ij} &\\sim \\textrm{Binomial}\\left(N_i, p\\right)\n\\end{aligned}\n\\tag{10}\\]\nThe conditional likelihood for the site-level vector of counts \\(\\boldsymbol{y}_i\\) is:\n\\[\n\\begin{aligned}\np_c\\left(\\boldsymbol{y}_i \\mid \\psi, \\lambda, p, z_i, N_i\\right) = \\ & \\textrm{Bernoulli}\\left(z_i \\mid \\psi\\right) \\\\\n&\\cdot \\textrm{Poisson}\\left(N_i \\mid z_i \\cdot \\lambda\\right) \\\\\n&\\cdot \\prod_{j=1}^{J_i} \\textrm{Binomial}\\left(y_{ij} \\mid N_i, p\\right)\n\\end{aligned}\n\\tag{11}\\]\nThe full marginal likelihood is unwieldy to write, but if we define the expression in Equation 9 as \\(\\theta\\), then the marginal likelihood can be written analogously to our starting occupancy model in Equation 4 as follows:\n\\[\np_m\\left(\\boldsymbol{y}_i \\mid \\psi, \\lambda, p\\right) = \\begin{cases}\n1 - \\psi + \\psi \\cdot \\theta, &\\text{if } Q_i = 0, \\\\\n\\psi \\cdot \\theta, &\\text{if } Q_i &gt; 0,\n\\end{cases}\n\\tag{12}\\]\nwhere \\(Q_i = \\sum_{j=1}^{J_i} y_{ij}\\) is the total number of individuals detected across surveys. When marginalising discrete parameters in several hierarchical levels, you should first compute the lower levels (abundance in this case) and then increment the higher levels (occupancy). Let’s simulate some data to start.\n\n\nCode\n# parameters\npsi &lt;- 0.8\n\n# simulate\nz &lt;- rbinom(I, 1, psi)\nN &lt;- rpois(I, z * lambda)\ny &lt;- matrix(0, I, J_max)\nfor (i in 1:I) {\n  y[i, 1:J[i]] &lt;- rbinom(J[i], N[i], p)\n}\n\n\nIn this Stan program, we don’t need to account for 0 abundance because this is implied for unoccupied sites. So for the cases where \\(\\max\\left(y_i\\right)=0\\), we compute the log probabilities of population sizes starting at 1. I’ve created a new indicator in transformed data to flag the first \\(k\\) to marginalise over. Also notice that in the generated quantities block we only simulate population sizes \\(N_i\\) if sites are occupied.\n\n\nCode\ndata {\n  int&lt;lower=0&gt; I, J_max, K;  // number of sites and maximum surveys and population size\n  array[I] int&lt;lower=1, upper=J_max&gt; J;  // number of surveys\n  array[I, J_max] int&lt;lower=0&gt; y;  // detection history\n}\n\ntransformed data {\n  array[I] int max_y, first_k;  // maximum counts and lowest population size\n  for (i in 1:I) {\n    max_y[i] = max(y[i, 1:J[i]]);\n    first_k[i] = max({max_y[i], 1});\n  }\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; psi_a, p_a;\n  real&lt;lower=0&gt; lambda_a;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(psi_a | 1, 1);\n  target += exponential_lupdf(lambda_a | 0.01);\n  target += beta_lupdf(p_a | 1, 1);\n  \n  // site-level expected counts and site-by-occasion level detection\n  vector[I] psi = rep_vector(psi_a, I);\n  vector[I] lambda = rep_vector(lambda_a, I);\n  matrix[I, J_max] p = rep_matrix(p_a, I, J_max);\n  \n  // log-transform occupancy probabilities\n  vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n  \n  // likelihood\n  vector[K] lp;\n  real theta;\n  for (i in 1:I) {\n    for (k in first_k[i]:K) {\n      lp[k] = poisson_lpmf(k | lambda[i]) \n              + binomial_lpmf(y[i, 1:J[i]] | k, p[i, 1:J[i]]);\n    }\n    theta = log_sum_exp(lp[first_k[i]:K]);\n    \n    // increment occupancy\n    if (max_y[i]) {\n      target += log_psi[i] + theta;\n    } else {\n      target += log_sum_exp(log1m_psi[i], log_psi[i] + theta);\n    }\n  }\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I] int z = ones_int_array(I), N = zeros_int_array(I);\n  {\n    vector[I] psi = rep_vector(psi_a, I);\n    vector[I] lambda = rep_vector(lambda_a, I);\n    matrix[I, J_max] p = rep_matrix(p_a, I, J_max);\n    vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n    vector[K] lp;\n    real theta;\n    vector[2] lp_z;\n    for (i in 1:I) {\n      for (k in first_k[i]:K) {\n        lp[k] = poisson_lpmf(k | lambda[i]) \n                + binomial_lpmf(y[i, 1:J[i]] | k, p[i, 1:J[i]]);\n      }\n      theta = log_sum_exp(lp[first_k[i]:K]);\n      if (max_y[i]) {\n        log_lik[i] = log_psi[i] + theta;\n        N[i] = categorical_logit_rng(lp[first_k[i]:K] - theta) + max_y[i] - 1;\n      } else {\n        lp_z = [ log1m_psi[i], log_psi[i] + theta ]';\n        log_lik[i] = log_sum_exp(lp_z);\n        z[i] = categorical_logit_rng(lp_z - log_lik[i]) - 1;\n        if (z[i]) {\n          N[i] = categorical_logit_rng(lp[first_k[i]:K] - theta) + max_y[i] - 1;\n        }\n      }\n    }\n  }\n}\n\n\nAfter running this model we see estimation went well and latent occupancy states had great recovery. There’s a lot to be said for using counts in the observation models of occupancy models as it provides much more information about \\(\\psi\\) than a Binomial likelihood, because collapsing counts to 1s and 0s always discards information. But this might be the subject of a future blog post.\n\n\nCode\nfit_zi_nmix &lt;- zi_nmix$sample(\n  data = list(I = I, J_max = J_max, K = 3 * max(N), J = J, y = y), \n  refresh = 0, chains = chains, \n  iter_warmup = iter_warmup, iter_sampling = iter_sampling\n)\n\n\nRunning MCMC with 8 parallel chains...\n\nChain 5 finished in 22.4 seconds.\nChain 1 finished in 22.6 seconds.\nChain 2 finished in 22.8 seconds.\nChain 4 finished in 25.5 seconds.\nChain 3 finished in 27.6 seconds.\nChain 6 finished in 27.4 seconds.\nChain 7 finished in 27.9 seconds.\nChain 8 finished in 28.2 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 25.5 seconds.\nTotal execution time: 28.4 seconds.\n\n\nCode\nfit_zi_nmix$summary(c(\"psi_a\", \"lambda_a\", \"p_a\")) |&gt; \n  select(variable, median, contains(\"q\"), rhat, contains(\"ess\")) |&gt; \n  mutate(truth = c(psi, lambda, p))\n\n\n# A tibble: 3 × 8\n  variable median    q5    q95  rhat ess_bulk ess_tail truth\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 psi_a     0.804 0.731  0.869  1.00    1742.    1955.   0.8\n2 lambda_a  6.82  4.81  11.7    1.01    1151.    1069.   8  \n3 p_a       0.232 0.136  0.328  1.01    1159.    1079.   0.2\n\n\nCode\n# proportion of sites correctly classified\nfit_zi_nmix |&gt; \n  spread_rvars(z[i]) |&gt; \n  mutate(truth = .env$z, \n         Q = rowSums(y) &gt; 0) |&gt; \n  filter(!Q) |&gt; \n  summarise(proportion_true = mean(median(z) == truth))\n\n\n# A tibble: 1 × 1\n  proportion_true\n            &lt;dbl&gt;\n1               1"
  },
  {
    "objectID": "blog/marginalisation.html#time-to-event-n-mixture-model",
    "href": "blog/marginalisation.html#time-to-event-n-mixture-model",
    "title": "Marginalising discrete variables in ecological models",
    "section": "Time-to-event N-mixture model",
    "text": "Time-to-event N-mixture model\nThe final model I’ll consider is a type of continuous time model.6 Time-to-event N-mixture models can be used to estimate abundance under the assumption that detection rates are proportional to abundance (Strebel et al. 2021). Consider sites \\(i \\in 1:I\\) surveyed \\(j \\in 1:J_i\\) times for a minimum of length of \\(\\Delta_{ij}\\) (e.g., 30 minutes).7 Surveys are conducted up to \\(\\Delta_{ij}\\) or are terminated after the time of first detection, yielding right-censored waiting times data \\(y_{ij} = \\Delta_{ij}\\) if no detection were made or \\(y_{ij} &lt; \\Delta_{ij}\\) if a detection was made. The waiting times are then modeled with some appropriate distribution such as the exponential. We can just replace the observation model in the zero-inflated N-mixture model in Equation 10 by modeling the waiting times as a function of the product of the per-individual detection rate \\(\\mu\\) and site-level abundance \\(N_i\\), yielding the following data-generating process:\n\\[\n\\begin{aligned}\n  z_i &\\sim \\textrm{Bernoulli} \\left(\\psi\\right) \\\\\n  N_i &\\sim \\textrm{Poisson}\\left(z_i \\cdot \\lambda\\right) \\\\\n  y_{ij} &\\sim \\min\\left(\\textrm{Exponential}\\left(N_i \\cdot \\mu\\right), \\Delta_{ij}\\right)\n\\end{aligned}\n\\tag{13}\\]\nJust out of curiosity, I’m going to use the same parameters as for first zero-inflated N-mixture model above, leveraging the fact that \\(\\mu = -\\log\\left(1 - p\\right)\\).8 Since I’m trying to compare these models, I’ll assume that \\(\\Delta = 1\\) for all surveys, but this doesn’t have to be case for these models.\n\n\nCode\n# convert detection probability to rate\nmu &lt;- -log(1 - p)\n\n# simulate\nDelta_max &lt;- 1\ny &lt;- Delta &lt;- matrix(Delta_max, I, J_max)\nfor (i in 1:I) {\n  if (N[i]) {\n    # pmin() is vectorised min() function \n    y[i, 1:J[i]] &lt;- pmin(rexp(J[i], N[i] * mu), \n                         Delta[i, 1:J[i]])\n  }\n}\n\n\nThe Stan program uses the log density function exponential_lpdf when detections were made (y[i, j] &lt; Delta[i, j]), and the complement of the cumulative density function exponential_lccdf if no detections were made. The latter gives the log probability of not making any detections during the survey of length \\(\\Delta_{ij}\\). Therefore, we can’t vectorise over the surveys because the density function depends on whether or not there was a detection in each survey. Since occupied sites need to have a population size of at least 1 individual, we marginalise over abundances \\(N_i \\in 1:K\\).\n\n\nCode\ndata {\n  int&lt;lower=0&gt; I, J_max, K, Delta_max;  // number of sites and maximum surveys, population size, and survey length\n  array[I] int&lt;lower=1, upper=J_max&gt; J;  // number of surveys\n  matrix&lt;lower=0&gt;[I, J_max] Delta, y;  // survey lengths and detection history\n}\n\ntransformed data {\n  array[I, J_max] int Q;  // indicator for detection\n  array[I] int Q_sum = zeros_int_array(I);  // number of detections\n  for (i in 1:I) {\n    for (j in 1:J[i]) {\n      Q[i, j] = y[i, j] &lt; Delta[i, j];\n    }\n    Q_sum[i] = sum(Q[i, 1:J[i]]);\n  }\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; psi_a;\n  real&lt;lower=0&gt; lambda_a, mu_a;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(psi_a | 1, 1);\n  target += exponential_lupdf(lambda_a | 0.01);\n  target += exponential_lupdf(mu_a | 0.1);\n  \n  // site-level expected counts and site-by-occasion level detection\n  vector[I] psi = rep_vector(psi_a, I);\n  vector[I] lambda = rep_vector(lambda_a, I);\n  matrix[I, J_max] mu = rep_matrix(mu_a, I, J_max);\n  \n  // log-transform occupancy probabilities\n  vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n  \n  // likelihood\n  vector[K] lp;\n  real theta;\n  for (i in 1:I) {\n    \n    // abundance\n    for (k in 1:K) {\n      lp[k] = poisson_lpmf(k | lambda[i]);\n      for (j in 1:J[i]) {\n        if (Q[i, j]) {\n          lp[k] += exponential_lpdf(y[i, j] | k * mu[i, j]);\n        } else {\n          lp[k] += exponential_lccdf(Delta[i, j] | k * mu[i, j]);\n        }\n      }\n    }\n    theta = log_sum_exp(lp);\n    \n    // increment occupancy\n    if (Q_sum[i]) {\n      target += log_psi[i] + theta;\n    } else {\n      target += log_sum_exp(log1m_psi[i], log_psi[i] + theta);\n    }\n  }\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I] int z = ones_int_array(I), N = zeros_int_array(I);\n  {\n    vector[I] psi = rep_vector(psi_a, I);\n    vector[I] lambda = rep_vector(lambda_a, I);\n    matrix[I, J_max] mu = rep_matrix(mu_a, I, J_max);\n    vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n    real theta;\n    vector[K] lp;\n    vector[2] lp_z;\n    for (i in 1:I) {\n      for (k in 1:K) {\n        lp[k] = poisson_lpmf(k | lambda[i]);\n        for (j in 1:J[i]) {\n          if (Q[i, j]) {\n            lp[k] += exponential_lpdf(y[i, j] | k * mu[i, j]);\n          } else {\n            lp[k] += exponential_lccdf(Delta[i, j] | k * mu[i, j]);\n          }\n        }\n      }\n      theta = log_sum_exp(lp);\n      if (Q_sum[i]) {\n        log_lik[i] = log_psi[i] + theta;\n        N[i] = categorical_logit_rng(lp - theta);\n      } else {\n        lp_z = [ log1m_psi[i], log_psi[i] + theta ]';\n        log_lik[i] = log_sum_exp(lp_z);\n        z[i] = categorical_logit_rng(lp_z - log_lik[i]) - 1;\n        if (z[i]) {\n          N[i] = categorical_logit_rng(lp - theta);\n        }\n      }\n    }\n  }\n}\n\n\nLet’s fit the time-to-event model, check the posteriors, and plot a subset of the abundance estimates alongside those of the first zero-inflated N-mixture model.\n\n\nCode\nfit_tte &lt;- tte$sample(\n  data = list(I = I, J_max = J_max, K = 3 * max(N), Delta_max = Delta_max, J = J, Delta = Delta, y = y), \n  refresh = 0, chains = chains, \n  iter_warmup = iter_warmup, iter_sampling = iter_sampling\n)\n\n\nRunning MCMC with 8 parallel chains...\n\n\nChain 3 finished in 12.7 seconds.\nChain 1 finished in 13.2 seconds.\nChain 8 finished in 13.2 seconds.\nChain 2 finished in 14.1 seconds.\nChain 6 finished in 14.4 seconds.\nChain 5 finished in 14.7 seconds.\nChain 7 finished in 14.7 seconds.\nChain 4 finished in 15.1 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 14.0 seconds.\nTotal execution time: 15.3 seconds.\n\n\nCode\nfit_tte$summary(c(\"psi_a\", \"lambda_a\", \"mu_a\")) |&gt; \n  select(variable, median, contains(\"q\"), rhat, contains(\"ess\")) |&gt; \n  mutate(truth = c(psi, lambda, mu))\n\n\n# A tibble: 3 × 8\n  variable  median     q5    q95  rhat ess_bulk ess_tail truth\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 psi_a     0.784  0.710   0.849  1.00    1584.    1246. 0.8  \n2 lambda_a 15.6    5.97   31.0    1.01     808.    1522. 8    \n3 mu_a      0.0991 0.0490  0.267  1.01     807.    1539. 0.223\n\n\n\n\nCode\nout &lt;- map(list(fit_zi_nmix, fit_tte), \n    ~(spread_rvars(.x, z[i], N[i]))) |&gt; \n  list_rbind(names_to = \"model\") |&gt; \n  mutate(model = factor(model, labels = c(\"Counts\", \"Time-to-event\"))) |&gt; \n  left_join(tibble(i = 1:I, truth = N), by = \"i\")\n\nout |&gt; \n  filter(i &lt;= 25) |&gt; \n  ggplot(aes(xdist = N, y = factor(i) |&gt; fct_rev())) + \n  facet_wrap(~ model) + \n  stat_pointinterval(point_interval = median_qi, .width = 0.9, size = 0.5, linewidth = 0.5) + \n  geom_point(aes(x = truth), \n             position = position_nudge(y = -0.3), \n             size = 2, \n             colour = green, \n             shape = \"cross\") + \n  labs(x = expression(N), \n       y = \"Occupied Site\", \n       colour = \"Model\")\n\n\n\n\n\n\n\n\n\nThe estimates for the count model look better and seemed to perform better according to RMSE of the population sizes, which makes sense because the count model can provide more information (number of individuals detected) compared to the time-to-event model (waiting time to the first individual detected). Since the \\(\\textrm{Binomial}\\left(N_i, p\\right)\\) distribution converges to \\(\\textrm{Poisson}\\left(N_i \\cdot p\\right)\\) for sufficiently large \\(N\\) and sufficiently small \\(p\\)9, and because \\(p \\approx \\mu\\) for small values of \\(p\\), researchers gain statistical power by not terminating the search after first detection but instead counting the number of individuals during the interval \\(\\Delta_{ij}\\). But of course, one of the big draws of the time-to-event model is less time in the field; you win some, you lose some.\n\n\nCode\nout |&gt; \n  filter(median(z) == 1) |&gt; \n  summarise(RMSE = sqrt(rvar_mean((truth - N)^2)), \n            .by = model)\n\n\n# A tibble: 2 × 2\n  model                RMSE\n  &lt;fct&gt;          &lt;rvar[1d]&gt;\n1 Counts          3.5 ± 1.3\n2 Time-to-event  10.5 ± 6.9\n\n\nAt least our occupancy states are still estimated well.\n\n\nCode\nfit_tte |&gt; \n  spread_rvars(z[i]) |&gt; \n  mutate(truth = .env$z, \n         Q = rowSums(y) &lt; J * Delta_max) |&gt; \n  filter(!Q) |&gt; \n  summarise(proportion_true = mean(median(z) == truth))\n\n\n# A tibble: 1 × 1\n  proportion_true\n            &lt;dbl&gt;\n1           0.913\n\n\nThanks for reading!"
  },
  {
    "objectID": "blog/marginalisation.html#footnotes",
    "href": "blog/marginalisation.html#footnotes",
    "title": "Marginalising discrete variables in ecological models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe number of surveys can vary by site, which is the norm rather than the exception in ecological data.↩︎\nWe do know that sites with detections are occupied.↩︎\nBoth \\(\\psi\\) and \\(p\\) can be modeled as functions of covariates or random effects at the site-level (occupancy) or the site-by-survey level (detection).↩︎\nIn order to get the posterior distribution of that site’s latent occupancy state \\(z_i\\), we need to sample the states relative to each state’s associated probabilities. We use the categorical_(logit_)rng() function(s) for this in Stan, which requires that the vector of probabilities or log probabilities is normalised. Normalising a vector of probabilities is achieved by dividing by the sum, and for a vector of log probabilities by subtracting the log_sum_exp().↩︎\nWe could still formulate this with varying number of surveys, except that now the cell probabilities have to be expanded to not only account for the \\(J + 1\\) possible outcomes but also the variation in \\(J_i\\) itself.↩︎\nThe traditional N-mixture model can become a continuous time model by incorporating the survey lengths into the observation model.↩︎\nThe survey times can vary by site and occasion.↩︎\nProbabilities can be converted to rates like this more generally, such as survival probabilities as a function of instantaneous mortality hazard rates \\(S = 1 - \\exp\\left(-h\\right)\\) (Ergon et al. 2018).↩︎\nSee Wikipedia for some rules of thumb.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol offers a range of methodological and analytical services for efficient, effective, and honest scientific inference.\n\n\nWe can save you time and money starting from the inception of a research question. Data forms the foundation of scientific studies, and statistical models make critical assumptions about how the data were collected which influence the validity of your conclusions. We can provide you with study designs tailored to your system, with recommendations for appropriate sample sizes, surveying schemes, and more.\n\n\n\nYour datasets often contain a wealth of information but complex models are frequently required to reveal these signals. We can help you with this process, ranging from tutorials and workshops for applying statistical methodology, to conducting the entire analysis for you in a clear, collaborative fashion.\n\nmodel {\n  // likelihood of diagnostic infection intensities\n  for (j in 1:n_x) {\n    target += lognormal_lupdf(x[j] | log_n[ind[j]][sec[j], prim[j]], sigma[3]);\n  }\n  \n  // likelihood of multievent model (threading)\n  target += reduce_sum(partial_sum_lupmf, seq_ind, grainsize, y, q, \n                       n_prim, n_sec, first, last, first_sec, tau, temp, \n                       phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, m, n_z, sigma);\n}\n\n\n\n\nCommunication of key results derived from our data is what advances scientific fields. We believe that honest reporting involves an appropriate display of uncertainty and does not hide the raw data. The services we offer are predicated on transparency and scientific integrity."
  },
  {
    "objectID": "index.html#what-we-do",
    "href": "index.html#what-we-do",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol offers a range of methodological and analytical services for efficient, effective, and honest scientific inference.\n\n\nWe can save you time and money starting from the inception of a research question. Data forms the foundation of scientific studies, and statistical models make critical assumptions about how the data were collected which influence the validity of your conclusions. We can provide you with study designs tailored to your system, with recommendations for appropriate sample sizes, surveying schemes, and more.\n\n\n\nYour datasets often contain a wealth of information but complex models are frequently required to reveal these signals. We can help you with this process, ranging from tutorials and workshops for applying statistical methodology, to conducting the entire analysis for you in a clear, collaborative fashion.\n\nmodel {\n  // likelihood of diagnostic infection intensities\n  for (j in 1:n_x) {\n    target += lognormal_lupdf(x[j] | log_n[ind[j]][sec[j], prim[j]], sigma[3]);\n  }\n  \n  // likelihood of multievent model (threading)\n  target += reduce_sum(partial_sum_lupmf, seq_ind, grainsize, y, q, \n                       n_prim, n_sec, first, last, first_sec, tau, temp, \n                       phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, m, n_z, sigma);\n}\n\n\n\n\nCommunication of key results derived from our data is what advances scientific fields. We believe that honest reporting involves an appropriate display of uncertainty and does not hide the raw data. The services we offer are predicated on transparency and scientific integrity."
  },
  {
    "objectID": "about/vision.html",
    "href": "about/vision.html",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol is a statistical consulting firm started in 2022 with the vision of providing clients with robust study designs and rigorous data analysis. We understand that every dataset is unique, often requiring custom model configurations to disentangle the signals form the noise. Our view is that ethical scientific behaviour involves honest reporting of uncertainty to buffer against spurious conclusions, while simultaneously extracting as much as possible from our data. Our aim is to work collaboratively with a range of researchers to communicate research output with a high degree of confidence."
  },
  {
    "objectID": "about/vision.html#our-vision",
    "href": "about/vision.html#our-vision",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol is a statistical consulting firm started in 2022 with the vision of providing clients with robust study designs and rigorous data analysis. We understand that every dataset is unique, often requiring custom model configurations to disentangle the signals form the noise. Our view is that ethical scientific behaviour involves honest reporting of uncertainty to buffer against spurious conclusions, while simultaneously extracting as much as possible from our data. Our aim is to work collaboratively with a range of researchers to communicate research output with a high degree of confidence."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Quantecol",
    "section": "",
    "text": "We provide custom services tailored to the requirements of the client. This ranges from brief consultations about methodology or analysis, to conducting full data analyses and scientific reporting. We offer:\n\nMethodological guidance\nConsultations about study design\nValidation of statistical analysis\nTutorials and workshops for specific statistical methods\nCollaboration on research projects\nData and output visualisation\n\nWe prefer to take a Bayesian approach because of the flexibility in building custom models, straightforward quantification of uncertainty, and ability to incorporate domain expertise when available. Our capabilities include, but are not limited to, the following statistical models:\n\n(Generalised) linear models\nPower analysis and simulations\nMultivariate analysis\nDose-response models\nOrdinal regression\nHidden Markov models\nItem-response models\nGaussian processes\nEcological models\n\nMark-recapture and multievent\n(Dynamic) occupancy, including co-occurrence\n(Dynamic) N- and multinomial-mixture\nSpatial mark-recapture\nDistance sampling\nSpecies distribution models\nContinuous time models\n\n\nIf you are interested in working with Quantecol, please get in touch."
  },
  {
    "objectID": "services.html#services",
    "href": "services.html#services",
    "title": "Quantecol",
    "section": "",
    "text": "We provide custom services tailored to the requirements of the client. This ranges from brief consultations about methodology or analysis, to conducting full data analyses and scientific reporting. We offer:\n\nMethodological guidance\nConsultations about study design\nValidation of statistical analysis\nTutorials and workshops for specific statistical methods\nCollaboration on research projects\nData and output visualisation\n\nWe prefer to take a Bayesian approach because of the flexibility in building custom models, straightforward quantification of uncertainty, and ability to incorporate domain expertise when available. Our capabilities include, but are not limited to, the following statistical models:\n\n(Generalised) linear models\nPower analysis and simulations\nMultivariate analysis\nDose-response models\nOrdinal regression\nHidden Markov models\nItem-response models\nGaussian processes\nEcological models\n\nMark-recapture and multievent\n(Dynamic) occupancy, including co-occurrence\n(Dynamic) N- and multinomial-mixture\nSpatial mark-recapture\nDistance sampling\nSpecies distribution models\nContinuous time models\n\n\nIf you are interested in working with Quantecol, please get in touch."
  },
  {
    "objectID": "about/contact.html",
    "href": "about/contact.html",
    "title": "Quantecol",
    "section": "",
    "text": "For inquiries about support with your projects, please get in touch via e-mail or call us at 0456 659 313."
  },
  {
    "objectID": "about/contact.html#contact",
    "href": "about/contact.html#contact",
    "title": "Quantecol",
    "section": "",
    "text": "For inquiries about support with your projects, please get in touch via e-mail or call us at 0456 659 313."
  },
  {
    "objectID": "about/people/matthijs.html",
    "href": "about/people/matthijs.html",
    "title": "Dr. Matthijs Hollanders",
    "section": "",
    "text": "Matthijs received Bachelor and Master’s degrees in Biology from Wageningen University & Research, The Netherlands, and was awarded his doctorate from Southern Cross University, Australia, in 2023. A passion for quantitative methods was sparked during his PhD at SCU where he investigated contemporary amphibian responses to the disease chytridiomycosis. Intrigued by the often noisy ecological datasets, he strives to account for uncertainty in datasets to make robust, honest scientific inference."
  },
  {
    "objectID": "blog/gaussian-processes.html",
    "href": "blog/gaussian-processes.html",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "",
    "text": "It is common to use random effects to capture heterogeneity in data that have been collected at spatial and/or temporal intervals. For example, counts of species or individuals are frequently replicated in space and time and researchers may add site or survey date as random effects in the model. These random effects are typically modeled as being normally distributed. Consider surveys \\(i \\in 1:I\\) where counts \\(y_i\\) are made which we model with Poisson regression. The model with normally distributed random survey effects on the log expected count looks like this:\n\\[\n\\begin{aligned}\n  y_i &\\sim \\textrm{Poisson} \\left( \\exp(\\alpha + \\epsilon_i) \\right) \\\\\n  \\epsilon_i &\\sim \\textrm{Normal} \\left( 0, \\tau \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere the intercept \\(\\alpha\\), the random survey-level offsets \\(\\epsilon_i\\), and the standard deviation of the survey effects \\(\\tau\\) are to be estimated by the model. In this post, I argue that when the random effects are not expected to be independent it almost always makes more sense to model the random effects using Gaussian processes (GPs).\nGPs are a bit complicated (Simpson 2021) but one of their salient features is that any set of observations generated by a Gaussian process are marginally distributed as multivariate normal \\(\\textrm{Normal} (\\boldsymbol{\\mu}, \\Sigma)\\).1 In practice, the realisations of the GP are added to our linear predictor so \\(\\boldsymbol{\\mu} = \\boldsymbol{0}\\). There are quite a few covariance kernels \\(\\Sigma\\) to choose from but one of the most common is the exponentiated quadratic kernel given by \\[\n\\Sigma_{ij} = \\tau^2 \\exp \\left( -\\frac{\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert^2}{2 \\rho ^2} \\right),\n\\tag{2}\\]\nwhere \\(\\tau^2\\) is the marginal variance (analogous to variance \\(\\tau^2\\) with normally distributed random effects), \\(\\rho\\) is the length-scale governing the covariance between observations, and the term \\(\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert\\) is the Euclidean distance between any two points of our input.2 The length-scale \\(\\rho\\) governs the correlation between points as a function of the distance between them. When \\(\\rho\\) is small, there is little correlation between neighbouring locations and when \\(\\rho\\) is large, there is high correlation between observations. Essentially, GPs allow modeling normally distributed random effects where observations close to each other (as in, surveys conducted at the same time of year) may be correlated."
  },
  {
    "objectID": "blog/gaussian-processes.html#introduction",
    "href": "blog/gaussian-processes.html#introduction",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "",
    "text": "It is common to use random effects to capture heterogeneity in data that have been collected at spatial and/or temporal intervals. For example, counts of species or individuals are frequently replicated in space and time and researchers may add site or survey date as random effects in the model. These random effects are typically modeled as being normally distributed. Consider surveys \\(i \\in 1:I\\) where counts \\(y_i\\) are made which we model with Poisson regression. The model with normally distributed random survey effects on the log expected count looks like this:\n\\[\n\\begin{aligned}\n  y_i &\\sim \\textrm{Poisson} \\left( \\exp(\\alpha + \\epsilon_i) \\right) \\\\\n  \\epsilon_i &\\sim \\textrm{Normal} \\left( 0, \\tau \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere the intercept \\(\\alpha\\), the random survey-level offsets \\(\\epsilon_i\\), and the standard deviation of the survey effects \\(\\tau\\) are to be estimated by the model. In this post, I argue that when the random effects are not expected to be independent it almost always makes more sense to model the random effects using Gaussian processes (GPs).\nGPs are a bit complicated (Simpson 2021) but one of their salient features is that any set of observations generated by a Gaussian process are marginally distributed as multivariate normal \\(\\textrm{Normal} (\\boldsymbol{\\mu}, \\Sigma)\\).1 In practice, the realisations of the GP are added to our linear predictor so \\(\\boldsymbol{\\mu} = \\boldsymbol{0}\\). There are quite a few covariance kernels \\(\\Sigma\\) to choose from but one of the most common is the exponentiated quadratic kernel given by \\[\n\\Sigma_{ij} = \\tau^2 \\exp \\left( -\\frac{\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert^2}{2 \\rho ^2} \\right),\n\\tag{2}\\]\nwhere \\(\\tau^2\\) is the marginal variance (analogous to variance \\(\\tau^2\\) with normally distributed random effects), \\(\\rho\\) is the length-scale governing the covariance between observations, and the term \\(\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert\\) is the Euclidean distance between any two points of our input.2 The length-scale \\(\\rho\\) governs the correlation between points as a function of the distance between them. When \\(\\rho\\) is small, there is little correlation between neighbouring locations and when \\(\\rho\\) is large, there is high correlation between observations. Essentially, GPs allow modeling normally distributed random effects where observations close to each other (as in, surveys conducted at the same time of year) may be correlated."
  },
  {
    "objectID": "blog/gaussian-processes.html#example-tadpole-counts",
    "href": "blog/gaussian-processes.html#example-tadpole-counts",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Example: tadpole counts",
    "text": "Example: tadpole counts\nFor this example, I’ll use data we collected on tadpoles of Fleay’s barred frogs (Mixophyes fleayi), a stream-dwelling species endemic to the Gondwana rainforests of northern New South Wales and southeast Queensland, Australia (Hollanders et al. 2024). During my PhD, I swept a dipnet through pools in the creeks for roughly 1 min every six weeks, deposited the tadpoles in a tub, took a photograph from above, and used Photoshop to count and measure the tadpoles.3 Here, I’ll fit the model in Equation 1 with and without GP to model the number of tadpoles I captured during each six-weekly survey at one of the sites. First I’ll download the data from my GitHub and plot the counts for the creek using ggplot2 (Wickham 2016) and other packages from the tidyverse (Wickham et al. 2019).\n\n\nCode\n# download data\nlibrary(tidyverse)\ndat &lt;- read_csv(\"https://raw.githubusercontent.com/mhollanders/mfleayi-tadpoles/main/data/tadpole-lengths.csv\") |&gt; \n  filter(site == \"brindle\") |&gt; \n  mutate(date = dmy(date)) |&gt; \n  count(date)\n\n# plot the counts\nfig &lt;- dat |&gt; \n  ggplot(aes(date, n)) + \n  geom_point(colour = green, shape = 16, size = 3, alpha = 3/4) + \n  scale_y_continuous(breaks = seq(50, 200, 50), limits = c(0, 225), expand = c(0, 0)) + \n  labs(x = \"Survey\", y = \"Number of tadpoles\")\nfig\n\n\n\n\n\n\n\n\n\nFleay’s barred frogs breed from the early spring to the middle of summer and generally there are several pulses of reproduction resulting in fresh cohorts of tadpoles. We’ll ignore this and just model the counts using random survey effects in the absence of any predictors.\n\n\n\nFleay’s barred frog (Mixophyes fleayi) from Brindle Creek, Border Ranges National Park, Australia."
  },
  {
    "objectID": "blog/gaussian-processes.html#setting-priors-for-gps",
    "href": "blog/gaussian-processes.html#setting-priors-for-gps",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Setting priors for GPs",
    "text": "Setting priors for GPs\nGPs have two parameters that require priors, \\(\\tau\\) and \\(\\rho\\). The prior for \\(\\tau\\) can just be something like an exponential prior commonly used for standard deviations of random effects.4 The prior for the length-scale \\(\\rho\\) is trickier as it is often not well identified by the data. In fact, the data cannot inform length-scales that are outside of the distances observed in the data. This means that we want a prior than can suppress values below and above certain thresholds. One popular distribution for length-scales that’s capable of doing exactly this is the inverse gamma distribution. Michael Betancourt’s GP case study discusses this at length and includes a function that uses Stan’s algebra solver to determine the shape and scale of the inverse gamma distribution that places a certain proportion of the probability mass between two values.5\nIf the study was well designed to estimate the length-scale of the GP, the survey intervals should be shorter than the minimum length-scale we think we might be dealing with. Unfortunately, I did not collect data with this question in mind, and I can easily see that the relevant length-scale is shorter than the minimum length between surveys (I surveyed at six-weekly intervals, give or take). However, for this demonstration, we’ll assume that the survey intervals were rigorously chosen and we’ll use the minimum and maximum observed temporal distances between surveys to choose a suitable inverse gamma prior for the length-scale. So, I compute the minimum and maximum distance and chose tail probabilities of 1% so that 98% of the probability mass of the length-scale falls between the minimum and maximum observed distances between the surveys."
  },
  {
    "objectID": "blog/gaussian-processes.html#fitting-the-model",
    "href": "blog/gaussian-processes.html#fitting-the-model",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Fitting the model",
    "text": "Fitting the model\nBelow I write out a Stan program (Carpenter et al. 2017) that uses an indicator to use either normally distributed random effects or to use a GP with a exponentiated quadratic covariance kernel. Stan includes the handy function gp_exp_quad_cov() that generates this covariance kernel from the input (survey times expressed in weeks, in this case) and standard deviation and length-scale. We also use this GP input to pre-compute the distance matrix to get the minimum and maximum observed (temporal) distances of the surveys to use Stan’s algebra solver to determine a suitable inverse gamma prior for \\(\\rho\\). For both types of random effects I use the non-centered parameterisation which generally has better sampling for few observations (we only have 10 data points). For GPs (and multivariate normals more generally), this entails Cholesky decomposing the covariance matrix (think of it as a matrix square root) and multiplying it by a vector of standard normal variates. Note that before Cholesky decomposing the covariance kernel, I add a diagonal matrix with a very small value (called the “nugget” or “jitter”) to ensure the matrix is positive definite.\n\n\nCode\nfunctions {\n  // function from Michael Betancourt for inverse gamma prior shape and scale\n  vector inv_gamma_prior(vector guess, vector theta, array[] real tails, array[] int x_i) {\n    vector[2] params;\n    params[1] = inv_gamma_cdf(theta[1] | guess[1], guess[2]) - tails[1];\n    params[2] = inv_gamma_cdf(theta[2] | guess[1], guess[2]) - tails[2];\n    return params;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; I;  // number of surveys\n  array[I] real survey;  // survey input as number of weeks\n  vector[2] min_max_dist;  // minimum and maximum observed distance between weeks\n  array[I] int y;  // tadpole counts per survey\n  int&lt;lower=0, upper=1&gt; GP;  // indicator for GP (0 = no, 1 = yes)\n}\n\ntransformed data {\n  vector&lt;lower=0&gt;[2] rho_prior = ones_vector(2);  // length-scale prior shape and scale\n  if (GP) {\n    rho_prior = algebra_solver(inv_gamma_prior, [1, 1]', min_max_dist, {0.01, 0.99}, {0});\n    print(\"rho_prior: \", rho_prior);\n  }\n}\n\nparameters {\n  real alpha;  // intercept\n  real&lt;lower=0&gt; tau, rho;  // random effect SD and GP length-scale\n  vector[I] z;  // standard-normal z-scores for non-centered parameterisation\n}\n\ntransformed parameters {\n  vector[I] epsilon;  // random survey effects\n  if (GP) {\n    epsilon = cholesky_decompose(add_diag(gp_exp_quad_cov(survey, tau, rho), 1e-9)) * z;\n  } else {\n    epsilon = tau * z;\n  }\n}\n\nmodel {\n  // priors\n  alpha ~ normal(0, 5);\n  tau ~ exponential(1);\n  rho ~ inv_gamma(rho_prior[1], rho_prior[2]);\n  z ~ std_normal();\n    \n  // likelihood\n  y ~ poisson_log(alpha + epsilon);\n}\n\ngenerated quantities {\n  vector[I] yrep, log_lik;\n  for (i in 1:I) {\n    yrep[i] = poisson_log_rng(alpha + epsilon[i]);\n    log_lik[i] = poisson_log_lpmf(y[i] | alpha + epsilon[i]);\n  }\n}\n\n\nI’ll fit both the normal and GP model with CmdStanR (Gabry et al. 2024) in R 4.4.1 (R Core Team 2024).\n\n\nCode\n# prepare data for Stan\nstan_data &lt;- list(I = nrow(dat), \n                  survey = as.numeric(dat$date) / 7, \n                  y = dat$n, \n                  GP = 0)\n\n# get distance matrix and add minimum and maximum observed distances\ndist_obs &lt;- dist(stan_data$survey) |&gt; as.matrix()\nstan_data$min_max_dist &lt;- range(dist_obs[dist_obs &gt; 0])\n\n# normal model\nfit_normal &lt;- mod$sample(data = stan_data, \n                         refresh = 0, chains = 1, \n                         iter_warmup = 500, iter_sampling = 4000, \n                         show_exceptions = F)\n\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.3 seconds.\n\n\nCode\n# GP model\nstan_data$GP &lt;- 1\nfit_gp &lt;- mod$sample(data = stan_data, \n                     refresh = 0, chains = 1, \n                     iter_warmup = 500, iter_sampling = 4000, \n                     show_exceptions = F)\n\n\nRunning MCMC with 1 chain...\n\nChain 1 rho_prior: [3.58235,38.8276] \nChain 1 finished in 1.4 seconds.\n\n\nI’ll first plot the inverse gamma prior for the length-scale that was generated by the solver (green), alongside the posterior distribution of \\(\\rho\\) (black) and the minimum and maximum observed temporal distances of the surveys (dashed lines), using the distributional (O’Hara-Wild et al. 2024) and ggdist and tidybayes packages (Kay 2024a, 2024b). The length-scale seems to want to be smaller than the prior allows, which is due to me enforcing an informative prior. Recall that I pretended the survey intervals were chosen in a principled manner to estimate this parameter by specifying a prior implying the length-scale really doesn’t be much shorter than six weeks.\n\n\nCode\n# packages\nlibrary(distributional)\nlibrary(ggdist)\nlibrary(tidybayes)\n\n# plot\ntibble(prior = dist_inverse_gamma(3.58, scale = 38.83)) |&gt; \n  ggplot(aes(xdist = prior)) + \n  geom_vline(xintercept = stan_data$min_max_dist, \n             linetype = \"dashed\", linewidth = 1/2, colour = \"#333333\") +\n  stat_slab(fill = green, alpha = 2/3) + \n  stat_slab(aes(xdist = rho), \n            data = spread_rvars(fit_gp, rho), \n            fill = \"#333333\", alpha = 2/3) + \n  scale_x_continuous(breaks = seq(0, 60, 20), limits = c(0, 70), expand = c(0, 0)) + \n  scale_y_continuous(breaks = NULL, expand = c(0, 0)) + \n  theme(panel.border = element_rect(colour = NA), \n        axis.line.x = element_line(colour = \"#333333\")) + \n  labs(x = expression(rho), y = NULL)\n\n\n\n\n\n\n\n\n\nFocusing now on the posterior draws for the random survey effects \\(\\epsilon_i\\), it’s clear that the predictions between the normal and GP model are very similar.\n\n\nCode\n# add predictions to figure\nlibrary(tidybayes)\nfig + \n  stat_pointinterval(aes(ydist = exp(alpha + epsilon), \n                         shape = factor(model) |&gt; fct_rev()), \n                     data = fit_normal |&gt; \n                       spread_rvars(alpha, epsilon[i]) |&gt; \n                       mutate(model = \"Normal\") |&gt; \n                       bind_cols(dat) |&gt; \n                       bind_rows(\n                         fit_gp |&gt; \n                           spread_rvars(alpha, epsilon[i]) |&gt; \n                           mutate(model = \"GP\") |&gt; \n                           bind_cols(dat)\n                       ), \n                     position = position_dodge(width = 40),\n                     point_interval = \"median_hdci\", .width = 0.95, \n                     size = 1, linewidth = 1/2) + \n  scale_colour_manual(values = c(\"#333333\", green)) + \n  labs(shape = \"Model\")\n\n\n\n\n\n\n\n\n\nWe can also see that the standard deviation of the normal model is very similar to the marginal standard deviation of the GP model, albeit with more uncertainty. To be honest, the difference in the estimates of \\(\\tau\\) here may be driven more by our prior on \\(\\rho\\) that may be a bit more informative than it should be.\n\n\nCode\nfit_normal |&gt; \n  spread_rvars(tau) |&gt; \n  mutate(model = \"Normal\") |&gt; \n  bind_rows(fit_gp |&gt; \n              spread_rvars(tau) |&gt; \n              mutate(model = \"GP\")) |&gt; \n  ggplot(aes(factor(model) |&gt; fct_rev(), ydist = tau)) +\n  stat_pointinterval(point_interval = \"median_hdci\", .width = 0.95, \n                     size = 1/2, linewidth = 1/2) + \n  scale_colour_manual(values = c(\"#333333\", \"red4\")) + \n  labs(x = \"Model\", y = expression(paste(\"Posterior distribution of \", tau)))"
  },
  {
    "objectID": "blog/gaussian-processes.html#conclusion",
    "href": "blog/gaussian-processes.html#conclusion",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Conclusion",
    "text": "Conclusion\nUsing GPs to model random effects makes a lot of sense if there is some interest in modeling correlation between temporally or spatially organised data. Given that switching out the normally distributed random effects for a GP is trivial in modern probabilistic programming languages such as Stan, it makes sense to default to using them, especially as it includes involves just one extra parameter, the length-scale \\(\\rho\\)."
  },
  {
    "objectID": "blog/gaussian-processes.html#footnotes",
    "href": "blog/gaussian-processes.html#footnotes",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDan Simpson’s blog is a great place to learn more about GPs.↩︎\nWhen our input is multidimensional, such as coordinates for spatial data, we are still working with the Euclidean distance between vectors.↩︎\nI also did other things with those tadpoles, but I won’t go into that here.↩︎\nThis also happens to the penalised complexity (PC) prior for this parameter.↩︎\nThe case study is a great place to learn more about GPs more generally.↩︎"
  },
  {
    "objectID": "blog/hmm-in-stan.html",
    "href": "blog/hmm-in-stan.html",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "",
    "text": "This post was updated\n\n\n\nI updated the post to replace the Viterbi algorithm (which gives the most likely sequence of latent discrete states based on the highest probabilities) with the backward sampling algorithm (which yields the most likely sequence of states by sampling them according to their probabilities) to retrieve the posterior distributions of the marginalised variables. Additionally, I made some models more efficient and updated some text and plotting."
  },
  {
    "objectID": "blog/hmm-in-stan.html#introduction",
    "href": "blog/hmm-in-stan.html#introduction",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Introduction",
    "text": "Introduction\nThe development of Bayesian statistical software had considerable influence on statistical ecology because many ecological models are complex and require custom likelihoods that were not necessarily available in off-the-shelf software such as R or Program Mark (Cooch and White 2008, Kéry and Royle 2015, Kéry and Royle 2020). One of the reasons why ecological models are so complex is because the data are often messy. For instance, in something like a randomised control trial, careful design means you can make stronger assumptions such as that measurements are obtained without error. However, ecological data are noisy and we frequently require complicated observation models that account for imperfect detection of species or individuals. As a result, a large class of ecological models can be formulated as hidden Markov models (HMMs), where time-series data are modeled with a (partially or completely) unobserved ecological model and an observation model conditioned on it. Examples of ecological HMMs include mark-recapture and occupancy models, including their more complex multistate variants.\nHMMs are straightforward to model with traditional Markov chain Monte Carlo (MCMC) methods because the latent ecological states can be sampled like parameters. Formulating these models became accessible to ecologists because of the simplicity afforded by statistical software like BUGS, JAGS, and NIMBLE (de Valpine et al. 2017). Consider the simple mark-recapture model where individuals \\(i \\in 1 : I\\) first captured at time \\(j = f_i\\) survive between times \\(j \\in f_i: J\\) with survival probability \\(\\phi\\) and are recaptured on each occasion with detection probability \\(p\\). The state-space formulation of this model from \\(j \\in (f_i + 1):J\\) with vague priors for \\(\\phi\\) and \\(p\\) is as follows:\n\\[\n\\begin{aligned}\n  z_{i,j} &\\sim \\textrm{Bernoulli} \\left( z_{i,j-1} \\cdot \\phi \\right) \\\\\n  y_{i,j} &\\sim \\textrm{Bernoulli} \\left( z_{i,j} \\cdot p \\right) \\\\\n  \\phi, p &\\sim \\textrm{Beta} \\left( 1, 1 \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(z_{i,j}\\) are the partially observed ecological states (where \\(z=1\\) is an alive individual and \\(z=0\\) is a dead individual) and \\(y_{i,j}\\) are the observed data (where \\(y=1\\) is an observed individual and \\(y=0\\) is an unobserved individual). Using NIMBLE 1.1.0 in R 4.3.2 (R Core Team 2023), coding this model is straightforward and follows the algebra above closely.\n\n\nCode\nlibrary(nimble)\ncmr_code &lt;- nimbleCode(\n  {\n    # priors\n    phi ~ dbeta(1, 1)\n    p ~ dbeta(1, 1)\n    # likelihood\n    for (i in 1:I) {\n      # initial state is known\n      z[i, f[i]] &lt;- y[i, f[i]]\n      # subsequent surveys\n      for (j in (f[i] + 1):J) {\n        z[i, j] ~ dbern(z[i, j - 1] * phi)\n        y[i, j] ~ dbern(z[i, j] * p)\n      }\n    }\n  }\n)\n\n\nAfter simulating some data, creating a NIMBLE model from the code, and configuring the MCMC, we can see that each unknown z[i, j] gets its own binary MCMC sampler.\n\n\nCode\n# metadata and parameters\nI &lt;- 100\nJ &lt;- 8\nphi &lt;- 0.6\np &lt;- 0.7\n\n# containers\nz &lt;- y &lt;- z_known &lt;- matrix(NA, I, J)\nf &lt;- sample(1:(J - 1), I, replace = T) |&gt; sort()\nl &lt;- numeric(I)\n\n# simulation\nfor (i in 1:I) {\n  z[i, f[i]] &lt;- y[i, f[i]] &lt;- 1\n  for (j in (f[i] + 1):J) {\n    z[i, j] &lt;- rbinom(1, 1, z[i, j - 1] * phi)\n    y[i, j] &lt;- rbinom(1, 1, z[i, j] * p)\n  } # t\n  l[i] &lt;- max(which(y[i, ] == 1))\n  z_known[i, f[i]:l[i]] &lt;- 1\n} # i\n\n# create NIMBLE model object\nRmodel &lt;- nimbleModel(cmr_code, \n                      constants = list(I = I, J = J, f = f),\n                      data = list(y = y, z = z_known))\n\n# configure MCMC\nconf &lt;- configureMCMC(Rmodel)\n\n\n===== Monitors =====\nthin = 1: p, phi\n===== Samplers =====\nRW sampler (2)\n  - phi\n  - p\nbinary sampler (372)\n  - z[]  (372 elements)\n\n\nNote that because we know with certainty that individuals were alive between the first and last survey they were captured, we can actually ease computation by supplying those z[i, f[i]:l[i]] as known. We’ll run this model to confirm that it’s able to recover our input parameters using MCMCvis (Youngflesh 2018).\n\n\nCode\n# compile and build MCMC\nCmodel &lt;- compileNimble(Rmodel)\nCmcmc &lt;- buildMCMC(conf) |&gt; compileNimble(project = Cmodel, resetFunctions = T)\n\n# fit model\nn_iter &lt;- 500 ; n_chains &lt;- 8\nfit_cmr_nimble &lt;- runMCMC(Cmcmc, niter = n_iter * 2, nburnin = n_iter, nchains = n_chains)\n\n\n\n\nCode\n# summarise\nlibrary(MCMCvis)\nsummary_cmr_nimble &lt;- MCMCsummary(fit_cmr_nimble, params = c(\"phi\", \"p\")) |&gt;\n  as_tibble(rownames = \"variable\") |&gt;\n  mutate(truth = c(phi, p))\nsummary_cmr_nimble\n\n\n# A tibble: 2 × 9\n  variable  mean     sd `2.5%` `50%` `97.5%`  Rhat n.eff truth\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 phi      0.532 0.0415  0.452 0.530   0.621  1.02   549   0.6\n2 p        0.711 0.0672  0.579 0.711   0.839  1.01   319   0.7\n\n\nAs flexible as Bayesian software like BUGS and JAGS are, most of them do not use Hamiltonian Monte Carlo (HMC, Neal 1994) to generate samples from the posterior distribution (but see nimbleHMC for a recent implementation in NIMBLE). Without going into detail myself1, HMC is considered a superior algorithm and moreover gives warnings when something goes wrong in the sampling, alerting the user to potential issues in estimation. As a result, HMC should generally be preferred by practitioners but it has one trait that can be challenging to ecologists in particular: it requires that all parameters are continuous, meaning that our typical state-space formulations of HMMs cannot be fit with HMC due to the presence of discrete parameters like our ecological states z[i, j] above. The remainder of this post will focus on dealing with fitting complex ecological models using Stan (Carpenter et al. 2017), a probabilistic programming language which implements a state-of-the-art No U-Turn Sampler (NUTS, Hoﬀman and Gelman 2014). I present a unified approach that is applicable to simple and complex models alike, highlighted with three examples of increasing complexity."
  },
  {
    "objectID": "blog/hmm-in-stan.html#marginalisation",
    "href": "blog/hmm-in-stan.html#marginalisation",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Marginalisation",
    "text": "Marginalisation\nSince HMC cannot sample discrete parameters, we have to reformulate our models without the latent states through a process called marginalisation. Marginalisation essentially just counts the mutually exclusive ways an observation can be made and sums these probabilities, as \\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B)\\). For instance, in the above mark-recapture example, consider an individual was captured on occasion 1 and recaptured on occasion 2. The only way this could have happened is that the individual survived the interval with probability \\(\\phi\\) and was recaptured on occasion 2 with probability \\(p\\), making the marginal likelihood of that datapoint \\(\\phi \\cdot p\\). However, if it was not observed at time 2, two things are possible: either the animal survived but was not recaptured with probability \\(\\phi \\cdot (1 - p)\\) or the individual died with probability \\(1 - \\phi\\) and was thus not recaptured. Summing these probabilities gives us the marginal likelihood for that datapoint, which is \\(\\phi \\cdot (1 - p) + (1 - \\phi)\\). Models can be marginalised in many different ways, but in this post I’m going to focus on the forward algorithm. Although it takes some getting used to, the forward algorithm facilitates fitting a wide variety of (increasingly complex) HMMs.\n\nForward algorithm\nFirst, I’m going to restructure our mark-recapture model as a multistate model with the introduction of transition probability matrices (TPMs), which I denote as \\(P\\) in the equations. In the mark-recapture example, we still have two latent ecological states, (1) alive and (2) dead. The transitions from one state to the other is given by the following TPM, where the states of departure (survey \\(j - 1\\)) are in the rows and states of arrival (survey \\(j\\)) are in the columns (note that the rows of a row-stochastic TPM must sum to 1):\n\\[\nP_z = \\begin{bmatrix}\n  \\phi & 1 - \\phi \\\\\n  0 & 1   \n\\end{bmatrix}\n\\tag{2}\\]\nHere, the dead state is an absorbing state and individuals remain in the alive states with probability \\(\\phi\\). The observation process still deals with two observed states, (1) not detected and (2) detected, and is also formulated as a \\(2 \\cdot 2\\) TPM, where the ecological states are in the rows (states of departure) and the observed states in the columns (states of arrival):\n\\[\nP_y = \\begin{bmatrix}\n  1 - p & p \\\\\n  1 & 0\n\\end{bmatrix}\n\\tag{3}\\]\nThe forward algorithm works by computing the marginal likelihood of an individual being in each state \\(s \\in 1 : S\\) at survey \\(j\\) given the observations up to \\(j\\), stored in a \\(S \\cdot J\\) matrix we’ll call \\(\\Omega\\). From here on, the notation for subsetting row \\(i\\) of a matrix \\(M\\) is \\(M_i\\) and \\(M_{:j}\\) for subsetting column \\(j\\). The forward algorithm starts from a vector of initial state probabilities, which in the mark-recapture example are known, given that we know with certainty that every individual is alive at first capture:\n\\[\n\\Omega_{:{f_i}} = \\left[ 1, 0 \\right]^\\intercal\n\\tag{4}\\]\nFor subsequent occasions after an individual’s first capture \\(j \\in (f_i+1):J\\), the marginal probabilities of being in state \\(s\\) at time \\(j\\) are computed as:\n\\[\n\\Omega_{:j} = P_z^\\intercal \\cdot \\Omega_{:j-1} \\odot {P_y}_{[:y_{i, j}]}\n\\tag{5}\\]\nThat is, matrix multiplying the transposed ecological TPM wih the vector of marginal probabilities at time \\(j-1\\) and then element-wise multiplying (where \\(\\odot\\) denotes the Hadamard or element-wise product) the resulting vector with the relevant column of the observation TPM, where the first column is used if the individual was not observed (\\(y_{i,j} = 1\\)) and the second column if it was observed (\\(y_{i,j} = 2\\)). This process continues iteratively until survey \\(J\\) after which the log density is incremented with the log of the sum of \\(\\Omega_{:J}\\), the marginal probabilities of being in each state at the end of the study period.\n\n\nPosterior distributions for the discrete parameters\nAfter marginalising the discrete variables from the model, we are frequently still interested in estimates for these quantities. For instance, in mark-recapture models we often want to estimate population sizes or in occupancy models we want to know the proportion of our sampled sites that were occupied. Ironically, we can actually get better estimates of the posterior distribution of the discrete variables after marginalising because of better exploration of the tails.2. In order to do so, we use the backward algorithm to generate a posterior distribution over the most likely state sequence. We start by sampling the most likely final state \\(z_{i,J}\\) by sampling from the associated probabilities, which we can do in Stan using the categorical_rng() function after normalising them (dividing by the sum of the probabilities). Then we work backwards to compute the most likely state that came prior, by sampling from the probabilities given by the marginal probabilities of each state at time \\(J-1\\) and having transitioned to state \\(z_{i,J}\\), \\(\\Omega_{:J-1} \\odot {P_z}_{[:z_{i,J}]}\\). We do this recursively until \\(j=1\\) (or in case of mark-recapture, until the survey after the last capture, as we know the individual was alive until then)."
  },
  {
    "objectID": "blog/hmm-in-stan.html#example-1-basic-mark-recapture-as-a-hmm",
    "href": "blog/hmm-in-stan.html#example-1-basic-mark-recapture-as-a-hmm",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Example 1: Basic mark-recapture as a HMM",
    "text": "Example 1: Basic mark-recapture as a HMM\nAs a first example of structuring ecological models as HMMs using the forward algorithm, I have translated the initial mark-recapture model for use in Stan. Stan programs specify a log density, so I wrote two versions, one with the probabilities which most closely resembles the above algebra and one using log probabilities throughout. Doing so involves swapping the multiplication of probabilities with summing the log probabilities, changing log(sum()) to log_sum_exp(), and performing matrix multiplication on the log scale with a custom function log_prod_exp()3, which I found on Stack Overflow (note that the function is overloaded to account for the dimensions of your two matrices). I have a few comments:\n\nJust like in the NIMBLE version, we don’t have to worry about the probabilities associated with being dead between the first and last capture. Note that for the occasions between first and last capture, I only compute the marginal probabilities of the alive state. At the occasion of last capture, I fix \\(\\Omega_{3,l_i}\\) to 0 (or negative_infinity() for log).\nIn all models, I also compute the site-level log likelihood in the vector log_lik for potential use in model checking using the loo package (Vehtari et al. 2023).\nMost parts of the model block are repeated in generated quantities. Although this makes the whole program more verbose, it is more efficient to compute derived quantities in the generated quantities block as they are only computed once per HMC iteration, whereas the transformed parameters and the model blocks get updated with each gradient evaluation, of which there can be many per iteration.\n\n\nStan programs\n\nProbabilitiesLog Probabilities\n\n\n\n\nCode\nfunctions {\n  // normalise a vector of probabilities\n  vector normalise(vector A) {\n    return A / sum(A);\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; I, J;\n  array[I] int&lt;lower=1, upper=J&gt; f;\n  array[I] int&lt;lower=f, upper=J&gt; l;\n  array[I, J] int&lt;lower=1, upper=2&gt; y;\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; phi, p;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(phi | 1, 1);\n  target += beta_lupdf(p | 1, 1);\n  \n  // TPMs\n  matrix[2, 2] P_z, P_y;\n  P_z = [[ phi, 1 - phi ],\n         [ 0, 1 ]];\n  P_y = [[ 1 - p, p ],\n         [ 1, 0 ]];\n  \n  // forward algorithm\n  matrix[2, J] Omega;\n  for (i in 1:I) {\n  \n    // initial state probabilities\n    Omega[:, f[i]] = [ 1, 0 ]';\n    \n    // must be alive up until last capture\n    for (j in (f[i] + 1):l[i]) {\n      Omega[1, j] = P_z[1, 1] * Omega[1, j - 1] * P_y[1, y[i, j]];\n    }\n    Omega[2, l[i]] = 0;\n    \n    // after last capture, condition both states on being undetected\n    for (j in (l[i] + 1):J) {\n      Omega[:, j] = P_z' * Omega[:, j - 1] .* P_y[:, 1];\n    }\n    \n    // increment log density\n    target += log(sum(Omega[:, J]));\n  }\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I, J] int z;\n  {\n    matrix[2, 2] P_z, P_y;\n    P_z = [[ phi, 1 - phi ],\n           [ 0, 1 ]];\n    P_y = [[ 1 - p, p ],\n           [ 1, 0 ]];\n    matrix[2, J] Omega;\n    for (i in 1:I) {\n    \n      // forward algorithm for log likelihood\n      Omega[:, f[i]] = [ 1, 0 ]';\n      for (j in (f[i] + 1):l[i]) {\n        Omega[1, j] = P_z[1, 1] * Omega[1, j - 1] * P_y[1, y[i, j]];\n      }\n      Omega[2, l[i]] = 0;\n      for (j in (l[i] + 1):J) {\n        Omega[:, j] = P_z' * Omega[:, j - 1] .* P_y[:, 1];\n      }\n      log_lik[i] = log(sum(Omega[:, J]));\n      \n      // backward algorithm for ecological states\n      z[i, J] = categorical_rng(normalise(Omega[:, J]));\n      for (j in (l[i] + 1):(J - 1)) {\n        int jj = J + l[i] - j;  // reverse indexing\n        z[i, jj] = categorical_rng(normalise(Omega[:, jj] \n                                             .* P_z[:, z[i, jj + 1]]));\n      }\n      // alive from first to last capture\n      z[i, f[i]:l[i]] = ones_int_array(l[i] - f[i] + 1);\n    }\n  }\n}\n\n\n\n\n\n\nCode\nfunctions{\n  // normalise a vector of log probabilities\n  vector normalise_log(vector A) {\n    return A - log_sum_exp(A);\n  }\n  \n  /**\n   * Return the natural logarithm of the product of the element-wise \n   * exponentiation of the specified matrices\n   *\n   * @param A  First matrix or (row_)vector\n   * @param B  Second matrix or (row_)vector\n   *\n   * @return   log(exp(A) * exp(B))\n   */\n  matrix log_prod_exp(matrix A, matrix B) {\n    int I = rows(A);\n    int J = cols(A);\n    int K = cols(B);\n    matrix[J, I] A_tr = A';\n    matrix[I, K] C;\n    for (k in 1:K) {\n      for (i in 1:I) {\n        C[i, k] = log_sum_exp(A_tr[:, i] + B[:, k]);\n      }\n    }\n    return C;\n  }\n  vector log_prod_exp(matrix A, vector B) {\n    int I = rows(A);\n    int J = cols(A);\n    matrix[J, I] A_tr = A';\n    vector[I] C;\n    for (i in 1:I) {\n      C[i] = log_sum_exp(A_tr[:, i] + B);\n    }\n    return C;\n  }\n  row_vector log_prod_exp(row_vector A, matrix B) {\n    int K = cols(B);\n    vector[size(A)] A_tr = A';\n    row_vector[K] C;\n    for (k in 1:K) {\n      C[k] = log_sum_exp(A_tr + B[:, k]);\n    }\n    return C;\n  }\n  real log_prod_exp(row_vector A, vector B) {\n    return log_sum_exp(A' + B);\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; I, J;\n  array[I] int&lt;lower=1, upper=J - 1&gt; f;\n  array[I] int&lt;lower=f, upper=J&gt; l;\n  array[I, J] int&lt;lower=1, upper=2&gt; y;\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; phi, p;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(phi | 1, 1);\n  target += beta_lupdf(p | 1, 1);\n  \n  // (log) TPMs\n  matrix[2, 2] P_z, P_y;\n  P_z = [[ phi, 1 - phi ],\n         [ 0, 1 ]];\n  P_y = [[ 1 - p, p ],\n         [ 1, 0 ]];\n  P_z = log(P_z);\n  P_y = log(P_y);\n  \n  // forward algorithm\n  matrix[2, J] Omega;\n  for (i in 1:I) {\n    \n    // initial state log probabilities\n    Omega[:, f[i]] = [ 0, negative_infinity() ]';\n    \n    // must be alive up until last capture\n    for (j in (f[i] + 1):l[i]) {\n      Omega[1, j] = P_z[1, 1] + Omega[1, j - 1] + P_y[1, y[i, j]];\n    }\n    Omega[2, l[i]] = negative_infinity();\n      \n    // after last capture, condition both states on being undetected\n    for (j in (l[i] + 1):J) {\n      Omega[:, j] = log_prod_exp(P_z', Omega[:, j - 1]) + P_y[:, 1];\n    }\n    \n    // increment log density\n    target += log_sum_exp(Omega[:, J]);\n  }\n}\n\ngenerated quantities {\n  array[I, J] int z;\n  vector[I] log_lik;\n  {\n    matrix[2, 2] P_z, P_y;\n    P_z = [[ phi, 1 - phi ],\n           [ 0, 1 ]];\n    P_y = [[ 1 - p, p ],\n           [ 1, 0 ]];\n    P_z = log(P_z);\n    P_y = log(P_y);\n    matrix[2, J] Omega;\n    for (i in 1:I) {\n    \n      // forward algorithm for log likelihood\n      Omega[:, f[i]] = [ 0, negative_infinity() ]';\n      for (j in (f[i] + 1):l[i]) {\n        Omega[1, j] = P_z[1, 1] + Omega[1, j - 1] + P_y[1, y[i, j]];\n      }\n      Omega[2, l[i]] = negative_infinity();\n      for (j in (l[i] + 1):J) {\n        Omega[:, j] = log_prod_exp(P_z', Omega[:, j - 1]) + P_y[:, 1];\n      }\n      log_lik[i] = log_sum_exp(Omega[:, J]);\n      \n      // backward algorithm for ecological states\n      z[i, J] = categorical_rng(exp(normalise_log(Omega[:, J])));\n      for (j in (l[i] + 1):(J - 1)) {\n        int jj = J + l[i] - j;  // reverse indexing\n        z[i, jj] = categorical_rng(exp(normalise_log(Omega[:, jj] \n                                                     + P_z[:, z[i, jj + 1]])));\n      }\n      // alive from first to last capture\n      z[i, f[i]:l[i]] = ones_int_array(l[i] - f[i] + 1);\n    }\n  }\n}\n\n\n\n\n\n\n\nWhy we like HMC\nUsing CmdStanR 0.7.1 to call CmdStan 2.34.1 (Gabry et al. 2023), we’ll just run the log probability model for the same number of iterations as the NIMBLE version, and we see that there were no issues with sampling (Stan would tell us otherwise) and the effective sample sizes (ESS) were several times higher than for the NIMBLE model.\n\n\nCode\n# data for Stan without NAs\ncmr_data &lt;- list(I = I, J = J, f = f, l = l, y = y + 1) |&gt;\n  sapply(\\(x) replace(x, is.na(x), 1))\n\n# run HMC\nlibrary(cmdstanr)\noptions(mc.cores = n_chains)\nfit_cmr &lt;- cmr_lp$sample(data = cmr_data, refresh = 0, chains = n_chains, \n                         iter_warmup = n_iter, iter_sampling = n_iter)\n\n\nRunning MCMC with 8 parallel chains...\n\nChain 1 finished in 1.4 seconds.\nChain 4 finished in 1.4 seconds.\nChain 6 finished in 1.3 seconds.\nChain 2 finished in 1.4 seconds.\nChain 3 finished in 1.5 seconds.\nChain 5 finished in 1.4 seconds.\nChain 7 finished in 1.3 seconds.\nChain 8 finished in 1.4 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 1.4 seconds.\nTotal execution time: 1.5 seconds.\n\n\nCode\n# summary\nlibrary(tidyverse)\nfit_cmr$summary(c(\"phi\", \"p\")) |&gt; \n  select(variable, median, contains(\"q\"), contains(\"ess\")) |&gt;\n  mutate(ess_nimble = summary_cmr_nimble$n.eff,\n         truth = c(phi, p))\n\n\n# A tibble: 2 × 8\n  variable median    q5   q95 ess_bulk ess_tail ess_nimble truth\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 phi       0.529 0.459 0.596    2127.    2315.        549   0.6\n2 p         0.719 0.599 0.819    2084.    2219.        319   0.7\n\n\nCode\n# check loo\nfit_cmr$loo()\n\n\n\nComputed from 4000 by 100 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -151.1 14.8\np_loo         2.2  0.7\nlooic       302.2 29.7\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 1.0]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nAnd just to confirm, we check if the backward algorithm was able to closely recover the latent alive states. In order to assess this fairly, I’ll only check what proportion of the ambiguous states (i.e., after last capture) was estimated correctly, based on the posterior medians.\n\n\nCode\nlibrary(tidybayes)\nfit_cmr |&gt; \n  spread_rvars(z[i, j]) |&gt; \n  left_join(tibble(i = 1:I, \n                   l = l), \n            by = \"i\") |&gt; \n  mutate(truth = if_else(c(.env$z) == 1, 1, 2)) |&gt; \n  filter(j &gt; l) |&gt; \n  summarise(prop_true = mean(median(z) == truth))\n\n\n# A tibble: 1 × 1\n  prop_true\n      &lt;dbl&gt;\n1     0.944"
  },
  {
    "objectID": "blog/hmm-in-stan.html#example-2-dynamic-multistate-occupancy-model",
    "href": "blog/hmm-in-stan.html#example-2-dynamic-multistate-occupancy-model",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Example 2: Dynamic multistate occupancy model",
    "text": "Example 2: Dynamic multistate occupancy model\nIn order to highlight the generality of the above modeling approach, we’re going to switch gears to a dynamic occupancy model. There are some trivial to implement differences with the basic mark-recapture model that can seem daunting at first:\n\nThere can be more than two states.\nThe initial state probabilities here are unknown, that is, we don’t know the occupancy state of a site when it’s first surveyed.\nState transitions can happen in more than one direction, whereas dead individuals in mark-recapture can never transition to alive (i.e., there is no absorbing state).\n\nConsider a study area containing sites \\(i \\in 1 : I\\) surveyed across seasons \\(j \\in 1 : J\\). We are investigating whether sites are occupied by tiger snakes (Notechis scutatus) in each season, but want to differentiate between sites that simply support tiger snakes or those that support breeding. Our ecological process thus has three possible states: (1) unoccupied by tiger snakes, (2) occupied by tiger snakes but not for breeding, or (3) occupied by breeding tiger snakes. The observation process also has three possible states: (1) no snakes found, (2) snakes found but no evidence of breeding, and (3) snakes found with evidence of breeding, as determined by the discovery of gravid females. Instead of directly formulating the ecological TPM here, we’ll do it in continuous time instead using a transition rate matrix (TRM), which I call \\(Q\\) in the equations. Although this may not be the best example for use with a continuous time model, I think it comes up often enough in these ecological models to warrant a demonstration.\n\n\n\nA rainforest tiger snake (Notechis scutatus) from northern New South Wales, Australia.\n\n\n\n\n\n\n\n\nEcology occurs in continuous time\n\n\n\nTPMs assume equal intervals between surveys, but this is the exception rather than the norm for ecological data. Although things like survival probabilities can easily be exponentiated to the time interval, this doesn’t work for processes that are not absorbing states, i.e., snakes moving in and out of sites. Unlike individual survival, sites are free to transition between states from year to year. Even with simpler models, however, it often makes more sense to model the survival process with mortality hazard rates (where the survival probability \\(S\\) is related to the mortality hazard rate \\(h\\) as \\(S = \\exp(-h)\\)) for a number of reasons (Ergon et al. 2018). The multistate extension of this is straightforward, where we instead take the matrix exponential of a TRM (Glennie et al. 2022). By constructing our transition matrices as TRMs, where the off-diagonals contain instantaneous hazard rates and the diagonals contain negative sums of these off-diagonals (ensuring rows sum to 0, not 1), we can generate time-specific TPMs by taking the matrix exponential of the TRM multiplied by the time interval between occasions (\\(\\tau_j\\)), thereby essentially marginalising over possible state transitions that occurred during survey intervals:\n\\[\n{P_z}_{[j]} = \\exp \\left( Q \\cdot \\tau_j \\right)\n\\tag{6}\\]\n\n\nA TRM for the tiger snake example might look something like this, with again states of departure (season \\(j-1\\)) in the rows and states of arrival (season \\(j\\)) in the columns:\n\\[\nQ = \\begin{bmatrix}\n  -(\\gamma_1 + \\gamma_2) & \\gamma_1 & \\gamma_2 \\\\\n  \\epsilon_1 & -(\\epsilon_1 + \\psi_1) & \\psi_1 \\\\\n  \\epsilon_2 & \\psi_2 & -(\\epsilon_2 + \\psi_2)\n\\end{bmatrix}\n\\tag{7}\\] with the following parameters:\n\n\\(\\gamma_1\\): the “non-breeding” colonisation rate, or the rate at which a site becomes occupied with non-breeding snakes at time \\(j\\) when the site was not occupied at time \\(j - 1\\).\n\\(\\gamma_2\\): the “breeding” colonisation rate, or the rate at which a site becomes occupied with breeding snakes at time \\(j\\) when the site was not occupied at time \\(j - 1\\).\n\\(\\epsilon_1\\): the “non-breeding” emigration rate, or the rate at which a site becomes onoccupied at time \\(j\\) when it was occupied with non-breeding snakes at time \\(j - 1\\).\n\\(\\epsilon_2\\): the “breeding” emigration rate, or the rate at which a site becomes onoccupied at time \\(j\\) when it was occupied with breeding snakes at time \\(j - 1\\).\n\\(\\psi_1\\): the breeding rate, or the rate at which a site becomes used for breeding at time \\(j\\) when the site was occupied but not used for breeding at time \\(j - 1\\).\n\\(\\psi_2\\): the breeding cessation rate, or the rate at which a site stops being used for breeding at time \\(j\\) when the site was used for breeding at time \\(j - 1\\).\n\nNote that even if these parameters aren’t exactly the quantities of interest, derivations of these parameters are easily produced as generated quantities after parameter estimation. Of course, these parameters can be modeled more complexly with linear or non-linear effects at the level of years, sites, or both, with the only changes being that the TRMs are constructed at those levels (for instance, by creating site- and year-level TRMs where the parameters populating the TRM are allowed to vary at that level—see Example 3 for an application).\nIn addition to the TRM/TPM, the ecological process is initiated on the first occasion with an initial state vector, giving the probabilities of being in each state at \\(j = 1\\):\n\\[\n  \\boldsymbol{\\eta} = \\left[ \\eta_1 , \\eta_2 , \\eta_3 \\right]\n\\tag{8}\\]\n\nSimulating continuous time\nBelow, I’ll simulate the ecological data by specifying a TRM and using the expm package to take the matrix exponential to generate the TPMs (Goulet et al. 2021). In order to simulate ecological states z[i, j] we use the categorical distribution, which is just the Bernoulli distribution generalised to multiple outcomes. I demonstrate the ability to accommodate unequal time intervals with the TRM.\n\n\nCode\n# metadata\nI &lt;- 100\nJ &lt;- 10\nK_max &lt;- 4\ntau &lt;- rlnorm(J - 1)\n\n# parameters\neta &lt;- c(NA, 0.3, 0.4)       # unoccupied/non-breeding/breeding initial probabilities\neta[1] &lt;- 1 - sum(eta[2:3])  # ensure probabilities sum to 1 (simplex) \ngamma &lt;- c(0.3, 0.2)         # non-breeding/breeding colonisation rates\nepsilon &lt;- c(0.4, 0.1)       # non-breeding/breeding emigration rates\npsi &lt;- c(0.4, 0.3)           # breeding/breeding cessation rates\n\n# function for random categorical draw\nrcat &lt;- function(n, prob) {\n  return(\n    rmultinom(n, size = 1, prob) |&gt; \n      apply(2, \\(x) which(x == 1))\n  )\n}\n\n# TRM\nQ &lt;- matrix(c(-(gamma[1] + gamma[2]), gamma[1], gamma[2],\n              epsilon[1], -(epsilon[1] + psi[1]), psi[1],\n              epsilon[2], psi[2], -(epsilon[2] + psi[2])),\n            3, byrow = T)\nQ\n\n\n     [,1] [,2] [,3]\n[1,] -0.5  0.3  0.2\n[2,]  0.4 -0.8  0.4\n[3,]  0.1  0.3 -0.4\n\n\nCode\n# here's the TPMs\nlibrary(expm)\nP_z &lt;- array(NA, c(J - 1, 3, 3))\nfor (j in 2:J) {\n  P_z[j - 1, , ] &lt;- expm(Q * tau[j - 1])\n}\nP_z[1, , ]\n\n\n       [,1]  [,2]   [,3]\n[1,] 0.9036 0.056 0.0404\n[2,] 0.0739 0.851 0.0754\n[3,] 0.0214 0.056 0.9226\n\n\nCode\n# ecological process simulation\nz &lt;- matrix(NA, I, J)\nfor (i in 1:I) {\n  z[i, 1] &lt;- rcat(1, eta)\n  for (j in 2:J) {\n    z[i, j] &lt;- rcat(1, P_z[j - 1, z[i, j - 1], ])\n  }\n}\nhead(z, 10)\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    1    1    1    2    1    1    1    2    2     3\n [2,]    2    2    3    3    1    1    3    2    1     1\n [3,]    1    1    2    3    2    2    3    1    1     2\n [4,]    3    3    2    3    1    1    3    3    3     3\n [5,]    3    3    3    3    2    1    1    1    1     3\n [6,]    1    1    3    2    3    2    1    1    1     1\n [7,]    3    3    2    2    1    2    3    3    2     3\n [8,]    3    3    3    3    3    2    2    2    2     2\n [9,]    1    1    2    2    3    3    3    1    3     3\n[10,]    3    3    3    3    2    2    2    3    3     1\n\n\n\n\nObservation process\nAlthough possible to construct in continuous time, often our observation model makes more sense in discrete time, especially if specific surveys were conducted instead of with something like an automated recording device. For instance, with dynamic occupancy models, an investigator would conduct multiple “secondary” surveys \\(k \\in 1:K_{ij}\\) within a season in order to disentangle the ecological process from the observation process.\n\n\n\n\n\n\nRobust Design\n\n\n\nWith multistate models where individuals or sites can transition between ecological states, in order to reliably estimate transitions rates and detection probabilities we need to conduct multiple consecutive surveys within periods of assumed closure, yielding secondary surveys nested within primary occasions. In the absence of this, there are parameter identifiability issues for the state transition rates and state-specific detection probabilities. This survey strategy was originally called the “robust design” (Pollock 1982) and the necessity of applying it for multistate models more generally has gone largely unappreciated by ecologists.\n\n\nAs in the mark-recapture example, we construct an observation TPM where the ecological states are in the rows and observed states in the columns, recognising that there’s uncertainty in being able to detect breeding:\n\\[\nP_y = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  1 - p_1 & p_1 & 0 \\\\\n  1 - p_2 & p_2 \\cdot (1 - \\delta) & p_2 \\cdot \\delta\n\\end{bmatrix}\n\\tag{9}\\] with the following parameters:\n\n\\(p_{1:2}\\): the state-specific detection probabilities, or the probabilities of detecting tiger snakes if they are present but not breeding (\\(p_1\\)) or present and breeding (\\(p_2\\)).\n\\(\\delta\\): the breeding detection probability, or the probability of detecting tiger snake breeding conditional on it occurring at a site. Note that in this example, breeding is detected when gravid females are found. Note that false-positive detections of any kind (erroneous snake identification, etc.) are not considered, but will be in Example 3.\n\nI’ll simulate some observational data below where the investigators aimed to conduct 4 secondary surveys per site and primary occasion (\\(K_{i,j}\\)), all conducted during the tiger snake breeding season, with some secondary surveys missed, and some years missed altogether (simulated as \\(K_{i,j} = 0\\)).\n\n\nCode\n# parameters\np &lt;- c(0.4, 0.5)  # non-breeding/breeding detection probabilities\ndelta &lt;- 0.6      # breeding detection probability\n\n# TPM\nP_y &lt;- matrix(c(1, 0, 0,\n                1 - p[1], p[1], 0,\n                1 - p[2], p[2] * (1 - delta), p[2] * delta),\n              3, byrow = T)\n\n# containers\nK &lt;- matrix(sample(0:K_max, I * J, replace = T), I, J)\ny &lt;- array(NA, c(I, J, K_max))\n\n# simulation\nfor (i in 1:I) {\n  for (j in 1:J) {\n    if (K[i, j]) {\n      y[i, j, 1:K[i, j]] &lt;- rcat(K[i, j], P_y[z[i, j], ])\n    }\n  }\n}\n\n\n\n\nStan program\nBelow is the Stan program for the above model written with log probabilities. In many ways it is similar to the mark-recapture example, with the following noteworthy changes:\n\nWhen no surveys were done in a season (K[i, j] = 0), Stan will automatically skip over them because a loop for (k in 1:0) is ignored, unlike in R!\nAll entries in the \\(3 \\cdot J\\) matrix Omega are initialised with the observation process. For each secondary survey, I subset the appropriate column from \\(P_y\\) and increment it onto \\(\\Omega_{:,j}\\). The marginal likelihood of the observation process is just the sum of the respective log probabilities.\nThe continuous time transition rates \\(\\gamma\\), \\(\\epsilon\\), and \\(\\psi\\) are given Exponential(3) priors, as these are good priors when the snapshot property of HMMs is satisfied (see Equation 2 in (Goulet et al. 2021)).\n\n\n\nCode\nfunctions{\n  // normalise a vector of log probabilities\n  vector normalise_log(vector A) {\n    return A - log_sum_exp(A);\n  }\n  \n  /**\n   * Return the natural logarithm of the product of the element-wise \n   * exponentiation of the specified matrices\n   *\n   * @param A  First matrix or (row_)vector\n   * @param B  Second matrix or (row_)vector\n   *\n   * @return   log(exp(A) * exp(B))\n   */\n  matrix log_prod_exp(matrix A, matrix B) {\n    int I = rows(A);\n    int J = cols(A);\n    int K = cols(B);\n    matrix[J, I] A_tr = A';\n    matrix[I, K] C;\n    for (k in 1:K) {\n      for (i in 1:I) {\n        C[i, k] = log_sum_exp(A_tr[:, i] + B[:, k]);\n      }\n    }\n    return C;\n  }\n  vector log_prod_exp(matrix A, vector B) {\n    int I = rows(A);\n    int J = cols(A);\n    matrix[J, I] A_tr = A';\n    vector[I] C;\n    for (i in 1:I) {\n      C[i] = log_sum_exp(A_tr[:, i] + B);\n    }\n    return C;\n  }\n  row_vector log_prod_exp(row_vector A, matrix B) {\n    int K = cols(B);\n    vector[size(A)] A_tr = A';\n    row_vector[K] C;\n    for (k in 1:K) {\n      C[k] = log_sum_exp(A_tr + B[:, k]);\n    }\n    return C;\n  }\n  real log_prod_exp(row_vector A, vector B) {\n    return log_sum_exp(A' + B);\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; I, J, K_max;\n  array[I, J] int&lt;lower=0, upper=K_max&gt; K;\n  vector&lt;lower=0&gt;[J - 1] tau;\n  array[I, J, K_max] int&lt;lower=1, upper=3&gt; y;\n}\n\nparameters {\n  simplex[3] eta;\n  vector&lt;lower=0&gt;[2] gamma, epsilon, psi;\n  vector&lt;lower=0, upper=1&gt;[2] p;\n  real&lt;lower=0, upper=1&gt; delta;\n}\n\nmodel {\n  // priors\n  target += dirichlet_lupdf(eta | ones_vector(3));\n  target += exponential_lupdf(gamma | 3);\n  target += exponential_lupdf(epsilon | 3);\n  target += exponential_lupdf(psi | 3);\n  target += beta_lupdf(p | 1, 1);\n  target += beta_lupdf(delta | 1, 1);\n  \n  // log initial state probabilities, TRM, and log TPMs\n  vector[3] log_eta = log(eta);\n  matrix[3, 3] Q, P_y;\n  array[J - 1] matrix[3, 3] P_z;\n  Q = [[ -(gamma[1] + gamma[2]), gamma[1], gamma[2] ],\n       [ epsilon[1], -(epsilon[1] + psi[1]), psi[1] ],\n       [ epsilon[2], psi[2], -(epsilon[2] + psi[2]) ]];\n  for (j in 2:J) {\n    P_z[j - 1] = matrix_exp(Q * tau[j - 1]);\n  }\n  P_z = log(P_z);\n  P_y = [[ 1, 0, 0 ],\n         [ 1 - p[1], p[1], 0 ],\n         [ 1 - p[2], p[2] * (1 - delta), p[2] * delta ]];\n  P_y = log(P_y);\n  \n  // forward algorithm\n  for (i in 1:I) {\n    \n    // initialise marginal log probabilities with observation model\n    matrix[3, J] Omega = rep_matrix(0, 3, J);\n    for (j in 1:J) {\n      for (k in 1:K[i, j]) {\n        Omega[:, j] += P_y[:, y[i, j, k]];\n      }\n    }\n    \n    // increment ecological process\n    Omega[:, 1] += log_eta;\n    for (j in 2:J) {\n      Omega[:, j] += log_prod_exp(P_z[j - 1]', Omega[:, j - 1]);\n    }\n    \n    // increment log density\n    target += log_sum_exp(Omega[:, J]);\n  }\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I, J] int z;\n  {\n    vector[3] log_eta = log(eta);\n    matrix[3, 3] Q, P_y;\n    array[J - 1] matrix[3, 3] P_z;\n    Q = [[ -(gamma[1] + gamma[2]), gamma[1], gamma[2] ],\n         [ epsilon[1], -(epsilon[1] + psi[1]), psi[1] ],\n         [ epsilon[2], psi[2], -(epsilon[2] + psi[2]) ]];\n    for (j in 2:J) {\n      P_z[j - 1] = matrix_exp(Q * tau[j - 1]);\n    }\n    P_z = log(P_z);\n    P_y = [[ 1, 0, 0 ],\n           [ 1 - p[1], p[1], 0 ],\n           [ 1 - p[2], p[2] * (1 - delta), p[2] * delta ]];\n    P_y = log(P_y);\n    for (i in 1:I) {\n      \n      // forward algorithm for log likelihood\n      matrix[3, J] Omega = rep_matrix(0, 3, J);\n      for (j in 1:J) {\n        for (k in 1:K[i, j]) {\n          Omega[:, j] += P_y[:, y[i, j, k]];\n        }\n      }\n      Omega[:, 1] += log_eta;\n      for (j in 2:J) {\n        Omega[:, j] += log_prod_exp(P_z[j - 1]', Omega[:, j - 1]);\n      }\n      log_lik[i] = log_sum_exp(Omega[:, J]);\n      \n      // backward algorithm for ecological states\n      z[i, J] = categorical_rng(exp(normalise_log(Omega[:, J])));\n      for (j in 2:J) {\n        int jj = J - j + 1;  // reverse 1:(J - 1)\n        z[i, jj] = categorical_rng(exp(normalise_log(Omega[:, jj] \n                                                     + P_z[jj][:, z[i, jj + 1]])));\n      }\n    }\n  }\n}\n\n\nWe’ll run Stan and visually summarise the parameter estimates, with the input plotted alongside in green. It looks like we did alright, but there is a lot of uncertainty, largely because sites can move between 3 states between each year. For this reason, dynamic occupancy models are notoriously data-hungry. Often the initial state parameters are particularly difficult to estimate and are sometimes best parameterised as the steady state vector of the TPM. I haven’t quite worked out how to implement this efficiently in Stan.\n\n\nCode\n# data for Stan\nocc_data &lt;- list(I = I, J = J, K_max = K_max, K = K, tau = tau, y = y) |&gt; \n  sapply(\\(x) replace(x, is.na(x), 1))\n\n# run HMC\nfit_dyn &lt;- occ_dyn_ms$sample(data = occ_data, refresh = 0, chains = n_chains,\n                             iter_warmup = n_iter, iter_sampling = n_iter)\n\n\nRunning MCMC with 8 parallel chains...\n\nChain 6 finished in 32.3 seconds.\nChain 8 finished in 32.9 seconds.\nChain 2 finished in 33.7 seconds.\nChain 5 finished in 35.1 seconds.\nChain 4 finished in 36.9 seconds.\nChain 7 finished in 37.0 seconds.\nChain 1 finished in 38.2 seconds.\nChain 3 finished in 44.3 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 36.3 seconds.\nTotal execution time: 44.5 seconds.\n\n\n\n\nResults\n\n\nCode\nfit_dyn |&gt; \n  gather_rvars(eta[i], gamma[i], epsilon[i], psi[i], p[i], delta) |&gt; \n  mutate(truth = c(eta, gamma, epsilon, psi, p, delta), \n         .variable = factor(.variable, levels = c(\"eta\", \"gamma\", \"epsilon\", \"psi\", \"p\", \"delta\")),\n         process = if_else(str_detect(.variable, c(\"^p$|delta\")), \"Observation Process\", \"Ecological Process\")) |&gt; \n  ggplot(aes(xdist = .value, \n             y = if_else(is.na(i), .variable, str_c(.variable, \"[\", i, \"]\")) |&gt;\n               fct_reorder(as.numeric(.variable)) |&gt; \n               fct_rev())) + \n  facet_wrap(~ process, scales = \"free_y\", ncol = 1) + \n  stat_pointinterval(point_interval = median_qi, .width = 0.9, linewidth = 0.5, position = position_nudge(y = 0.1)) +\n  geom_point(aes(truth), colour = green, position = position_nudge(y = -0.1)) +\n  scale_x_continuous(breaks = seq(0.2, 1, 0.2), expand = c(0, 0)) +\n  scale_y_discrete(labels = ggplot2:::parse_safe) +\n  labs(x = \"Posterior\", y = \"Parameter\")\n\n\n\n\n\n\n\n\n\nLet’s also check how many of our ecological ecological states were correctly recovered. Since only state 3 (snakes present and breeding) can be directly observed with certainty, we’ll only check the ecological states for those times where no breeding was observed.\n\n\nCode\nfit_dyn |&gt; \n  spread_rvars(z[i, j]) |&gt; \n  mutate(truth = c(.env$z),\n         max_y = apply(y, 1:2,  max, na.rm = T) |&gt; c()) |&gt;\n  filter(between(max_y, 1, 2)) |&gt; \n  summarise(prop_true = mean(median(z) == truth))\n\n\n# A tibble: 1 × 1\n  prop_true\n      &lt;dbl&gt;\n1     0.643\n\n\n\n\n\n\n\n\nSteady state distribution\n\n\n\nChatGPT tells me that to get the steady state distribution from a TPM, you perform the eigendecomposition of the TPM and the eigenvector corresponding to the eigenvalue of 1 is the steady state distribution. Stan does have eigendecomposition, so I’m sure there’s a way to parameterise the initial states nicely, but it gets more complicated with time- and/or individual-varying parameters."
  },
  {
    "objectID": "blog/hmm-in-stan.html#example-3-disease-structured-mark-recapture-with-state-misclassification",
    "href": "blog/hmm-in-stan.html#example-3-disease-structured-mark-recapture-with-state-misclassification",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Example 3: Disease-structured mark-recapture with state misclassification",
    "text": "Example 3: Disease-structured mark-recapture with state misclassification\nThe main reason I dove so deep into HMMs is because during my PhD I ran into a problem with my mark-recapture study on Fleay’s barred frogs (Mixophyes fleayi). For two years I conducted mark-recapture surveys along Gondwana rainforest streams in northern New South Wales, Australia, swabbing every frog I found to test for the presence of the pathogenic amphibian chytrid fungus (Batrachochytrium dendrobatidis, Bd) (Scheele et al. 2019) to hopefully infer something about (1) the effect of Bd on frog mortality rates and (2) the infection dynamics.\n\n\n\nAdult male Fleay’s or silverblue-eyed barred frog (Mixophyes fleayi) from Border Ranges National Park, New South Wales, Australia.\n\n\nThe ecological TRM of such a disease-structured multistate model is pretty straightforward, with three possible states: (1) alive and uninfected, (2) alive and infected, and (3) dead, which is an absorbing state:\n\\[\nQ = \\begin{bmatrix}\n  -(\\psi_1 + \\phi_1) & \\psi_1 & \\phi_1 \\\\\n  \\psi_2 & -(\\psi_2 + \\phi_2) & \\phi_2 \\\\\n  0 & 0 & 0\n\\end{bmatrix}\n\\tag{10}\\]\nHere, \\(\\phi_{1:2}\\) are the mortality hazard rates of uninfected and infected frogs, respectively, and \\(\\psi_{1:2}\\) are the rates of gaining and clearing infections, respectively.\n\nNothing is certain\nWhile I was running qPCRs in the lab on some swabs collected during high frequency surveys (1 week intervals) with large numbers of recaptures, I noticed some frogs were apparently frequently clearing and re-gaining infections. For instance, a frog captured on 5 successive weeks may have looked something like this: \\(\\left[ 2, 2, 1, 2, 1 \\right]\\), implying the infection was lost twice and gained once across 4 weeks. This seemed unlikely, and it struck me as more probable that I was simply getting false negatives. At the same time, I couldn’t rule out false positives, either, given that contamination is always a risk. The best way to account for this uncertainty is to model it.\nI realised that if you conduct robust design sampling and collect a swab with each capture, you could essentially model the swabbing process like an occupancy model within a mark-recapture model. That is, imagine surveying frogs \\(i \\in 1:I\\) during \\(j \\in 1:J\\) primary occasions (say every 2 months). During each primary occasion, you conduct multiple secondary surveys \\(k \\in 1:K_j\\) within a short amount of time where you’re willing to assume the population is closed, that is, no animals dying, coming in, changing their infection state, etc. Every time you capture a frog, including those captured multiple times within a primary, you collect a swab. There are 3 possible observed states (\\(o\\)) per secondary: (1) frog captured with swab Bd–, (2) frog captured with swab Bd+, and (3) frog not captured. The TPM of this observation process for repeat secondaries with false negatives and false positives is as follows, with ecological states in the rows and observed states in the columns:\n\\[\nP_o = \\begin{bmatrix}\n  p_1 \\cdot ( 1 - \\lambda_1) & p_2 \\cdot \\lambda_1 & 1 - p_1 \\\\\n  p_2 \\cdot (1 - \\delta_1) & p_2 \\cdot \\delta_1 & 1 - p_2 \\\\\n  0 & 0 & 1\n\\end{bmatrix}\n\\tag{11}\\] Where \\(p_{1:2}\\) are the detection probabilities of uninfected and infected frogs, respectively, \\(\\lambda_1\\) is the probability of getting a Bd+ swab from an uninfected frog (false positive), and \\(\\delta_2\\) is the probability of detecting Bd on a swab from an infected frog (where \\(1 - \\delta_1\\) is the false negative probability). The Bd detection parameters \\(\\delta_1\\) and \\(\\lambda_1\\) are identified if at least some frogs are recaptured multiple times within a primary occasion.\n\n\nDouble-decking the HMM\nThis is already a lot, but it’s really no different from Example 2: a continuous time ecological process, simpler this time because the dead state is absorbing, and a discrete time observation process, albeit with an extra false positive parameter. However, the reality of the data was slightly more complex. It is conventional to run qPCR several times per swab to accurately quantify the DNA load present in a sample. But this is analogous to the swabbing process described above. Presumably, it’s possible to get false negatives and false positives in the qPCR diagnostic process as well. There are thus three diagnostic states (\\(y\\), data): (1) qPCR replicate Bd–, (2) qPCR replicate Bd+, or (3) no qPCR performed, because no frog was captured and thus no swab was collected. Typically, people will just average their measured DNA loads across runs and apply an inclusion criterion, such as that samples are considered infected when 2/3 qPCR replicates return Bd DNA. But we can do better by just modeling it. Here’s the TPM of the diagnostic process, with latent observed states in the rows and diagnostic states in the columns:\n\\[\nP_y = \\begin{bmatrix}\n  1 - \\lambda_2 & \\lambda_2 & 0 \\\\\n  1 - \\delta_2 & \\delta_2 & 0 \\\\\n  0 & 0 & 1\n\\end{bmatrix}\n\\tag{12}\\] where \\(\\lambda_2\\) is another false positive parameter and \\(\\delta_2\\) is the detection probability in the qPCR process.\n\n\nAlways try to use all of the data\nAnother element of this model involved incorporating infection intensity into the model. qPCR doesn’t just tell you whether a sample is infected, but it tells you how many DNA copies are present in the sample. So with infection intensities \\(x\\) from multiple qPCR runs \\(l \\in 1:L\\) collected from multiple samples (\\(n\\)), we can actually estimate the latent infection intensity on the individual \\(m\\) and samples:\n\\[\n\\begin{aligned}\n  m_{i,j} &\\sim \\mathrm{Lognormal} \\left( \\mu, \\sigma_1 \\right) \\\\\n  n_{i,j,k} &\\sim \\mathrm{Lognormal} \\left( m_{i,j}, \\sigma_2 \\right) \\\\\n  x_{i,j,k,l} &\\sim \\mathrm{Lognormal} \\left( n_{i,j,k}, \\sigma_3 \\right)\n\\end{aligned}\n\\tag{13}\\] where lognormals are used to ensure the infection intensities are positive, \\(\\mu\\) is the log population average and \\(\\sigma_{1:3}\\) are the population standard deviation and the errors of the swabbing and qPCR processes, respectively. One could of course model the individual infection intensities \\(m_{ij}\\) more flexibly as individual-level time series, but my data was too sparse and probably way too noisy. We can now incorporate the infection intensities to the ecological and observation models by modeling \\(\\phi_2\\), the mortality rates of infected frogs, and \\(\\delta_{1:2}\\), the Bd detection probabilities, as functions of the relevant infection intensities, for example like this:\n\\[\n\\begin{aligned}\n  {\\phi_2}_{[i, j]} &= \\exp \\left( \\log {\\phi_2}_\\alpha + {\\phi_2}_\\beta \\cdot m_{i,j} \\right) \\\\\n  {\\delta_1}_{[i,j]} &= 1 - (1 - r_1)^{m_{i,j}} \\\\\n  {\\delta_2}_{[i,j,k]} &= 1 - (1 - r_2)^{n_{i,j,k}}\n\\end{aligned}\n\\tag{14}\\] Here, the first equation is a simple GLM of the mortality hazard rate as a function of time-varying individual infection intensity, and the Bd detection probabilities are modeled as a function of \\(r_{1:2}\\), or the probabilities of detecting one log gene copy of DNA on a swab or qPCR replicate.\nDr. Andy Royle and myself published this model in Methods in Ecology and Evolution where we initially implemented the model in NIMBLE (Hollanders and Royle 2022). But of course, I was keen to try to marginalise out the latent ecological and observed states to fit this model in Stan (note that the ecological states are completely unobserved, as there’s no way of knowing whether an infected qPCR run actually came from an infected sample or frog!).\n\n\nSimulation\nBelow I simulate some data where the rates of gaining and clearing infection are also affected by temperature (negative and positive, respectively), which seems to be the case for the frog-Bd system.4 To account for the varying infection dynamics in the state probabilities at first capture, I’ll use the steady state distribution for each primary which is computed using the infection dynamics probabilities, given by \\({\\psi_p}_{[1:2]} = 1 - \\exp(-\\psi_{1:2})\\). Then, the expected probability of being infected at first capture in each primary is \\(\\eta = \\frac{{\\psi_p}_{[1]}}{{\\psi_p}_{[1]} + {\\psi_p}_{[2]}}\\).\n\n\nCode\n# metadata\nI &lt;- 100\nJ &lt;- 8\nK_max &lt;- 3\nK &lt;- rep(K_max, J)\nL &lt;- 3\ntau &lt;- rlnorm(J - 1, log(1), 0.5)             # unequal primary occasion intervals\ntemp &lt;- rnorm(J - 1) |&gt; sort(decreasing = T)  # it's getting colder\n\n# parameters\nphi_a &lt;- c(0.1, 0.1)       # mortality rates of uninfected/infected frogs (intercepts)\nphi_b &lt;- 0.3               # effect of one log Bd gene copy on mortality\npsi_a &lt;- c(0.7, 0.4)       # rates of gaining/clearing infections (intercepts)\npsi_b &lt;- c(-0.4, 0.3)      # effect of temperature on rates of gaining/clearing infections\np_a &lt;- c(0.7, 0.6)         # detection probabilities of uninfected/infected frogs\nr &lt;- c(0.4, 0.6)           # pathogen detection probabilities (per log Bd gene copy)\nlambda &lt;- c(0.05, 0.10)    # sampling/diagnostic false positive probabilities\nmu &lt;- 1.0                  # population log average infection intensity\nsigma &lt;- c(0.3, 0.2, 0.1)  # population and sampling/diagnostic SDs\n\n# parameter containers\nphi &lt;- matrix(phi_a, 2, J - 1)\npsi &lt;- exp(matrix(log(psi_a), 2, J - 1) + psi_b %*% t(temp))\npsi_p &lt;- 1 - exp(-psi)\neta &lt;- psi_p[1, ] / colSums(psi_p)\np &lt;- array(p_a, c(2, J, K_max))\ndelta1 &lt;- numeric(J)\ndelta2 &lt;- matrix(NA, J, K_max)\nm &lt;- matrix(NA, I, J)\nn &lt;- array(NA, c(I, J, K_max))\n\n# TPM containers\nQ &lt;- P_z &lt;- array(NA, c(3, 3, J - 1))\nP_o &lt;- array(NA, c(3, 3, J, K_max))\nP_y &lt;- array(NA, c(3, 3, J, K_max))\n\n# primary occasions of first capture\nf &lt;- sort(sample(1:(J - 1), I, replace = T))\n\n# containers\nz &lt;- matrix(NA, I, J)\no &lt;- array(NA, c(I, J, K_max))\ny &lt;- x &lt;- array(NA, c(I, J, K_max, L))\n\n# simulate\nfor (i in 1:I) {\n  \n  # fix detection probabilities in first secondary of first primary to ensure capture\n  p[, f[i], 1] &lt;- 1\n  \n  for (j in f[i]:J) {\n    \n    # infection intensities and pathogen detection probabilities\n    m[i, j] &lt;- rlnorm(1, mu, sigma[1])\n    delta1[j] &lt;- 1 - (1 - r[1])^m[i, j]\n    n[i, j, 1:K[j]] &lt;- rlnorm(K[j], log(m[i, j]), sigma[2])\n    delta2[j, 1:K[j]] &lt;- 1 - (1 - r[2])^n[i, j, 1:K[j]]\n    \n    # observation and diagnostic TPMs\n    for (k in 1:K[j]) {\n      P_o[, , j, k] &lt;- matrix(c(p[1, j, k] * (1 - lambda[1]), p[1, j, k] * lambda[1], 1 - p[1, j, k],\n                                p[2, j, k] * (1 - delta1[j]), p[2, j, k] * delta1[j], 1 - p[2, j, k], \n                                0, 0, 1), \n                              3, byrow = T)\n      P_y[, , j, k] &lt;- matrix(c(1 - lambda[2], lambda[2], 0, \n                                1 - delta2[j, k], delta2[j, k], 0, \n                                0, 0, 1), \n                              3, byrow = T)\n    }\n  }\n  \n  # mortality rates and ecological TRM/TPMs\n  for (j in f[i]:(J - 1)) {\n    phi[2, j] &lt;- exp(log(phi_a[2]) + phi_b * m[i, j])\n    Q[, , j] &lt;- matrix(c(-(psi[1, j] + phi[1, j]), psi[1, j], phi[1, j], \n                         psi[2, j], -(psi[2, j] + phi[2, j]), phi[2, j], \n                         0, 0, 0), \n                       3, byrow = T)\n    P_z[, , j] &lt;- expm(Q[, , j] * tau[j])\n  }\n  \n  # ecological process\n  z[i, f[i]] &lt;- rcat(1, c(1 - eta[f[i]], eta[f[i]]))\n  for (j in (f[i] + 1):J) {\n    z[i, j] &lt;- rcat(1, P_z[z[i, j - 1], , j - 1])\n  }\n  \n  # observation and diagnostic process\n  for (j in f[i]:J) {\n    for (k in 1:K[j]) {\n      o[i, j, k] &lt;- rcat(1, P_o[z[i, j], , j, k])\n      y[i, j, k, ] &lt;- rcat(L, P_y[o[i, j, k], , j, k])\n      \n      # diagnostic run infection intensity\n      for (l in 1:L) {\n        if (y[i, j, k, l] == 2) {\n          x[i, j, k, l] &lt;- rlnorm(1, log(n[i, j, k]), sigma[3])\n        }\n      }\n    }\n  }\n}\n\n\n\n\nStan implementation\nThis model took me a lot of time to get going in Stan, but I got there in the end. I won’t go into detail about explaining everything, but I’ll mention a few key things here:\n\nWe only need to account for the pathogen detection components between an individual’s first and last primary of capture, because no samples are collected when an individual isn’t captured. This means \\(P_o\\) and \\(P_y\\) can be constructed as \\(2 \\cdot 2\\) matrices corresponding to the two alive states. Since this model doesn’t include any variation on detection, I created a vector lp_not_detected which holds the marginal log probabilities of not being detected for all three states, which is just \\(\\log\\left([1 - p_1, 1 - p_2, 1\\right]^\\intercal)\\).\nRobust design sampling means we can actually model the detection probabilities of the first primary. We do have to fix the first secondary that the individual was caught to 1, but are free to estimate the remaining detection probabilities.\nThe marginal log probabilities Omega are initialised with the observation and diagnostic TPMs for each secondary. This involves a sort of intermediate forward algorithm, where I (log) matrix multiply the observation TPM with the marginal log probabilities of the diagnostic outcomes (summing the log probabilities of each diagnostic run for each state, for which I created a row_sums() function).\nThe inclusion of false positives means the observation and diagnostic processes are examples of finite mixtures that may have multimodality in certain model constructions (Royle and Link 2006). Here, the true pathogen detection probabilities \\(\\delta\\) are modeled as a function of infection intensity, which may be enough to avoid the multimodality. Just to be sure, however, I’ve constrained \\(\\lambda_{1:2}\\) to be less than \\(r_{1:2}\\) in the program and further supplied \\(\\mathrm{Beta} (1, 10)\\) priors for \\(\\lambda\\).\nThe individual infection intensities m[i, j] are parameterised as centered lognormal and the sample infection intensities n[i, j, k] as non-centered lognormal, as this seemed to give the best HMC performance.\n\n\n\nCode\nfunctions {\n  // normalise a vector of log probabilities\n  vector normalise_log(vector A) {\n    return A - log_sum_exp(A);\n  }\n  \n  /**\n   * Return the natural logarithm of the product of the element-wise \n   * exponentiation of the specified matrices\n   *\n   * @param A  First matrix or (row_)vector\n   * @param B  Second matrix or (row_)vector\n   *\n   * @return   log(exp(A) * exp(B))\n   */\n  matrix log_prod_exp(matrix A, matrix B) {\n    int I = rows(A);\n    int J = cols(A);\n    int K = cols(B);\n    matrix[J, I] A_tr = A';\n    matrix[I, K] C;\n    for (k in 1:K) {\n      for (i in 1:I) {\n        C[i, k] = log_sum_exp(A_tr[:, i] + B[:, k]);\n      }\n    }\n    return C;\n  }\n  vector log_prod_exp(matrix A, vector B) {\n    int I = rows(A);\n    int J = cols(A);\n    matrix[J, I] A_tr = A';\n    vector[I] C;\n    for (i in 1:I) {\n      C[i] = log_sum_exp(A_tr[:, i] + B);\n    }\n    return C;\n  }\n  row_vector log_prod_exp(row_vector A, matrix B) {\n    int K = cols(B);\n    vector[size(A)] A_tr = A';\n    row_vector[K] C;\n    for (k in 1:K) {\n      C[k] = log_sum_exp(A_tr + B[:, k]);\n    }\n    return C;\n  }\n  real log_prod_exp(row_vector A, vector B) {\n    return log_sum_exp(A' + B);\n  }\n  \n  /**\n   * Row- or column sums of a matrix\n   *\n   * @param A  Matrix\n   *\n   * @return   (Row-)vector with the sums of each row or column\n   */\n  vector row_sums(matrix A) {\n    int I = rows(A);\n    vector[I] B;\n    for (i in 1:I) {\n      B[i] = sum(A[i]);\n    }\n    return B;\n  }\n  row_vector col_sums(matrix A) {\n    int J = cols(A);\n    row_vector[J] B;\n    for (j in 1:J) {\n      B[j] = sum(A[:, j]);\n    }\n    return B;\n  }\n}\n\ndata {\n  int&lt;lower=0&gt; I, J, K_max, L, S;\n  array[J] int&lt;lower=1, upper=K_max&gt; K;\n  array[I] int&lt;lower=1, upper=J&gt; f; \n  array[I] int&lt;lower=f, upper=J&gt; l; \n  array[I] int&lt;lower=1, upper=K_max&gt; f_k;\n  vector&lt;lower=0&gt;[J - 1] tau;\n  row_vector[J - 1] temp;\n  array[I, J, K_max, L] int&lt;lower=1, upper=3&gt; y;\n  array[S] int&lt;lower=1&gt; ind, prim, sec;\n  vector[S] x;\n}\n\ntransformed data {\n  array[I, J, K_max] int q;  // detected during secondary\n  int M = 0, N = 0;  // number of parameters for individual and sample loads\n  for (i in 1:I) {\n    for (j in f[i]:J) {\n      M += 1;\n      for (k in 1:K[j]) {\n        q[i, j, k] = min(y[i, j, k]) &lt; 3;\n        N += 1;\n      }\n    }\n  }\n}\n\nparameters {\n  vector&lt;lower=0&gt;[2] phi_a, psi_a;\n  real phi_b;\n  vector[2] psi_b;\n  vector&lt;lower=0, upper=1&gt;[2] p_a, r;\n  vector&lt;lower=0, upper=r&gt;[2] lambda;\n  real mu;\n  vector&lt;lower=0&gt;[3] sigma;   \n  vector[M] log_m;\n  vector[N] log_n_z;         \n}\n\nmodel {\n  // priors\n  target += exponential_lupdf(phi_a | 1);\n  target += std_normal_lupdf(phi_b);\n  target += exponential_lupdf(psi_a | 1);\n  target += std_normal_lupdf(psi_b);\n  target += beta_lupdf(p_a | 1, 1);\n  target += beta_lupdf(r | 1, 1);\n  target += beta_lupdf(lambda | 1, 10);\n  target += normal_lupdf(mu | 0, 1);\n  target += exponential_lupdf(sigma | 1);\n  target += normal_lupdf(log_m | mu, sigma[1]);\n  target += std_normal_lupdf(log_n_z);\n  \n  // fill in individual and sample infection containers\n  matrix[I, J] m;\n  array[I] matrix[J, K_max] n;\n  int m_idx = 1, n_idx = 1;\n  for (i in 1:I) {\n    for (j in f[i]:J) {\n      m[i, j] = log_m[m_idx];\n      m_idx += 1;\n      if (j &lt;= l[i]) {\n        for (k in 1:K[j]) {\n          n[i][j, k] = m[i, j] + log_n_z[n_idx] * sigma[2];\n          n_idx += 1;\n        }\n      }\n    }\n  }\n  \n  // likelihood of diagnostic infection intensities\n  for (s in 1:S) {\n    target += lognormal_lupdf(x[s] | n[ind[s]][prim[s], sec[s]], sigma[3]);\n  }\n  \n  // exponentiate log infection intensities\n  m = exp(m);\n  n = exp(n);\n  \n  // ecological rates and initial state log probabilities\n  matrix[2, J - 1] phi = rep_matrix(phi_a, J - 1);\n  matrix[2, J - 1] psi = exp(rep_matrix(log(psi_a), J - 1) + psi_b * temp);\n  matrix[2, J - 1] psi_p = 1 - exp(-psi);\n  row_vector[J - 1] eta = psi_p[1] ./ col_sums(psi_p);\n  row_vector[J - 1] log_eta = log(eta), log1m_eta = log1m(eta);\n  \n  // containers\n  vector[3] lp_not_detected = append_row(log1m(p_a), 0);  // undetected vector\n  tuple(row_vector[J], matrix[J, K_max]) delta;\n  array[J - 1] matrix[3, 3] Q, P_z;\n  matrix[2, 2] P_o, P_y;\n  matrix[3, J] Omega;\n  \n  // for each individual\n  for (i in 1:I) {\n    \n    // overwite mortality rates of infected\n    phi[2][f[i]:(J - 1)] = exp(log(phi_a[2]) + phi_b * m[i, f[i]:(J - 1)]);\n    \n    // ecological TRMs and (log) TPMs (Stan doesn't like the log of all 0s)\n    for (j in f[i]:(J - 1)) {\n      Q[j] = [[ -(psi[1, j] + phi[1, j]), psi[1, j], phi[1, j] ],\n              [ psi[2, j], -(psi[2, j] + phi[2, j]), phi[2, j] ],\n              zeros_row_vector(3) ];\n      P_z[j][1:2] = log(matrix_exp(Q[j] * tau[j])[1:2]);\n      P_z[j][3] = [ negative_infinity(), negative_infinity(), 0 ];\n    }\n    \n    // pathogen detection only needed for observed primaries\n    delta.1[f[i]:l[i]] = 1 - pow(1 - r[1], m[i, f[i]:l[i]]);\n    delta.2[f[i]:l[i]] = 1 - pow(1 - r[2], n[i][f[i]:l[i]]);\n    \n    // initialise log probabilities\n    Omega = rep_matrix(0, 3, J);\n    \n    // for each primary between first and last capture\n    for (j in f[i]:l[i]) {\n      for (k in 1:K[j]) {\n\n        // fix detection probablity to 1 at secondary of first capture\n        vector[2] p = (j == f[i] && k == f_k[i]) ? ones_vector(2) : p_a;\n        \n        // observation and diagnostic (log) TPMs\n        P_o = [[ p[1] * (1 - lambda[1]), p[1] * lambda[1] ],\n               [ p[2] * (1 - delta.1[j]), p[2] * delta.1[j] ]];\n        P_y = [[ 1 - lambda[2], lambda[2] ],\n               [ 1 - delta.2[j, k], delta.2[j, k] ]];\n        P_o = log(P_o);\n        P_y = log(P_y);\n\n        // increment observation and diagnostic process if detected\n        if (q[i, j, k]) {\n          Omega[1:2, j] += log_prod_exp(P_o, row_sums(P_y[:, y[i, j, k]]));\n        } else {\n          Omega[1:2, j] += lp_not_detected[1:2];\n        }\n      }\n    }\n\n    // increment ecological process up to last capture\n    Omega[1:2, f[i]] += [ log1m_eta[f[i]], log_eta[f[i]] ]';\n    for (j in (f[i] + 1):l[i]) {\n      Omega[1:2, j] += log_prod_exp(P_z[j - 1][1:2, 1:2]', Omega[1:2, j - 1]);\n    }\n    Omega[3, l[i]] = negative_infinity();\n\n    // increment ecological and observation process after last capture\n    for (j in (l[i] + 1):J) {\n      Omega[:, j] += log_prod_exp(P_z[j - 1]', Omega[:, j - 1]);\n      for (k in 1:K[j]) {\n        Omega[:, j] += lp_not_detected;\n      }\n    }\n\n    // increment log density\n    target += log_sum_exp(Omega[:, J]);\n  }\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I, J] int z;\n  {\n    matrix[I, J] m;\n    array[I] matrix[J, K_max] n;\n    int m_idx = 0, n_idx = 0;\n    for (i in 1:I) {\n      for (j in f[i]:J) {\n        m_idx += 1;\n        m[i, j] = log_m[m_idx];\n        if (j &lt;= l[i]) {\n          for (k in 1:K[j]) {\n            n_idx += 1;\n            n[i][j, k] = m[i, j] + log_n_z[n_idx] * sigma[2];\n          }\n        }\n      }\n    }\n    m = exp(m);\n    n = exp(n);\n    matrix[2, J - 1] psi = exp(rep_matrix(log(psi_a), J - 1) + psi_b * temp);\n    matrix[2, J - 1] psi_p = 1 - exp(-psi);\n    row_vector[J - 1] eta = psi_p[1] ./ col_sums(psi_p);\n    row_vector[J - 1] log_eta = log(eta), log1m_eta = log1m(eta);\n    vector[3] lp_not_detected = append_row(log1m(p_a), 0);\n    tuple(row_vector[J], matrix[J, K_max]) delta;\n    array[J - 1] matrix[3, 3] Q, P_z;\n    matrix[2, 2] P_o, P_y;\n    matrix[3, J] Omega;\n    for (i in 1:I) {\n      matrix[2, J - 1] phi;\n      phi[1] = rep_row_vector(phi_a[1], J - 1);\n      phi[2][f[i]:(J - 1)] = exp(log(phi_a[2]) + phi_b * m[i, f[i]:(J - 1)]);\n      for (j in f[i]:(J - 1)) {\n        Q[j] = [[ -(psi[1, j] + phi[1, j]), psi[1, j], phi[1, j] ],\n                [ psi[2, j], -(psi[2, j] + phi[2, j]), phi[2, j] ],\n                zeros_row_vector(3) ];\n        P_z[j][1:2] = log(matrix_exp(Q[j] * tau[j])[1:2]);\n        P_z[j][3] = [ negative_infinity(), negative_infinity(), 0 ];\n      }\n      delta.1[f[i]:l[i]] = 1 - pow(1 - r[1], m[i, f[i]:l[i]]);\n      delta.2[f[i]:l[i]] = 1 - pow(1 - r[2], n[i][f[i]:l[i]]);\n      Omega = rep_matrix(0, 3, J);\n      for (j in f[i]:l[i]) {\n        for (k in 1:K[j]) {\n          vector[2] p = (j == f[i] && k == f_k[i]) ? ones_vector(2) : p_a;\n          P_o = [[ p[1] * (1 - lambda[1]), p[1] * lambda[1] ],\n                 [ p[2] * (1 - delta.1[j]), p[2] * delta.1[j] ]];\n          P_y = [[ 1 - lambda[2], lambda[2] ],\n                 [ 1 - delta.2[j, k], delta.2[j, k] ]];\n          P_o = log(P_o);\n          P_y = log(P_y);\n          \n          // forward algorithm for log likelihood\n          if (q[i, j, k]) {\n            Omega[1:2, j] += log_prod_exp(P_o, row_sums(P_y[:, y[i, j, k]]));\n          } else {\n            Omega[1:2, j] += lp_not_detected[1:2];\n          }\n        }\n      }\n      Omega[1:2, f[i]] += [ log1m_eta[f[i]], log_eta[f[i]] ]';\n      for (j in (f[i] + 1):l[i]) {\n        Omega[1:2, j] += log_prod_exp(P_z[j - 1][1:2, 1:2]', Omega[1:2, j - 1]);\n      }\n      Omega[3, l[i]] = negative_infinity();\n      for (j in (l[i] + 1):J) {\n        Omega[:, j] += log_prod_exp(P_z[j - 1]', Omega[:, j - 1]);\n        for (k in 1:K[j]) {\n          Omega[:, j] += lp_not_detected;\n        }\n      }\n      log_lik[i] = log_sum_exp(Omega[:, J]);\n      \n      // backward algorithm for ecological states\n      z[i, J] = categorical_rng(exp(normalise_log(Omega[:, J])));\n      for (j in (f[i]) + 1:J) {\n        int jj = J + f[i] - j;\n        z[i, jj] = categorical_rng(exp(normalise_log(Omega[:, jj]\n                                                     + P_z[jj][:, z[i, jj + 1]])));\n      }\n    }\n  }\n}\n\n\nNext I convert the qPCR infection intensities to long format, prepare the data for Stan, and fit the model.\n\n\nCode\n# convert diagnostic infection intensities to long format\nx_long &lt;- reshape2::melt(x, value.name = \"x\", varnames = c(\"i\", \"j\", \"k\", \"l\")) |&gt; \n  as_tibble() |&gt;\n  drop_na()\n\n# data for Stan\nf_k &lt;- rep(1, I)  # simulated to be first captured in first secondary\nl &lt;- apply(y, 1:2, min, na.rm = T) |&gt; apply(1, \\(y) max(which(y &lt; 3)))\nmultievent_data &lt;- list(I = I, J = J, K_max = K_max, L = L, S = nrow(x_long),\n                        K = K, f = f, l = l, f_k = f_k, tau = tau, temp = temp,\n                        y = ifelse(is.na(y), 3, y),\n                        ind = x_long$i, prim = x_long$j, sec = x_long$k,\n                        x = x_long$x)\n\n\n# run HMC\nfit_multievent &lt;- multievent$sample(data = multievent_data, refresh = 0, chains = n_chains, \n                                    iter_warmup = n_iter, iter_sampling = n_iter)\n\n\nRunning MCMC with 8 parallel chains...\n\n\nChain 1 finished in 375.4 seconds.\nChain 4 finished in 421.9 seconds.\nChain 6 finished in 422.6 seconds.\nChain 2 finished in 475.8 seconds.\nChain 3 finished in 502.1 seconds.\nChain 7 finished in 527.8 seconds.\nChain 8 finished in 586.8 seconds.\nChain 5 finished in 661.5 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 496.7 seconds.\nTotal execution time: 661.8 seconds.\n\n\n\n\nResults\nWe’ll visually check the parameter estimates with the simulation input and see that it was recovered well. For what it’s worth, in the paper we showed that the infection dynamics get overestimated significantly (5-fold in our application) when not accounting for state misclassification.\n\n\nCode\nfit_multievent |&gt;\n  gather_rvars(phi_a[i], phi_b, psi_a[i], psi_b[i], p_a[i], r[i], lambda[i], mu, sigma[i]) |&gt;\n  mutate(truth = c(phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, mu, sigma),\n         parameter = str_extract(.variable, \"^[^_]+\") |&gt; \n           fct_inorder(),\n         process = case_when(str_detect(.variable, \"phi|psi\") ~ \"Ecological Process\",\n                             str_detect(.variable, \"p_a|r|lambda\") ~ \"Observation Process\",\n                             .default = \"Infection Intensities\") |&gt; \n           fct_inorder(),\n         variable = case_when(str_detect(.variable, \"_a\") ~ str_c(parameter, \"[\", i, \"[alpha]]\"),\n                              .variable == \"phi_b\" ~ \"phi[beta]\",\n                              .variable == \"psi_b\" ~ str_c(parameter, \"[\", i, \"[beta]]\"),\n                              .variable %in% c(\"r\", \"lambda\", \"sigma\") ~ str_c(parameter, \"[\", i, \"]\"),\n                              .default = .variable) |&gt; \n           fct_reorder(as.numeric(parameter))) |&gt; \n  ggplot(aes(xdist = .value, y = fct_rev(variable))) + \n  facet_wrap(~ process, ncol = 1, scales = \"free_y\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", colour = \"#333333\") +\n  stat_pointinterval(point_interval = median_qi, .width = 0.9, linewidth = 0.5, position = position_nudge(y = 0.1)) +\n  geom_point(aes(truth), colour = green, position = position_nudge(y = -0.1)) +\n  scale_x_continuous(breaks = seq(-1, 1, 0.5), expand = c(0, 0)) +\n  scale_y_discrete(labels = ggplot2:::parse_safe) +\n  labs(x = \"Posterior\", y = \"Parameter\")\n\n\n\n\n\n\n\n\n\nAgain, we might check to see how well we recovered our latent ecological states, which were completely unobservable from the data. This worked pretty well!\n\n\nCode\nout &lt;- fit_multievent |&gt; \n  spread_rvars(z[i, j]) |&gt; \n  mutate(truth = c(.env$z)) |&gt; \n  left_join(tibble(i = 1:I, \n                   f = f), \n            by = \"i\") |&gt; \n  drop_na()\nsummarise(out, prop_true = mean(median(z) == truth))\n\n\n# A tibble: 1 × 1\n  prop_true\n      &lt;dbl&gt;\n1     0.908\n\n\nOne of the main reasons to account for imperfect pathogen detection is to get more realistic estimates of infection prevalence in the population. We can use the full posterior distribution of the latent ecological states from the backward algorithm to estimate the infection prevalence per primary occasion, which here was simulated to be increasing due to dropping temperatures going into austral winter plotted for some fictional dates to show the unequal primary occasion intervals.\n\n\nCode\n# dates\nstart &lt;- ymd(\"2020-12-01\")\ntime_unit &lt;- 21\ndates &lt;- seq.Date(start, start + days(1 + round(sum(tau) * time_unit)), by = 1)\nprim &lt;- dates[c(1, sapply(1:(J - 1), \\(t) round(sum(tau[1:t] * time_unit))))]\n\n# plot\nlibrary(posterior)\nout |&gt; \n  left_join(tibble(j = 1:J, \n                   prim = prim), \n            by = \"j\") |&gt; \n  summarise(prev = rvar_sum(z == 2) / rvar_sum(z &lt; 3),\n            truth = sum(truth == 2) / sum(truth &lt; 3),\n            .by = prim) |&gt; \n  ggplot(aes(prim, truth, ydist = prev)) + \n  stat_pointinterval(point_interval = median_qi, .width = 0.9, linewidth = 0.5, position = position_nudge(x = 0.2)) +\n  geom_point(colour = green, position = position_nudge(x = -2)) + \n  scale_y_continuous(breaks = seq(0.2, 1, 0.2), limits = c(0, 1.001), expand = c(0, 0)) +\n  labs(x = \"Primary Occasion\", y = \"Infection Prevalence\")\n\n\n\n\n\n\n\n\n\nThanks for reading if you made it this far. I hope this guide will be useful for ecologists (and others) wanting to transition to gradient-based sampling methods for complex ecological models. I’m sure I’ve some errors somewhere, so if someone finds any or has any questions, please don’t hesitate to reach out."
  },
  {
    "objectID": "blog/hmm-in-stan.html#footnotes",
    "href": "blog/hmm-in-stan.html#footnotes",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Betancourt (2018) for a deep dive into HMC.↩︎\nAccording to the Stan User’s Guide↩︎\nSumming probabilities on the log scale requires exponentiating the log probabilities, summing them, and taking the logarithm once again.↩︎\nThere are some identifiability issues here, reflected in the posterior correlations that exist between the intercepts and slopes.↩︎"
  },
  {
    "objectID": "portfolio/dmso.html",
    "href": "portfolio/dmso.html",
    "title": "Controlling for DMSO",
    "section": "",
    "text": "Kate Summer et al. recently published an open access manuscript in Biofilm, for which Dr. Matthijs Hollanders from Quantecol conducted the statistical analysis.\nThe authors make a strong case for controlling for the effects of the solvent dimethyl-sulfoxide (DMSO) in biofilm studies. In addition to a literature review of 76 published studies, Summer conducted experiments on the bacteria Streptococcus pneumoniae and Pseudomonas aeruginosa to determine to what extent DMSO affected biofilm formation. We analysed the microbial activity and biofilm inhibition in these experiments using hormetic dose-response models."
  },
  {
    "objectID": "portfolio/subtropical-cats.html",
    "href": "portfolio/subtropical-cats.html",
    "title": "Estimating feral cat densities in subtropical rainforest",
    "section": "",
    "text": "Dr. Darren McHugh led an extensive field project employing camera traps in Border Ranges National Park—a large World Heritage listed subtropical rainforest reserve in northeast New South Wales, Australia—to estimate the density of feral cats (Felis catus). Feral cats are a leading cause of species decline throughout Australia, particularly for mammals which in Australia experience the highest extinction rates globally. Unfortunately, the research revealed that cat densities were more than triple the national average, a surprising result in light of popular beliefs about rainforest cat populations.\nDr. Matthijs Hollanders from Quantecol worked alongside Dr. Ben Augustine to fit his state-of-the-art spatial mark-recapture model that incorporates detections that could not be identified to individuals. Since the majority of cats detected on cameras could not be individually assigned due to their similar pelage types, being able to still incorporate these observations into the analysis increases the statistical power and yields improved estimates of abundance.\nThe work was published today as an Open Access article in Wildlife Research and presents the most thorough assessment of cat densities in Australian rainforests to date."
  }
]