[
  {
    "objectID": "blog/gaussian-processes.html",
    "href": "blog/gaussian-processes.html",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "",
    "text": "It is common to use random effects to capture heterogeneity in data that have been collected at spatial and/or temporal intervals. For example, counts of species or individuals are frequently replicated in space and time and researchers may add site or survey date as random effects in the model. These random effects are typically modeled as being normally distributed. Consider surveys \\(i \\in 1:I\\) where counts \\(y_i\\) are made which we model with Poisson regression. The model with normally distributed random survey effects on the log expected count looks like this:\n\\[\n\\begin{aligned}\n  y_i &\\sim \\textrm{Poisson} \\left( \\exp(\\alpha + \\epsilon_i) \\right) \\\\\n  \\epsilon_i &\\sim \\textrm{Normal} \\left( 0, \\tau \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere the intercept \\(\\alpha\\), the random survey-level offsets \\(\\epsilon_i\\), and the standard deviation of the survey effects \\(\\tau\\) are to be estimated by the model. In this post, I argue that when the random effects are not expected to be independent it almost always makes more sense to model the random effects using Gaussian processes (GPs).\nGPs are a bit complicated (Simpson 2021) but one of their salient features is that any set of observations generated by a Gaussian process are marginally distributed as multivariate normal \\(\\textrm{Normal} (\\boldsymbol{\\mu}, \\Sigma)\\).1 In practice, the realisations of the GP are added to our linear predictor so \\(\\boldsymbol{\\mu} = \\boldsymbol{0}\\). There are quite a few covariance kernels \\(\\Sigma\\) to choose from but one of the most common is the exponentiated quadratic kernel given by \\[\n\\Sigma_{ij} = \\tau^2 \\exp \\left( -\\frac{\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert^2}{2 \\rho ^2} \\right),\n\\tag{2}\\]\nwhere \\(\\tau^2\\) is the marginal variance (analogous to variance \\(\\tau^2\\) with normally distributed random effects), \\(\\rho\\) is the length-scale governing the covariance between observations, and the term \\(\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert\\) is the Euclidean distance between any two points of our input.2 The length-scale \\(\\rho\\) governs the correlation between points as a function of the distance between them. When \\(\\rho\\) is small, there is little correlation between neighbouring locations and when \\(\\rho\\) is large, there is high correlation between observations. Essentially, GPs allow modeling normally distributed random effects where observations close to each other (as in, surveys conducted at the same time of year) may be correlated."
  },
  {
    "objectID": "blog/gaussian-processes.html#introduction",
    "href": "blog/gaussian-processes.html#introduction",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "",
    "text": "It is common to use random effects to capture heterogeneity in data that have been collected at spatial and/or temporal intervals. For example, counts of species or individuals are frequently replicated in space and time and researchers may add site or survey date as random effects in the model. These random effects are typically modeled as being normally distributed. Consider surveys \\(i \\in 1:I\\) where counts \\(y_i\\) are made which we model with Poisson regression. The model with normally distributed random survey effects on the log expected count looks like this:\n\\[\n\\begin{aligned}\n  y_i &\\sim \\textrm{Poisson} \\left( \\exp(\\alpha + \\epsilon_i) \\right) \\\\\n  \\epsilon_i &\\sim \\textrm{Normal} \\left( 0, \\tau \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere the intercept \\(\\alpha\\), the random survey-level offsets \\(\\epsilon_i\\), and the standard deviation of the survey effects \\(\\tau\\) are to be estimated by the model. In this post, I argue that when the random effects are not expected to be independent it almost always makes more sense to model the random effects using Gaussian processes (GPs).\nGPs are a bit complicated (Simpson 2021) but one of their salient features is that any set of observations generated by a Gaussian process are marginally distributed as multivariate normal \\(\\textrm{Normal} (\\boldsymbol{\\mu}, \\Sigma)\\).1 In practice, the realisations of the GP are added to our linear predictor so \\(\\boldsymbol{\\mu} = \\boldsymbol{0}\\). There are quite a few covariance kernels \\(\\Sigma\\) to choose from but one of the most common is the exponentiated quadratic kernel given by \\[\n\\Sigma_{ij} = \\tau^2 \\exp \\left( -\\frac{\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert^2}{2 \\rho ^2} \\right),\n\\tag{2}\\]\nwhere \\(\\tau^2\\) is the marginal variance (analogous to variance \\(\\tau^2\\) with normally distributed random effects), \\(\\rho\\) is the length-scale governing the covariance between observations, and the term \\(\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert\\) is the Euclidean distance between any two points of our input.2 The length-scale \\(\\rho\\) governs the correlation between points as a function of the distance between them. When \\(\\rho\\) is small, there is little correlation between neighbouring locations and when \\(\\rho\\) is large, there is high correlation between observations. Essentially, GPs allow modeling normally distributed random effects where observations close to each other (as in, surveys conducted at the same time of year) may be correlated."
  },
  {
    "objectID": "blog/gaussian-processes.html#example-tadpole-counts",
    "href": "blog/gaussian-processes.html#example-tadpole-counts",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Example: tadpole counts",
    "text": "Example: tadpole counts\nFor this example, I’ll use data we collected on tadpoles of Fleay’s barred frogs (Mixophyes fleayi), a stream-dwelling species endemic to the Gondwana rainforests of northern New South Wales and southeast Queensland, Australia (Hollanders et al. 2024). During my PhD, I swept a dipnet through pools in the creeks for roughly 1 min every six weeks, deposited the tadpoles in a tub, took a photograph from above, and used Photoshop to count and measure the tadpoles.3 Here, I’ll fit the model in Equation 1 with and without GP to model the number of tadpoles I captured during each six-weekly survey at one of the sites. First I’ll download the data from my GitHub and plot the counts for the creek using ggplot2 (Wickham 2016) and other packages from the tidyverse (Wickham et al. 2019).\n\n\nCode\n# download data\nlibrary(tidyverse)\ndat &lt;- read_csv(\"https://raw.githubusercontent.com/mhollanders/mfleayi-tadpoles/main/data/tadpole-lengths.csv\") |&gt; \n  filter(site == \"brindle\") |&gt; \n  mutate(date = dmy(date)) |&gt; \n  count(date)\n\n# plot the counts\nfig &lt;- dat |&gt; \n  ggplot(aes(date, n)) + \n  geom_point(colour = green, shape = 16, size = 3, alpha = 3/4) + \n  scale_y_continuous(breaks = seq(50, 200, 50), limits = c(0, 225), expand = c(0, 0)) + \n  labs(x = \"Survey\", y = \"Number of tadpoles\")\nfig\n\n\n\n\n\nFleay’s barred frogs breed from the early spring to the middle of summer and generally there are several pulses of reproduction resulting in fresh cohorts of tadpoles. We’ll ignore this and just model the counts using random survey effects.\n\n\n\nFleay’s barred frog (Mixophyes fleayi) from Brindle Creek, Border Ranges National Park, Australia."
  },
  {
    "objectID": "blog/gaussian-processes.html#setting-priors-for-gps",
    "href": "blog/gaussian-processes.html#setting-priors-for-gps",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Setting priors for GPs",
    "text": "Setting priors for GPs\nGPs have two parameters that require priors, \\(\\tau\\) and \\(\\rho\\). The prior for \\(\\tau\\) can just be something like an exponential prior commonly used for standard deviations of random effects.4 The prior for the length-scale \\(\\rho\\) is trickier as it is often not well identified by the data. In fact, the data cannot inform length-scales that are outside of the distances observed in the data. This means that we want a prior than can suppress values below and above certain thresholds. One popular distribution for length-scales that’s capable of doing exactly this is the inverse gamma distribution. Michael Betancourt’s GP case study discusses this at length and includes a function that uses Stan’s algebra solver to determine the shape and scale of the inverse gamma distribution that places a certain proportion of the probability mass between two values.5\nIf the study was well designed to estimate the length-scale of the GP, the survey intervals should be shorter than the minimum length-scale we think we might be dealing with. Unfortunately, I did not collect data with this question in mind, and I can easily see that the relevant length-scale is shorter than the minimum length between surveys (I surveyed at six-weekly intervals, give or take). However, for this demonstration, we’ll assume that the survey intervals were rigorously chosen and we’ll use the minimum and maximum observed temporal distances between surveys to choose a suitable inverse gamma prior for the length-scale. So, I compute the minimum and maximum distance and chose tail probabilities of 1% so that 98% of the probability mass of the length-scale falls between the minimum and maximum observed distances between the surveys."
  },
  {
    "objectID": "blog/gaussian-processes.html#fitting-the-model",
    "href": "blog/gaussian-processes.html#fitting-the-model",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Fitting the model",
    "text": "Fitting the model\nBelow I write out a Stan program (Carpenter et al. 2017) that uses an indicator to use either normally distributed random effects or to use a GP with a exponentiated quadratic covariance kernel. Stan includes the handy function gp_exp_quad_cov() that generates this covariance kernel from the input (survey times expressed in weeks, in this case) and standard deviation and length-scale. We also use this GP input to pre-compute the distance matrix to get the minimum and maximum observed (temporal) distances of the surveys to use Stan’s algebra solver to determine a suitable inverse gamma prior for \\(\\rho\\). For both types of random effects I use the non-centered parameterisation which generally has better sampling for few observations (we only have 10 data points). For GPs (and multivariate normals more generally), this entails Cholesky decomposing the covariance matrix (think of it as a matrix square root) and multiplying it by a vector of standard normal variates. Note that before Cholesky decomposing the covariance kernel, I add a diagonal matrix with a very small value (called the “nugget” or “jitter”) to ensure the matrix is positive definite.\n\n\nCode\nfunctions {\n  // function from Michael Betancourt for inverse gamma prior shape and scale\n  vector inv_gamma_prior(vector guess, vector theta, array[] real tails, array[] int x_i) {\n    vector[2] params;\n    params[1] = inv_gamma_cdf(theta[1] | guess[1], guess[2]) - tails[1];\n    params[2] = inv_gamma_cdf(theta[2] | guess[1], guess[2]) - tails[2];\n    return params;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; I;  // number of surveys\n  array[I] real survey;  // survey input as number of weeks\n  vector[2] min_max_dist;  // minimum and maximum observed distance between weeks\n  array[I] int y;  // tadpole counts per survey\n  int&lt;lower=0, upper=1&gt; GP;  // indicator for GP (0 = no, 1 = yes)\n}\n\ntransformed data {\n  vector&lt;lower=0&gt;[2] rho_prior = ones_vector(2);  // length-scale prior shape and scale\n  if (GP) {\n    rho_prior = algebra_solver(inv_gamma_prior, [1, 1]', min_max_dist, {0.01, 0.99}, {0});\n    print(\"rho_prior: \", rho_prior);\n  }\n}\n\nparameters {\n  real alpha;  // intercept\n  real&lt;lower=0&gt; tau, rho;  // random effect SD and GP length-scale\n  vector[I] z;  // standard-normal z-scores for non-centered parameterisation\n}\n\ntransformed parameters {\n  vector[I] epsilon;  // random survey effects\n  if (GP) {\n    epsilon = cholesky_decompose(add_diag(gp_exp_quad_cov(survey, tau, rho), 1e-9)) * z;\n  } else {\n    epsilon = tau * z;\n  }\n}\n\nmodel {\n  // priors\n  alpha ~ normal(0, 5);\n  tau ~ exponential(1);\n  rho ~ inv_gamma(rho_prior[1], rho_prior[2]);\n  z ~ std_normal();\n    \n  // likelihood\n  y ~ poisson_log(alpha + epsilon);\n}\n\ngenerated quantities {\n  vector[I] yrep, log_lik;\n  for (i in 1:I) {\n    yrep[i] = poisson_log_rng(alpha + epsilon[i]);\n    log_lik[i] = poisson_log_lpmf(y[i] | alpha + epsilon[i]);\n  }\n}\n\n\nI’ll fit both the normal and GP model with CmdStanR (Gabry et al. 2024) in R 4.4.1 (R Core Team 2024).\n\n\nCode\n# prepare data for Stan\nstan_data &lt;- list(I = nrow(dat), \n                  survey = as.numeric(dat$date) / 7, \n                  y = dat$n, \n                  GP = 0)\n\n# get distance matrix and add minimum and maximum observed distances\ndist_obs &lt;- dist(stan_data$survey) |&gt; as.matrix()\nstan_data$min_max_dist &lt;- range(dist_obs[dist_obs &gt; 0])\n\n# normal model\nfit_normal &lt;- mod$sample(data = stan_data, \n                         refresh = 0, chains = 1, \n                         iter_warmup = 500, iter_sampling = 4000, \n                         show_exceptions = F)\n\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.3 seconds.\n\n\nCode\n# GP model\nstan_data$GP &lt;- 1\nfit_gp &lt;- mod$sample(data = stan_data, \n                     refresh = 0, chains = 1, \n                     iter_warmup = 500, iter_sampling = 4000, \n                     show_exceptions = F)\n\n\nRunning MCMC with 1 chain...\n\nChain 1 rho_prior: [3.58235,38.8276] \nChain 1 finished in 1.4 seconds.\n\n\nI’ll first plot the inverse gamma prior for the length-scale that was generated by the solver (green), alongside the posterior distribution of \\(\\rho\\) (black) and the minimum and maximum observed temporal distances of the surveys (dashed lines), using the distributional (O’Hara-Wild et al. 2024) and ggdist and tidybayes packages (Kay 2024a, 2024b). The length-scale seems to want to be smaller than the prior allows, which is due to me enforcing an informative prior. Recall that I pretended the survey intervals were chosen in a principled manner to estimate this parameter by specifying a prior implying the length-scale really doesn’t be much shorter than six weeks.\n\n\nCode\n# packages\nlibrary(distributional)\nlibrary(ggdist)\nlibrary(tidybayes)\n\n# plot\ntibble(prior = dist_inverse_gamma(3.58, scale = 38.83)) |&gt; \n  ggplot(aes(xdist = prior)) + \n  geom_vline(xintercept = stan_data$min_max_dist, \n             linetype = \"dashed\", linewidth = 1/2, colour = \"#333333\") +\n  stat_slab(fill = green, alpha = 2/3) + \n  stat_slab(aes(xdist = rho), \n            data = spread_rvars(fit_gp, rho), \n            fill = \"#333333\", alpha = 2/3) + \n  scale_x_continuous(breaks = seq(0, 60, 20), limits = c(0, 70), expand = c(0, 0)) + \n  scale_y_continuous(breaks = NULL, expand = c(0, 0)) + \n  theme(panel.border = element_rect(colour = NA), \n        axis.line.x = element_line(colour = \"#333333\")) + \n  labs(x = expression(rho), y = NULL)\n\n\n\n\n\nFocusing now on the posterior draws for the random survey effects \\(\\epsilon_i\\), it’s clear that the predictions between the normal and GP model are very similar.\n\n\nCode\n# add predictions to figure\nlibrary(tidybayes)\nfig + \n  stat_pointinterval(aes(ydist = exp(alpha + epsilon), \n                         shape = factor(model) |&gt; fct_rev()), \n                     data = fit_normal |&gt; \n                       spread_rvars(alpha, epsilon[i]) |&gt; \n                       mutate(model = \"Normal\") |&gt; \n                       bind_cols(dat) |&gt; \n                       bind_rows(\n                         fit_gp |&gt; \n                           spread_rvars(alpha, epsilon[i]) |&gt; \n                           mutate(model = \"GP\") |&gt; \n                           bind_cols(dat)\n                       ), \n                     position = position_dodge(width = 40),\n                     point_interval = \"median_hdci\", .width = 0.95, \n                     size = 1, linewidth = 1/2) + \n  scale_colour_manual(values = c(\"#333333\", green)) + \n  labs(shape = \"Model\")\n\n\n\n\n\nWe can also see that the standard deviation of the normal model is very similar to the marginal standard deviation of the GP model, albeit with more uncertainty. To be honest, the difference in the estimates of \\(\\tau\\) here may be driven more by our prior on \\(\\rho\\) that may be a bit more informative than it should be.\n\n\nCode\nfit_normal |&gt; \n  spread_rvars(tau) |&gt; \n  mutate(model = \"Normal\") |&gt; \n  bind_rows(fit_gp |&gt; \n              spread_rvars(tau) |&gt; \n              mutate(model = \"GP\")) |&gt; \n  ggplot(aes(factor(model) |&gt; fct_rev(), ydist = tau)) +\n  stat_pointinterval(point_interval = \"median_hdci\", .width = 0.95, \n                     size = 1/2, linewidth = 1/2) + \n  scale_colour_manual(values = c(\"#333333\", \"red4\")) + \n  labs(x = \"Model\", y = expression(paste(\"Posterior distribution of \", tau)))"
  },
  {
    "objectID": "blog/gaussian-processes.html#conclusion",
    "href": "blog/gaussian-processes.html#conclusion",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Conclusion",
    "text": "Conclusion\nUsing GPs to model random effects makes a lot of sense if there is some interest in modeling correlation between temporally or spatially organised data. Given that switching out the normally distributed random effects for a GP is trivial in modern probabilistic programming languages such as Stan, it makes sense to default to using them, especially as it includes involves just one extra parameter, the length-scale \\(\\rho\\)."
  },
  {
    "objectID": "blog/gaussian-processes.html#footnotes",
    "href": "blog/gaussian-processes.html#footnotes",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDan Simpson’s blog is a great place to learn more about GPs.↩︎\nWhen our input is multidimensional, such as coordinates for spatial data, we are still working with the Euclidean distance between vectors.↩︎\nI also did other things with those tadpoles, but I won’t go into that here.↩︎\nThis also happens to the penalised complexity (PC) prior for this parameter.↩︎\nThe case study is a great place to learn more about GPs more generally.↩︎"
  }
]