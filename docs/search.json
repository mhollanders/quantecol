[
  {
    "objectID": "portfolio/fleayi-adult.html",
    "href": "portfolio/fleayi-adult.html",
    "title": "Amphibian disease resistance",
    "section": "",
    "text": "Dr. Matthijs Hollanders et al. conducted a four-year mark-recapture study on Fleay’s barred frogs (Mixophyes fleayi) across three sites in northern New South Wales, Australia, to assess their current susceptibility to the amphibian disease chytridiomycosis.\nThe research was recently published as an open access article in Ecological Applications, where they argue that this species shows hallmarks of the evolution of pathogen resistance. Using disease-structured multistate models incorporating individual pathogen loads measured during recapture events, they show that although some individuals likely succumb to the disease, overall mortality was low and frogs were much more likely to clear their infections than to gain them. Good news!\nThis research was featured in The Conversation and ABC News."
  },
  {
    "objectID": "portfolio/multievent.html",
    "href": "portfolio/multievent.html",
    "title": "Novel multievent mark-recapture model",
    "section": "",
    "text": "Dr. Matthijs Hollanders and Dr. J. Andrew Royle recently developed a novel multievent mark-recapture model that accounts for infection state assignment errors. By estimating the false-negative and false-positive probabilities in the disease detection protocols, these errors can be propagated and accounted for while estimating the ecological process of interest. They used simulations and a case study with Fleay’s barred frog (Mixophyes fleayi) infected with the amphibian chytrid fungus Batrachochytrium dendrobatidis as a case study.\nThey found that infection prevalence was underestimated by \\(\\frac{1}{3}\\) while the rates of gaining and clearing infections were overestimated by factors of 4–5 when state misclassification was not accounted for. This was largely due to the limited ability of swab samples to detect low-level infections of the chytrid fungus.\nThe research was published as an Open Access article in Methods in Ecology and Evolution, and includes code to simulate and analyse your own datasets in the Supporting Information and on GitHub."
  },
  {
    "objectID": "portfolio/christmas-island.html",
    "href": "portfolio/christmas-island.html",
    "title": "Reintroducing Extinct in the Wild reptiles",
    "section": "",
    "text": "Dr. Jon-Paul Emery and colleagues conducted experimental releases of the Extinct in the Wild (EW) blue-tailed skinks (Cryptoblepharus egeriae) and Lister’s geckos (Lepidodactylus listeri) on Christmas Island. These endemic reptiles disappeared from the wild following the establishment of the invasive common wolf snake (Lycodon capucinus) on the island in the 1980s. In 2018–2019, 170 skinks and 160 geckos were released into a predator-free area after which they were monitored with robust design mark-recapture surveys for one year post-release.\nDr. Matthijs Hollanders from Quantecol got involved with the analysis of the survey data, where we built a continuous time Jolly-Seber model to estimate per-capita recruitment, mortality rates of juvenile and adult skinks and geckos, and population sizes. Blue-tailed skinks flourished with high survival and recruitment rates leading to an increasing population size. Unfortuantely, Lister’s geckos did not fare so well with a steadily decreasing population size and no evidence of recruitment.\nThe work was published today as an Open Access article in Animal Conservation and represents an important contribution to conservation science."
  },
  {
    "objectID": "blog/marginalisation.html",
    "href": "blog/marginalisation.html",
    "title": "Marginalising discrete variables in ecological models",
    "section": "",
    "text": "Bayesian methods are popular for ecological modeling because of the flexibility afforded by software like JAGS, NIMBLE, and Stan. JAGS and NIMBLE are derivatives of the BUGS (Bayesian inference Using Gibbs Sampling) language and, unlike Stan, permit discrete parameters. Discrete parameters are common in conditional likelihood parameterisations of many ecological models, such as the alive state in mark-recapture, occupancy in occupancy models, and abundance in N-mixture models. All of these models can be expressed in the marginal likelihood form where the likelihood is expressed without the discrete parameters, effectively integrating over the possible values of each parameter.\n\n\n\n\n\n\nMarginal and conditional likelihoods\n\n\n\nWhen we speak about conditional likelihoods in the context of ecological models, we express the likelihood of the data \\(y\\) as a function of parameters \\(\\theta\\) and (partially) hidden discrete states \\(Z\\), denoted \\(\\mathcal{L}_c(y \\mid \\theta, Z)\\). The marginal likelihood expresses the same likelihood without the discrete states, thus marginalising them from the model, \\(\\mathcal{L}_m(y \\mid \\theta)\\)\n\n\nMarginalisation leads to faster estimation and, ironically, better exploration of the posterior distributions of the underlying discrete states compared to sampling them directly. The downside is that these models can be more difficult to code or understand, because the marginal likelihood parameterisations don’t reflect the data-generating process as closely as the conditional parameterisations. Ironically, all of these ecological models were first constructed in the marginalised form when frequentist methods dominated the field. Although the conditional parameterisations facilitated the on-boarding of many ecologists that were less stats-savvy (like myself), generally the marginalised forms should be preferred. And when using gradient-based MCMC methods such as the No U-Turn Sampler (NUTS) implemented in Stan, you don’t have a choice.\nWhen I became interested in statistics during my PhD I was using JAGS and subsequently NIMBLE. I always knew I wanted to learn Stan but I found the marginalisation of discrete parameters daunting. Once I decided to bite the bullet, I quickly realised it wasn’t so hard, and it culminated in my first blog post about implementing hidden Markov models in Stan. In this post, I’ll focus on two types of popular models that are generally not constructed as hidden Markov models: occupancy and N-mixture models. I’ll show how to quickly marginalise the latent states, how to recover posterior distributions for them after estimation, and explore different parameterisations and model types."
  },
  {
    "objectID": "blog/marginalisation.html#general-approach",
    "href": "blog/marginalisation.html#general-approach",
    "title": "Marginalising discrete variables in ecological models",
    "section": "General approach",
    "text": "General approach\nThe first approach I outline follows Equation 4 and computes the site-level log-likelihood in a for-loop. A few things to note about this and some upcoming programs:\n\nI’m using mostly flat or weakly informative priors.\nWhen marginalising out discrete parameters, since we’re summing over the mutually exclusive probabilities, we have to keep track of our normalising constants. Therefore, we need to use the _lpmf instead of the _lupmf distributions. This doesn’t make a difference for the Bernoulli distribution because it doesn’t have constant terms, but it does for other examples.\nAlthough I’ve just simulated data with fixed occupancy and detection probabilities, these models facilitate site-level effects on occupancy and site-by-occasion effects on detection. Therefore, in the model block, I created a vector of occupancy probabilities and a matrix of detection probabilities (using the “intercepts” psi_a and p_a) to guide practitioners using this as a template for their own models.\nIn the model block, since we specify a log probability density in Stan, I pre-compute \\(\\log(\\psi)\\) and \\(\\log(1 - \\psi)\\), as this vectorised operation is more efficient than doing it for each site in the for-loop.\nIn the same vein, summing probabilities on the second line of Equation 3 equates to using the log_sum_exp() function in Stan when working with log probabilities.\nIn the generated quantities block, I recover the latent occupancy states that most likely generated the data. I initialise an array of ones (all sites occupied), but then for sites without detections I sort the log probabilities associated with each occupancy state and extract the most likely one, subtracting 1 from the index to yield 0s and 1s.\n\n\n\nCode\n# load packages\nif (!require(pacman)) install.packages(\"pacman\")\npacman::p_load(cmdstanr, loo, tidyverse, tidybayes, posterior)\nchains &lt;- 8 ; iter_warmup &lt;- 200 ; iter_sampling &lt;- 500\noptions(mc.cores = chains)\n\n\n\n\nCode\ndata {\n  int&lt;lower=1&gt; I, J_max;  // number of sites and maximum surveys\n  array[I] int&lt;lower=1, upper=J_max&gt; J; // number of surveys\n  array[I, J_max] int&lt;lower=0, upper=1&gt; y;  // detection history\n}\n\ntransformed data {\n  array[I] int Q;  // number of detections\n  for (i in 1:I) {\n    Q[i] = sum(y[i, 1:J[i]]);\n  }\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; psi_a, p_a;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(psi_a | 1, 1);\n  target += beta_lupdf(p_a | 1, 1);\n  \n  // site-level occupancy and site-by-occasion level detection\n  vector[I] psi = rep_vector(psi_a, I);\n  matrix[I, J_max] p = rep_matrix(p_a, I, J_max);\n  \n  // log-transform occupancy probabilities\n  vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n  \n  // likelihood\n  for (i in 1:I) {\n    if (Q[i]) {\n      target += log_psi[i] + bernoulli_lpmf(y[i, 1:J[i]] | p[i, 1:J[i]]);\n    } else {\n      target += log_sum_exp(log1m_psi[i], \n                            log_psi[i] + bernoulli_lpmf(y[i, 1:J[i]] | p[i, 1:J[i]]));\n    }\n  }\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I] int z = ones_int_array(I);\n  {\n    vector[I] psi = rep_vector(psi_a, I);\n    matrix[I, J_max] p = rep_matrix(p_a, I, J_max);\n    vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n    row_vector[2] lp;\n    for (i in 1:I) {\n      if (Q[i]) {\n        log_lik[i] = log_psi[i] + bernoulli_lpmf(y[i, 1:J[i]] | p[i, 1:J[i]]);\n      } else {\n        lp = [ log1m_psi[i], \n               log_psi[i] + bernoulli_lpmf(y[i, 1:J[i]] | p[i, 1:J[i]]) ];\n        log_lik[i] = log_sum_exp(lp);\n        z[i] = sort_indices_desc(lp)[1] - 1;\n      }\n    }\n  }\n}\n\n\nLet’s run the model using cmdstanr (Gabry et al. 2024). This samples well and fast, so we’ll run many iterations to compare speed.\n\n\nCode\nfit_occ &lt;- occ$sample(\n  data = list(I = I, J_max = J_max, J = J, y = y), \n  refresh = 0, chains = chains, \n  iter_warmup = 1e3, iter_sampling = 1e4\n)\n\n\nRunning MCMC with 8 parallel chains...\n\nChain 2 finished in 2.1 seconds.\nChain 8 finished in 2.1 seconds.\nChain 4 finished in 2.2 seconds.\nChain 5 finished in 2.2 seconds.\nChain 6 finished in 2.2 seconds.\nChain 7 finished in 2.2 seconds.\nChain 1 finished in 2.3 seconds.\nChain 3 finished in 2.3 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 2.2 seconds.\nTotal execution time: 2.4 seconds.\n\n\nCode\nfit_occ$summary(c(\"psi_a\", \"p_a\")) |&gt; \n  select(variable, median, contains(\"q\"), rhat, contains(\"ess\")) |&gt; \n  mutate(truth = c(psi, p))\n\n\n# A tibble: 2 × 8\n  variable median    q5   q95  rhat ess_bulk ess_tail truth\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 psi_a     0.431 0.338 0.537  1.00   40557.   40851.   0.6\n2 p_a       0.490 0.392 0.584  1.00   43643.   46154.   0.4\n\n\nCode\nfit_occ$loo()\n\n\n\nComputed from 80000 by 100 log-likelihood matrix.\n\n         Estimate   SE\nelpd_loo   -140.0 12.2\np_loo         2.1  0.3\nlooic       279.9 24.3\n------\nMCSE of elpd_loo is 0.0.\nMCSE and ESS estimates assume MCMC draws (r_eff in [0.4, 0.9]).\n\nAll Pareto k estimates are good (k &lt; 0.7).\nSee help('pareto-k-diagnostic') for details.\n\n\nWe can also check the proportion of sites that had the latent occupancy estimated correctly. Not too bad.\n\n\nCode\nmean(fit_occ$summary(\"z\")$median == z)\n\n\n[1] 0.85"
  },
  {
    "objectID": "blog/marginalisation.html#multinomial-likelihood",
    "href": "blog/marginalisation.html#multinomial-likelihood",
    "title": "Marginalising discrete variables in ecological models",
    "section": "Multinomial likelihood",
    "text": "Multinomial likelihood\nThere’s quicker ways to fit the same model, specifically with this intercepts-only model with the same number of surveys per site. This uses the multinomial likelihood by specifying cell probabilities for the possible detection histories, for which I’ll show the Stan program below. This approach leverages the definition that the number of surveys with detections follows a \\(\\textrm{Binomial}(J, p)\\) distribution and that sites have a finite number of possible detection histories.4 We can use the multinomial_logit_lupmf function to increment the log density after normalising the cell probabilities on the log scale.5\n\n\nCode\ndata {\n  int&lt;lower=1&gt; I, J;  // number of sites and surveys\n  array[I, J] int&lt;lower=0, upper=1&gt; y;  // detection history\n}\n\ntransformed data {\n  array[I] int Q;  // number of detections\n  for (i in 1:I) {\n    Q[i] = sum(y[i]);\n  }\n  int K = J + 1;  // number of possible detection histories\n  array[K] int P = zeros_int_array(K);  // category counts\n  for (i in 1:I) {\n    P[Q[i] + 1] += 1;\n  }\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; psi_a, p_a;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(psi_a | 1, 1);\n  target += beta_lupdf(p_a | 1, 1);\n  \n  // log probabilities with detections\n  vector[K] lp = rep_vector(log(psi_a), K);\n  for (k in 1:K) {\n    lp[k] += binomial_lpmf(k - 1 | J, p_a);\n  }\n  \n  // log probability without detections\n  lp[1] = log_sum_exp(log1m(psi_a), lp[1]);\n  \n  // increment target\n  target += multinomial_logit_lupmf(P | lp - log_sum_exp(lp));\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I] int z = ones_int_array(I);\n  {\n    real log_psi = log(psi_a), log1m_psi = log1m(psi_a);\n    row_vector[2] lp;\n    for (i in 1:I) {\n      if (Q[i]) {\n        log_lik[i] = log_psi + bernoulli_lpmf(y[i] | p_a);\n      } else {\n        lp = [ log1m_psi, log_psi + bernoulli_lpmf(y[i] | p_a) ];\n        log_lik[i] = log_sum_exp(lp);\n        z[i] = sort_indices_desc(lp)[1] - 1;\n      }\n    }\n  }\n}\n\n\nThis runs even faster and of course recovers the same posterior. We still need the same computations as the first model in the generated quantities block if we want the site-level log-likelihood values and latent states.\n\n\nCode\nfit_occ_multi &lt;- occ_multi$sample(\n  data = list(I = I, J = J_max, y = y), \n  refresh = 0, chains = chains, \n  iter_warmup = 1e3, iter_sampling = 1e4\n)\n\n\nRunning MCMC with 8 parallel chains...\n\n\nChain 1 finished in 0.6 seconds.\nChain 2 finished in 0.6 seconds.\nChain 3 finished in 0.6 seconds.\nChain 4 finished in 0.6 seconds.\nChain 5 finished in 0.6 seconds.\nChain 6 finished in 0.6 seconds.\nChain 7 finished in 0.6 seconds.\nChain 8 finished in 0.6 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 0.6 seconds.\nTotal execution time: 0.8 seconds.\n\n\nCode\nloo_compare(fit_occ$loo(), fit_occ_multi$loo())\n\n\n       elpd_diff se_diff\nmodel1 0.0       0.0    \nmodel2 0.0       0.0    \n\n\nThe approaches outlined so far works well for single season occupancy. For any flavour of multi-season (dynamic) occupancy models, you’re much better off implementing the scaleable forward algorithm I describe in detail in my first post."
  },
  {
    "objectID": "blog/marginalisation.html#combining-occupancy-and-abundance",
    "href": "blog/marginalisation.html#combining-occupancy-and-abundance",
    "title": "Marginalising discrete variables in ecological models",
    "section": "Combining occupancy and abundance",
    "text": "Combining occupancy and abundance\nWhat if we’re counting individuals at sites but not all sites are occupied? This can be described by the following generative model, which is essentially an N-mixture model nested in an occupancy model or a zero-inflated N-mixture model:\n\\[\n\\begin{aligned}\n  z_i &\\sim \\textrm{Bernoulli} (\\psi) \\\\\n  N_i &\\sim \\textrm{Poisson}(z_i \\cdot \\lambda) \\\\\n  y_{ij} &\\sim \\textrm{Binomial}(N_i, p)\n\\end{aligned}\n\\tag{10}\\]\nThe conditional likelihood for the site-level vector of counts \\(\\boldsymbol{y}_i\\) is:\n\\[\n\\mathcal{L}_c(\\boldsymbol{y}_i \\mid \\psi, \\lambda, p, z_i, N_i) = \\textrm{Bernoulli}(z_i \\mid \\psi) \\cdot \\textrm{Poisson}(N_i \\mid z_i \\cdot \\lambda) \\cdot \\prod_{j=1}^{J_i} \\textrm{Binomial}(y_{ij} \\mid N_i, p)\n\\tag{11}\\]\nThe full marginal likelihood is unwieldy to write, but if we define the expression in Equation 9 as \\(\\theta\\), then the marginal likelihood can be written analogously to our starting occupancy model in Equation 4 as follows:\n\\[\n\\mathcal{L}_m(\\boldsymbol{y}_i \\mid \\psi, \\lambda, p) = \\begin{cases}\n\\psi \\cdot \\theta, &Q_i &gt; 0, \\\\\n1 - \\psi + \\psi \\cdot \\theta, &Q_i = 0,\n\\end{cases}\n\\tag{12}\\]\nwhere \\(Q_i\\) is the total number of individuals detected across surveys. When marginalising discrete parameters in several hierarchical levels, you should first compute the lower levels (abundance in this case) and then increment the higher levels (occupancy). Let’s simulate some data to start.\n\n\nCode\n# parameters\npsi &lt;- 0.8\n\n# simulate\nz &lt;- rbinom(I, 1, psi)\nN &lt;- rpois(I, z * lambda)\ny &lt;- matrix(0, I, J_max)\nfor (i in 1:I) {\n  y[i, 1:J[i]] &lt;- rbinom(J[i], N[i], p)\n}\n\n\nNotice that in the generated quantities block, we first simulate posterior draws for \\(N_i\\) for all sites, but then overwrite this value with 0 if the site is estimated to have not been occupied (\\(z_i = 0\\)). This is akin to the forward-backward (Viterbi) algorithm for recovering hidden states in hidden Markov models.\n\n\nCode\ndata {\n  int&lt;lower=0&gt; I, J_max, K;  // number of sites and maximum surveys and population size\n  array[I] int&lt;lower=1, upper=J_max&gt; J;  // number of surveys\n  array[I, J_max] int&lt;lower=0&gt; y;  // detection history\n}\n\ntransformed data {\n  array[I] int max_y, ks;  // maximum counts and number of population sizes\n  for (i in 1:I) {\n    max_y[i] = max(y[i, 1:J[i]]);\n    ks[i] = K - max_y[i] + 1;\n  }\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; psi_a, p_a;\n  real&lt;lower=0&gt; lambda_a;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(psi_a | 1, 1);\n  target += exponential_lupdf(lambda_a | 0.01);\n  target += beta_lupdf(p_a | 1, 1);\n  \n  // site-level expected counts and site-by-occasion level detection\n  vector[I] psi = rep_vector(psi_a, I);\n  vector[I] lambda = rep_vector(lambda_a, I);\n  matrix[I, J_max] p = rep_matrix(p_a, I, J_max);\n  \n  // log-transform occupancy probabilities\n  vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n  \n  // likelihood\n  real theta;\n  for (i in 1:I) {\n    \n    // abundance\n    vector[ks[i]] lp;\n    for (k in max_y[i]:K) {\n      lp[k - max_y[i] + 1] = poisson_lpmf(k | lambda[i]) \n                             + binomial_lpmf(y[i, 1:J[i]] | k, p[i, 1:J[i]]);\n    }\n    theta = log_sum_exp(lp);\n    \n    // increment occupancy\n    if (max_y[i]) {\n      target += log_psi[i] + theta;\n    } else {\n      target += log_sum_exp(log1m_psi[i], log_psi[i] + theta);\n    }\n  }\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I] int z = ones_int_array(I), N;\n  {\n    vector[I] psi = rep_vector(psi_a, I);\n    vector[I] lambda = rep_vector(lambda_a, I);\n    matrix[I, J_max] p = rep_matrix(p_a, I, J_max);\n    vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n    real theta;\n    row_vector[2] lp_z;\n    for (i in 1:I) {\n      vector[ks[i]] lp;\n      for (k in max_y[i]:K) {\n        lp[k - max_y[i] + 1] = poisson_lpmf(k | lambda[i]) \n                               + binomial_lpmf(y[i, 1:J[i]] | k, p[i, 1:J[i]]);\n      }\n      theta = log_sum_exp(lp);\n      N[i] = categorical_logit_rng(lp - theta) + max_y[i] - 1;\n      if (max_y[i]) {\n        log_lik[i] = log_psi[i] + theta;\n      } else {\n        lp_z = [ log1m_psi[i], log_psi[i] + theta ];\n        log_lik[i] = log_sum_exp(lp_z);\n        z[i] = sort_indices_desc(lp_z)[1] - 1;\n        if (!z[i]) {\n          N[i] = 0;\n        }\n      }\n    }\n  }\n}\n\n\nAfter running this model we see estimation went well and latent occupancy states had great recovery. There’s a lot to be said for using counts in the observation models of occupancy models as it provides much more information about \\(\\psi\\) than a Binomial likelihood, because collapsing counts to 1s and 0s always discards information. But this might be the subject of a future blog post.\n\n\nCode\nzi_nmix_data &lt;- list(I = I, J_max = J_max, K = 3 * max(N), J = J, y = y)\nfit_zi_nmix &lt;- zi_nmix$sample(\n  data = zi_nmix_data, \n  refresh = 0, chains = chains, \n  iter_warmup = iter_warmup, iter_sampling = iter_sampling\n)\n\n\nRunning MCMC with 8 parallel chains...\n\nChain 8 finished in 67.2 seconds.\nChain 5 finished in 81.4 seconds.\nChain 6 finished in 83.3 seconds.\nChain 1 finished in 86.4 seconds.\nChain 3 finished in 88.2 seconds.\nChain 2 finished in 91.0 seconds.\nChain 7 finished in 91.0 seconds.\nChain 4 finished in 92.2 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 85.1 seconds.\nTotal execution time: 92.4 seconds.\n\n\nCode\nfit_zi_nmix$summary(c(\"psi_a\", \"lambda_a\", \"p_a\")) |&gt; \n  select(variable, median, contains(\"q\"), rhat, contains(\"ess\")) |&gt; \n  mutate(truth = c(psi, lambda, p))\n\n\n# A tibble: 3 × 8\n  variable median      q5    q95  rhat ess_bulk ess_tail truth\n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 psi_a     0.767  0.697   0.830  1.00    1597.    1475.   0.8\n2 lambda_a 20.3   13.5    41.2    1.01     852.     854.  20  \n3 p_a       0.199  0.0979  0.302  1.01     853.     849.   0.2\n\n\nCode\n# proportion of sites correctly classified\nmean(fit_zi_nmix$summary(\"z\")$median == z)\n\n\n[1] 1"
  },
  {
    "objectID": "blog/marginalisation.html#time-to-event-n-mixture-model",
    "href": "blog/marginalisation.html#time-to-event-n-mixture-model",
    "title": "Marginalising discrete variables in ecological models",
    "section": "Time-to-event N-mixture model",
    "text": "Time-to-event N-mixture model\nThe final model I’ll consider is a type of continuous time model.7 Time-to-event N-mixture models can be used to estimate abundance under the assumption that detection rates are proportional to abundance (Strebel et al. 2021). Consider sites \\(i \\in 1:I\\) surveyed \\(j \\in 1:J_i\\) times for a minimum of length of \\(\\Delta_{ij}\\) (e.g., 30 minutes).8 Surveys are conducted up to \\(\\Delta_{ij}\\) or are terminated after the time of first detection, yielding right-censored data \\(y_{ij} = \\Delta_{ij}\\) if no detection were made or \\(y_{ij} &lt; \\Delta_{ij}\\) if a detection was made. The waiting times are then modeled with some appropriate distribution such as the exponential. We can just replace the observation model in the zero-inflated N-mixture model in Equation 10 by modeling the waiting times as a function of the product of the per-individual detection rate \\(\\mu\\) and site-level abundance \\(N_i\\), yielding the following data-generating process:\n\\[\n\\begin{aligned}\n  z_i &\\sim \\textrm{Bernoulli} (\\psi) \\\\\n  N_i &\\sim \\textrm{Poisson}(z_i \\cdot \\lambda) \\\\\n  y_{ij} &\\sim \\min(\\textrm{Exponential}(N_i \\cdot \\mu), \\Delta_{ij})\n\\end{aligned}\n\\tag{13}\\]\nJust out of curiosity, I’m going to use the same parameters as for first zero-inflated N-mixture model above, leveraging the fact that \\(\\mu = -\\log(1 - p)\\).9 Since I’m trying to compare these models, I’ll assume that \\(\\Delta = 1\\) for all surveys, but this doesn’t have to be case for these models.\n\n\nCode\n# convert detection probability to rate\nmu &lt;- -log(1 - p)\n\n# simulate\nDelta_max &lt;- 1\ny &lt;- Delta &lt;- matrix(Delta_max, I, J_max)\nfor (i in 1:I) {\n  if (N[i]) {\n    y[i, 1:J[i]] &lt;- pmin(rexp(J[i], N[i] * mu), Delta[i, 1:J[i]])\n  }\n}\n\n\nThe Stan program uses the log density function exponential_lpdf when detections were made (y[i, j] &lt; Delta[i, j]), and the complement of the cumulative density function exponential_lccdf if no detections were made. The latter gives the log probability of not making any detections during the survey of length \\(\\Delta_{ij}\\). Therefore, we can’t vectorise over the surveys because the density function depends on whether or not there was a detection in each survey.\n\n\nCode\ndata {\n  int&lt;lower=0&gt; I, J_max, K, Delta_max;  // number of sites and maximum surveys, population size, and survey length\n  array[I] int&lt;lower=1, upper=J_max&gt; J;  // number of surveys\n  matrix&lt;lower=0&gt;[I, J_max] Delta, y;  // survey lengths and detection history\n}\n\ntransformed data {\n  array[I, J_max] int Q;  // indicator for detection\n  array[I] int Q_sum = zeros_int_array(I);  // number of detections\n  for (i in 1:I) {\n    for (j in 1:J[i]) {\n      Q[i, j] = y[i, j] &lt; Delta[i, j];\n    }\n    Q_sum[i] = sum(Q[i, 1:J[i]]);\n  }\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; psi_a;\n  real&lt;lower=0&gt; lambda_a, mu_a;\n}\n\nmodel {\n  // priors\n  target += beta_lupdf(psi_a | 1, 1);\n  target += exponential_lupdf(lambda_a | 0.01);\n  target += exponential_lupdf(mu_a | 0.1);\n  \n  // site-level expected counts and site-by-occasion level detection\n  vector[I] psi = rep_vector(psi_a, I);\n  vector[I] lambda = rep_vector(lambda_a, I);\n  matrix[I, J_max] mu = rep_matrix(mu_a, I, J_max);\n  \n  // log-transform occupancy probabilities\n  vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n  \n  // likelihood\n  vector[K] lp;\n  real theta;\n  for (i in 1:I) {\n    \n    // abundance\n    for (k in 1:K) {\n      lp[k] = poisson_lpmf(k | lambda[i]);\n      for (j in 1:J[i]) {\n        if (Q[i, j]) {\n          lp[k] += exponential_lpdf(y[i, j] | k * mu[i, j]);\n        } else {\n          lp[k] += exponential_lccdf(Delta[i, j] | k * mu[i, j]);\n        }\n      }\n    }\n    theta = log_sum_exp(lp);\n    \n    // increment occupancy\n    if (Q_sum[i]) {\n      target += log_psi[i] + theta;\n    } else {\n      target += log_sum_exp(log1m_psi[i], log_psi[i] + theta);\n    }\n  }\n}\n\ngenerated quantities {\n  vector[I] log_lik;\n  array[I] int z = ones_int_array(I), N;\n  {\n    vector[I] psi = rep_vector(psi_a, I);\n    vector[I] lambda = rep_vector(lambda_a, I);\n    matrix[I, J_max] mu = rep_matrix(mu_a, I, J_max);\n    vector[I] log_psi = log(psi), log1m_psi = log1m(psi);\n    real theta;\n    vector[K] lp;\n    row_vector[2] lp_z;\n    for (i in 1:I) {\n      for (k in 1:K) {\n        lp[k] = poisson_lpmf(k | lambda[i]);\n        for (j in 1:J[i]) {\n          if (Q[i, j]) {\n            lp[k] += exponential_lpdf(y[i, j] | k * mu[i, j]);\n          } else {\n            lp[k] += exponential_lccdf(Delta[i, j] | k * mu[i, j]);\n          }\n        }\n      }\n      theta = log_sum_exp(lp);\n      N[i] = categorical_logit_rng(lp - theta);\n      if (Q_sum[i]) {\n        log_lik[i] = log_psi[i] + theta;\n      } else {\n        lp_z = [ log1m_psi[i], log_psi[i] + theta ];\n        log_lik[i] = log_sum_exp(lp_z);\n        z[i] = sort_indices_desc(lp_z)[1] - 1;\n        if (!z[i]) {\n          N[i] = 0;\n        }\n      }\n    }\n  }\n}\n\n\nLet’s fit the time-to-event model, check the posteriors, and plot a subset of the abundance estimates alongside those of the first zero-inflated N-mixture model.\n\n\nCode\nfit_tte &lt;- tte$sample(\n  data = list(I = I, J_max = J_max, K = 3 * max(N), Delta_max = Delta_max, J = J, Delta = Delta, y = y), \n  refresh = 0, chains = chains, \n  iter_warmup = iter_warmup, iter_sampling = iter_sampling\n)\n\n\nRunning MCMC with 8 parallel chains...\n\n\nChain 5 finished in 30.1 seconds.\nChain 8 finished in 33.0 seconds.\nChain 1 finished in 33.7 seconds.\nChain 6 finished in 33.6 seconds.\nChain 3 finished in 35.0 seconds.\nChain 7 finished in 37.4 seconds.\nChain 4 finished in 39.5 seconds.\nChain 2 finished in 42.0 seconds.\n\nAll 8 chains finished successfully.\nMean chain execution time: 35.5 seconds.\nTotal execution time: 42.1 seconds.\n\n\nCode\nfit_tte$summary(c(\"psi_a\", \"lambda_a\", \"mu_a\")) |&gt; \n  select(variable, median, contains(\"q\"), rhat, contains(\"ess\")) |&gt; \n  mutate(truth = c(psi, lambda, mu))\n\n\n# A tibble: 3 × 8\n  variable median     q5    q95  rhat ess_bulk ess_tail  truth\n  &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 psi_a     0.767 0.692   0.829  1.00    1538.    1460.  0.8  \n2 lambda_a 12.6   5.85   46.2    1.01     740.    1055. 20    \n3 mu_a      0.381 0.0971  0.887  1.01     749.     993.  0.223\n\n\n\n\nCode\nout &lt;- map(list(fit_zi_nmix, fit_tte), \n    ~(spread_rvars(.x, z[i], N[i]))) |&gt; \n  list_rbind(names_to = \"model\") |&gt; \n  mutate(model = factor(model, labels = c(\"Counts\", \"Time-to-event\"))) |&gt; \n  left_join(tibble(i = 1:I, truth = N), by = \"i\")\n\nout |&gt; \n  filter(i &lt;= 25) |&gt; \n  ggplot(aes(xdist = N, y = factor(i) |&gt; fct_rev())) + \n  facet_wrap(~ model) + \n  stat_pointinterval(point_interval = median_qi, .width = 0.9, size = 0.5, linewidth = 0.5) + \n  geom_point(aes(x = truth), \n             position = position_nudge(y = -0.3), \n             size = 2, \n             colour = \"red4\", \n             shape = \"cross\") + \n  labs(x = expression(N), \n       y = \"Occupied Site\", \n       colour = \"Model\")\n\n\n\n\n\nThe estimates for the count model look better and seemed to perform better according to RMSE of the population sizes, which makes sense because the count model can provide more information (number of individuals detected) compared to the time-to-event model (waiting time to the first individual detected). Since the \\(\\textrm{Binomial}(N_i, p)\\) distribution converges to \\(\\textrm{Poisson}(N_i \\cdot p)\\) for sufficiently large \\(N\\) and sufficiently small \\(p\\)10, and because \\(p \\approx \\mu\\) for small values of \\(p\\), researchers gain statistical power by not terminating the search after first detection but instead counting the number of individuals during the interval \\(\\Delta_{ij}\\). But of course, one of the big draws of the time-to-event model is less time in the field; you win some, you lose some.\n\n\nCode\nout |&gt; \n  summarise(RMSE = sqrt(rvar_mean((truth - N)^2)), \n            .by = model)\n\n\n# A tibble: 2 × 2\n  model                RMSE\n  &lt;fct&gt;          &lt;rvar[1d]&gt;\n1 Counts          7.5 ± 6.5\n2 Time-to-event  10.8 ± 6.4\n\n\nAt least our occupancy states are still estimated well.\n\n\nCode\nmean(fit_tte$summary(\"z\")$median == z)\n\n\n[1] 1\n\n\nThanks for reading!"
  },
  {
    "objectID": "blog/marginalisation.html#footnotes",
    "href": "blog/marginalisation.html#footnotes",
    "title": "Marginalising discrete variables in ecological models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe number of surveys can vary by site, which is the norm rather than the exception in ecological data.↩︎\nWe do know that sites with detections are occupied.↩︎\nBoth \\(\\psi\\) and \\(p\\) can be modeled as functions of covariates or random effects at the site-level (occupancy) or the site-by-survey level (detection).↩︎\nWe could still formulate this with varying number of surveys, except that now the cell probabilities have to be expanded to not only account for the \\(J + 1\\) possible outcomes but also the variation in \\(J_i\\) itself.↩︎\nNormalising probabilities means to divide by the sum of total probabilities, which is achieved by subtracting the log_sum_exp of the log probabilities.↩︎\nThis is referred to as inverse transform sampling.↩︎\nThe traditional N-mixture model can become a continuous time model by incorporating the survey lengths into the observation model.↩︎\nThe survey times can vary by site and occasion.↩︎\nProbabilities can be converted to rates like this more generally, such as survival probabilities as a function of instantaneous mortality hazard rates \\(S = 1 - \\exp(-h)\\) (Ergon et al. 2018).↩︎\nSee Wikipedia for some rules of thumb.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol offers a range of methodological and analytical services for efficient, effective, and honest scientific inference.\n\n\nWe can save you time and money starting from the inception of a research question. Data forms the foundation of scientific studies, and statistical models make critical assumptions about how the data were collected which influence the validity of your conclusions. We can provide you with study designs tailored to your system, with recommendations for appropriate sample sizes, surveying schemes, and more.\n\n\n\nYour datasets often contain a wealth of information but complex models are frequently required to reveal these signals. We can help you with this process, ranging from tutorials and workshops for applying statistical methodology, to conducting the entire analysis for you in a clear, collaborative fashion.\n\nmodel {\n  // likelihood of diagnostic infection intensities\n  for (j in 1:n_x) {\n    target += lognormal_lupdf(x[j] | log_n[ind[j]][sec[j], prim[j]], sigma[3]);\n  }\n  \n  // likelihood of multievent model (threading)\n  target += reduce_sum(partial_sum_lupmf, seq_ind, grainsize, y, q, \n                       n_prim, n_sec, first, last, first_sec, tau, temp, \n                       phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, m, n_z, sigma);\n}\n\n\n\n\nCommunication of key results derived from our data is what advances scientific fields. We believe that honest reporting involves an appropriate display of uncertainty and does not hide the raw data. The services we offer are predicated on transparency and scientific integrity."
  },
  {
    "objectID": "index.html#what-we-do",
    "href": "index.html#what-we-do",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol offers a range of methodological and analytical services for efficient, effective, and honest scientific inference.\n\n\nWe can save you time and money starting from the inception of a research question. Data forms the foundation of scientific studies, and statistical models make critical assumptions about how the data were collected which influence the validity of your conclusions. We can provide you with study designs tailored to your system, with recommendations for appropriate sample sizes, surveying schemes, and more.\n\n\n\nYour datasets often contain a wealth of information but complex models are frequently required to reveal these signals. We can help you with this process, ranging from tutorials and workshops for applying statistical methodology, to conducting the entire analysis for you in a clear, collaborative fashion.\n\nmodel {\n  // likelihood of diagnostic infection intensities\n  for (j in 1:n_x) {\n    target += lognormal_lupdf(x[j] | log_n[ind[j]][sec[j], prim[j]], sigma[3]);\n  }\n  \n  // likelihood of multievent model (threading)\n  target += reduce_sum(partial_sum_lupmf, seq_ind, grainsize, y, q, \n                       n_prim, n_sec, first, last, first_sec, tau, temp, \n                       phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, m, n_z, sigma);\n}\n\n\n\n\nCommunication of key results derived from our data is what advances scientific fields. We believe that honest reporting involves an appropriate display of uncertainty and does not hide the raw data. The services we offer are predicated on transparency and scientific integrity."
  },
  {
    "objectID": "about/vision.html",
    "href": "about/vision.html",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol is a statistical consulting firm started in 2022 with the vision of providing clients with robust study designs and rigorous data analysis. We understand that every dataset is unique, often requiring custom model configurations to disentangle the signals form the noise. Our view is that ethical scientific behaviour involves honest reporting of uncertainty to buffer against spurious conclusions, while simultaneously extracting as much as possible from our data. Our aim is to work collaboratively with a range of researchers to communicate research output with a high degree of confidence."
  },
  {
    "objectID": "about/vision.html#our-vision",
    "href": "about/vision.html#our-vision",
    "title": "Quantecol",
    "section": "",
    "text": "Quantecol is a statistical consulting firm started in 2022 with the vision of providing clients with robust study designs and rigorous data analysis. We understand that every dataset is unique, often requiring custom model configurations to disentangle the signals form the noise. Our view is that ethical scientific behaviour involves honest reporting of uncertainty to buffer against spurious conclusions, while simultaneously extracting as much as possible from our data. Our aim is to work collaboratively with a range of researchers to communicate research output with a high degree of confidence."
  },
  {
    "objectID": "services.html",
    "href": "services.html",
    "title": "Quantecol",
    "section": "",
    "text": "We provide custom services tailored to the requirements of the client. This ranges from brief consultations about methodology or analysis, to conducting full data analyses and scientific reporting. We offer:\n\nMethodological guidance\nConsultations about study design\nValidation of statistical analysis\nTutorials and workshops for specific statistical methods\nCollaboration on research projects\nData and output visualisation\n\nWe prefer to take a Bayesian approach because of the flexibility in building custom models, straightforward quantification of uncertainty, and ability to incorporate domain expertise when available. Our capabilities include, but are not limited to, the following statistical models:\n\n(Generalised) linear models\nPower analysis and simulations\nMultivariate analysis\nDose-response models\nOrdinal regression\nHidden Markov models\nItem-response models\nGaussian processes\nEcological models\n\nMark-recapture and multievent\n(Dynamic) occupancy, including co-occurrence\n(Dynamic) N- and multinomial-mixture\nSpatial mark-recapture\nDistance sampling\nSpecies distribution models\nContinuous time models\n\n\nIf you are interested in working with Quantecol, please get in touch."
  },
  {
    "objectID": "services.html#services",
    "href": "services.html#services",
    "title": "Quantecol",
    "section": "",
    "text": "We provide custom services tailored to the requirements of the client. This ranges from brief consultations about methodology or analysis, to conducting full data analyses and scientific reporting. We offer:\n\nMethodological guidance\nConsultations about study design\nValidation of statistical analysis\nTutorials and workshops for specific statistical methods\nCollaboration on research projects\nData and output visualisation\n\nWe prefer to take a Bayesian approach because of the flexibility in building custom models, straightforward quantification of uncertainty, and ability to incorporate domain expertise when available. Our capabilities include, but are not limited to, the following statistical models:\n\n(Generalised) linear models\nPower analysis and simulations\nMultivariate analysis\nDose-response models\nOrdinal regression\nHidden Markov models\nItem-response models\nGaussian processes\nEcological models\n\nMark-recapture and multievent\n(Dynamic) occupancy, including co-occurrence\n(Dynamic) N- and multinomial-mixture\nSpatial mark-recapture\nDistance sampling\nSpecies distribution models\nContinuous time models\n\n\nIf you are interested in working with Quantecol, please get in touch."
  },
  {
    "objectID": "about/contact.html",
    "href": "about/contact.html",
    "title": "Quantecol",
    "section": "",
    "text": "For inquiries about support with your projects, please get in touch via e-mail or call us at 0456 659 313."
  },
  {
    "objectID": "about/contact.html#contact",
    "href": "about/contact.html#contact",
    "title": "Quantecol",
    "section": "",
    "text": "For inquiries about support with your projects, please get in touch via e-mail or call us at 0456 659 313."
  },
  {
    "objectID": "about/people/matthijs.html",
    "href": "about/people/matthijs.html",
    "title": "Dr. Matthijs Hollanders",
    "section": "",
    "text": "Matthijs received Bachelor and Master’s degrees in Biology from Wageningen University & Research, The Netherlands, and was awarded his doctorate from Southern Cross University, Australia, in 2023. A passion for quantitative methods was sparked during his PhD at SCU where he investigated contemporary amphibian responses to the disease chytridiomycosis. Intrigued by the often noisy ecological datasets, he strives to account for uncertainty in datasets to make robust, honest scientific inference."
  },
  {
    "objectID": "blog/gaussian-processes.html",
    "href": "blog/gaussian-processes.html",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "",
    "text": "It is common to use random effects to capture heterogeneity in data that have been collected at spatial and/or temporal intervals. For example, counts of species or individuals are frequently replicated in space and time and researchers may add site or survey date as random effects in the model. These random effects are typically modeled as being normally distributed. Consider surveys \\(i \\in 1:I\\) where counts \\(y_i\\) are made which we model with Poisson regression. The model with normally distributed random survey effects on the log expected count looks like this:\n\\[\n\\begin{aligned}\n  y_i &\\sim \\textrm{Poisson} \\left( \\exp(\\alpha + \\epsilon_i) \\right) \\\\\n  \\epsilon_i &\\sim \\textrm{Normal} \\left( 0, \\tau \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere the intercept \\(\\alpha\\), the random survey-level offsets \\(\\epsilon_i\\), and the standard deviation of the survey effects \\(\\tau\\) are to be estimated by the model. In this post, I argue that when the random effects are not expected to be independent it almost always makes more sense to model the random effects using Gaussian processes (GPs).\nGPs are a bit complicated (Simpson 2021) but one of their salient features is that any set of observations generated by a Gaussian process are marginally distributed as multivariate normal \\(\\textrm{Normal} (\\boldsymbol{\\mu}, \\Sigma)\\).1 In practice, the realisations of the GP are added to our linear predictor so \\(\\boldsymbol{\\mu} = \\boldsymbol{0}\\). There are quite a few covariance kernels \\(\\Sigma\\) to choose from but one of the most common is the exponentiated quadratic kernel given by \\[\n\\Sigma_{ij} = \\tau^2 \\exp \\left( -\\frac{\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert^2}{2 \\rho ^2} \\right),\n\\tag{2}\\]\nwhere \\(\\tau^2\\) is the marginal variance (analogous to variance \\(\\tau^2\\) with normally distributed random effects), \\(\\rho\\) is the length-scale governing the covariance between observations, and the term \\(\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert\\) is the Euclidean distance between any two points of our input.2 The length-scale \\(\\rho\\) governs the correlation between points as a function of the distance between them. When \\(\\rho\\) is small, there is little correlation between neighbouring locations and when \\(\\rho\\) is large, there is high correlation between observations. Essentially, GPs allow modeling normally distributed random effects where observations close to each other (as in, surveys conducted at the same time of year) may be correlated."
  },
  {
    "objectID": "blog/gaussian-processes.html#introduction",
    "href": "blog/gaussian-processes.html#introduction",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "",
    "text": "It is common to use random effects to capture heterogeneity in data that have been collected at spatial and/or temporal intervals. For example, counts of species or individuals are frequently replicated in space and time and researchers may add site or survey date as random effects in the model. These random effects are typically modeled as being normally distributed. Consider surveys \\(i \\in 1:I\\) where counts \\(y_i\\) are made which we model with Poisson regression. The model with normally distributed random survey effects on the log expected count looks like this:\n\\[\n\\begin{aligned}\n  y_i &\\sim \\textrm{Poisson} \\left( \\exp(\\alpha + \\epsilon_i) \\right) \\\\\n  \\epsilon_i &\\sim \\textrm{Normal} \\left( 0, \\tau \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere the intercept \\(\\alpha\\), the random survey-level offsets \\(\\epsilon_i\\), and the standard deviation of the survey effects \\(\\tau\\) are to be estimated by the model. In this post, I argue that when the random effects are not expected to be independent it almost always makes more sense to model the random effects using Gaussian processes (GPs).\nGPs are a bit complicated (Simpson 2021) but one of their salient features is that any set of observations generated by a Gaussian process are marginally distributed as multivariate normal \\(\\textrm{Normal} (\\boldsymbol{\\mu}, \\Sigma)\\).1 In practice, the realisations of the GP are added to our linear predictor so \\(\\boldsymbol{\\mu} = \\boldsymbol{0}\\). There are quite a few covariance kernels \\(\\Sigma\\) to choose from but one of the most common is the exponentiated quadratic kernel given by \\[\n\\Sigma_{ij} = \\tau^2 \\exp \\left( -\\frac{\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert^2}{2 \\rho ^2} \\right),\n\\tag{2}\\]\nwhere \\(\\tau^2\\) is the marginal variance (analogous to variance \\(\\tau^2\\) with normally distributed random effects), \\(\\rho\\) is the length-scale governing the covariance between observations, and the term \\(\\lVert \\boldsymbol{x}_i - \\boldsymbol{x}_j \\rVert\\) is the Euclidean distance between any two points of our input.2 The length-scale \\(\\rho\\) governs the correlation between points as a function of the distance between them. When \\(\\rho\\) is small, there is little correlation between neighbouring locations and when \\(\\rho\\) is large, there is high correlation between observations. Essentially, GPs allow modeling normally distributed random effects where observations close to each other (as in, surveys conducted at the same time of year) may be correlated."
  },
  {
    "objectID": "blog/gaussian-processes.html#example-tadpole-counts",
    "href": "blog/gaussian-processes.html#example-tadpole-counts",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Example: tadpole counts",
    "text": "Example: tadpole counts\nFor this example, I’ll use data we collected on tadpoles of Fleay’s barred frogs (Mixophyes fleayi), a stream-dwelling species endemic to the Gondwana rainforests of northern New South Wales and southeast Queensland, Australia (Hollanders et al. 2024). During my PhD, I swept a dipnet through pools in the creeks for roughly 1 min every six weeks, deposited the tadpoles in a tub, took a photograph from above, and used Photoshop to count and measure the tadpoles.3 Here, I’ll fit the model in Equation 1 with and without GP to model the number of tadpoles I captured during each six-weekly survey at one of the sites. First I’ll download the data from my GitHub and plot the counts for the creek using ggplot2 (Wickham 2016) and other packages from the tidyverse (Wickham et al. 2019).\n\n\nCode\n# download data\nlibrary(tidyverse)\ndat &lt;- read_csv(\"https://raw.githubusercontent.com/mhollanders/mfleayi-tadpoles/main/data/tadpole-lengths.csv\") |&gt; \n  filter(site == \"brindle\") |&gt; \n  mutate(date = dmy(date)) |&gt; \n  count(date)\n\n# plot the counts\nfig &lt;- dat |&gt; \n  ggplot(aes(date, n)) + \n  geom_point(colour = green, shape = 16, size = 3, alpha = 3/4) + \n  scale_y_continuous(breaks = seq(50, 200, 50), limits = c(0, 225), expand = c(0, 0)) + \n  labs(x = \"Survey\", y = \"Number of tadpoles\")\nfig\n\n\n\n\n\nFleay’s barred frogs breed from the early spring to the middle of summer and generally there are several pulses of reproduction resulting in fresh cohorts of tadpoles. We’ll ignore this and just model the counts using random survey effects in the absence of any predictors.\n\n\n\nFleay’s barred frog (Mixophyes fleayi) from Brindle Creek, Border Ranges National Park, Australia."
  },
  {
    "objectID": "blog/gaussian-processes.html#setting-priors-for-gps",
    "href": "blog/gaussian-processes.html#setting-priors-for-gps",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Setting priors for GPs",
    "text": "Setting priors for GPs\nGPs have two parameters that require priors, \\(\\tau\\) and \\(\\rho\\). The prior for \\(\\tau\\) can just be something like an exponential prior commonly used for standard deviations of random effects.4 The prior for the length-scale \\(\\rho\\) is trickier as it is often not well identified by the data. In fact, the data cannot inform length-scales that are outside of the distances observed in the data. This means that we want a prior than can suppress values below and above certain thresholds. One popular distribution for length-scales that’s capable of doing exactly this is the inverse gamma distribution. Michael Betancourt’s GP case study discusses this at length and includes a function that uses Stan’s algebra solver to determine the shape and scale of the inverse gamma distribution that places a certain proportion of the probability mass between two values.5\nIf the study was well designed to estimate the length-scale of the GP, the survey intervals should be shorter than the minimum length-scale we think we might be dealing with. Unfortunately, I did not collect data with this question in mind, and I can easily see that the relevant length-scale is shorter than the minimum length between surveys (I surveyed at six-weekly intervals, give or take). However, for this demonstration, we’ll assume that the survey intervals were rigorously chosen and we’ll use the minimum and maximum observed temporal distances between surveys to choose a suitable inverse gamma prior for the length-scale. So, I compute the minimum and maximum distance and chose tail probabilities of 1% so that 98% of the probability mass of the length-scale falls between the minimum and maximum observed distances between the surveys."
  },
  {
    "objectID": "blog/gaussian-processes.html#fitting-the-model",
    "href": "blog/gaussian-processes.html#fitting-the-model",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Fitting the model",
    "text": "Fitting the model\nBelow I write out a Stan program (Carpenter et al. 2017) that uses an indicator to use either normally distributed random effects or to use a GP with a exponentiated quadratic covariance kernel. Stan includes the handy function gp_exp_quad_cov() that generates this covariance kernel from the input (survey times expressed in weeks, in this case) and standard deviation and length-scale. We also use this GP input to pre-compute the distance matrix to get the minimum and maximum observed (temporal) distances of the surveys to use Stan’s algebra solver to determine a suitable inverse gamma prior for \\(\\rho\\). For both types of random effects I use the non-centered parameterisation which generally has better sampling for few observations (we only have 10 data points). For GPs (and multivariate normals more generally), this entails Cholesky decomposing the covariance matrix (think of it as a matrix square root) and multiplying it by a vector of standard normal variates. Note that before Cholesky decomposing the covariance kernel, I add a diagonal matrix with a very small value (called the “nugget” or “jitter”) to ensure the matrix is positive definite.\n\n\nCode\nfunctions {\n  // function from Michael Betancourt for inverse gamma prior shape and scale\n  vector inv_gamma_prior(vector guess, vector theta, array[] real tails, array[] int x_i) {\n    vector[2] params;\n    params[1] = inv_gamma_cdf(theta[1] | guess[1], guess[2]) - tails[1];\n    params[2] = inv_gamma_cdf(theta[2] | guess[1], guess[2]) - tails[2];\n    return params;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; I;  // number of surveys\n  array[I] real survey;  // survey input as number of weeks\n  vector[2] min_max_dist;  // minimum and maximum observed distance between weeks\n  array[I] int y;  // tadpole counts per survey\n  int&lt;lower=0, upper=1&gt; GP;  // indicator for GP (0 = no, 1 = yes)\n}\n\ntransformed data {\n  vector&lt;lower=0&gt;[2] rho_prior = ones_vector(2);  // length-scale prior shape and scale\n  if (GP) {\n    rho_prior = algebra_solver(inv_gamma_prior, [1, 1]', min_max_dist, {0.01, 0.99}, {0});\n    print(\"rho_prior: \", rho_prior);\n  }\n}\n\nparameters {\n  real alpha;  // intercept\n  real&lt;lower=0&gt; tau, rho;  // random effect SD and GP length-scale\n  vector[I] z;  // standard-normal z-scores for non-centered parameterisation\n}\n\ntransformed parameters {\n  vector[I] epsilon;  // random survey effects\n  if (GP) {\n    epsilon = cholesky_decompose(add_diag(gp_exp_quad_cov(survey, tau, rho), 1e-9)) * z;\n  } else {\n    epsilon = tau * z;\n  }\n}\n\nmodel {\n  // priors\n  alpha ~ normal(0, 5);\n  tau ~ exponential(1);\n  rho ~ inv_gamma(rho_prior[1], rho_prior[2]);\n  z ~ std_normal();\n    \n  // likelihood\n  y ~ poisson_log(alpha + epsilon);\n}\n\ngenerated quantities {\n  vector[I] yrep, log_lik;\n  for (i in 1:I) {\n    yrep[i] = poisson_log_rng(alpha + epsilon[i]);\n    log_lik[i] = poisson_log_lpmf(y[i] | alpha + epsilon[i]);\n  }\n}\n\n\nI’ll fit both the normal and GP model with CmdStanR (Gabry et al. 2024) in R 4.4.1 (R Core Team 2024).\n\n\nCode\n# prepare data for Stan\nstan_data &lt;- list(I = nrow(dat), \n                  survey = as.numeric(dat$date) / 7, \n                  y = dat$n, \n                  GP = 0)\n\n# get distance matrix and add minimum and maximum observed distances\ndist_obs &lt;- dist(stan_data$survey) |&gt; as.matrix()\nstan_data$min_max_dist &lt;- range(dist_obs[dist_obs &gt; 0])\n\n# normal model\nfit_normal &lt;- mod$sample(data = stan_data, \n                         refresh = 0, chains = 1, \n                         iter_warmup = 500, iter_sampling = 4000, \n                         show_exceptions = F)\n\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 0.3 seconds.\n\n\nCode\n# GP model\nstan_data$GP &lt;- 1\nfit_gp &lt;- mod$sample(data = stan_data, \n                     refresh = 0, chains = 1, \n                     iter_warmup = 500, iter_sampling = 4000, \n                     show_exceptions = F)\n\n\nRunning MCMC with 1 chain...\n\nChain 1 rho_prior: [3.58235,38.8276] \nChain 1 finished in 1.4 seconds.\n\n\nI’ll first plot the inverse gamma prior for the length-scale that was generated by the solver (green), alongside the posterior distribution of \\(\\rho\\) (black) and the minimum and maximum observed temporal distances of the surveys (dashed lines), using the distributional (O’Hara-Wild et al. 2024) and ggdist and tidybayes packages (Kay 2024a, 2024b). The length-scale seems to want to be smaller than the prior allows, which is due to me enforcing an informative prior. Recall that I pretended the survey intervals were chosen in a principled manner to estimate this parameter by specifying a prior implying the length-scale really doesn’t be much shorter than six weeks.\n\n\nCode\n# packages\nlibrary(distributional)\nlibrary(ggdist)\nlibrary(tidybayes)\n\n# plot\ntibble(prior = dist_inverse_gamma(3.58, scale = 38.83)) |&gt; \n  ggplot(aes(xdist = prior)) + \n  geom_vline(xintercept = stan_data$min_max_dist, \n             linetype = \"dashed\", linewidth = 1/2, colour = \"#333333\") +\n  stat_slab(fill = green, alpha = 2/3) + \n  stat_slab(aes(xdist = rho), \n            data = spread_rvars(fit_gp, rho), \n            fill = \"#333333\", alpha = 2/3) + \n  scale_x_continuous(breaks = seq(0, 60, 20), limits = c(0, 70), expand = c(0, 0)) + \n  scale_y_continuous(breaks = NULL, expand = c(0, 0)) + \n  theme(panel.border = element_rect(colour = NA), \n        axis.line.x = element_line(colour = \"#333333\")) + \n  labs(x = expression(rho), y = NULL)\n\n\n\n\n\nFocusing now on the posterior draws for the random survey effects \\(\\epsilon_i\\), it’s clear that the predictions between the normal and GP model are very similar.\n\n\nCode\n# add predictions to figure\nlibrary(tidybayes)\nfig + \n  stat_pointinterval(aes(ydist = exp(alpha + epsilon), \n                         shape = factor(model) |&gt; fct_rev()), \n                     data = fit_normal |&gt; \n                       spread_rvars(alpha, epsilon[i]) |&gt; \n                       mutate(model = \"Normal\") |&gt; \n                       bind_cols(dat) |&gt; \n                       bind_rows(\n                         fit_gp |&gt; \n                           spread_rvars(alpha, epsilon[i]) |&gt; \n                           mutate(model = \"GP\") |&gt; \n                           bind_cols(dat)\n                       ), \n                     position = position_dodge(width = 40),\n                     point_interval = \"median_hdci\", .width = 0.95, \n                     size = 1, linewidth = 1/2) + \n  scale_colour_manual(values = c(\"#333333\", green)) + \n  labs(shape = \"Model\")\n\n\n\n\n\nWe can also see that the standard deviation of the normal model is very similar to the marginal standard deviation of the GP model, albeit with more uncertainty. To be honest, the difference in the estimates of \\(\\tau\\) here may be driven more by our prior on \\(\\rho\\) that may be a bit more informative than it should be.\n\n\nCode\nfit_normal |&gt; \n  spread_rvars(tau) |&gt; \n  mutate(model = \"Normal\") |&gt; \n  bind_rows(fit_gp |&gt; \n              spread_rvars(tau) |&gt; \n              mutate(model = \"GP\")) |&gt; \n  ggplot(aes(factor(model) |&gt; fct_rev(), ydist = tau)) +\n  stat_pointinterval(point_interval = \"median_hdci\", .width = 0.95, \n                     size = 1/2, linewidth = 1/2) + \n  scale_colour_manual(values = c(\"#333333\", \"red4\")) + \n  labs(x = \"Model\", y = expression(paste(\"Posterior distribution of \", tau)))"
  },
  {
    "objectID": "blog/gaussian-processes.html#conclusion",
    "href": "blog/gaussian-processes.html#conclusion",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Conclusion",
    "text": "Conclusion\nUsing GPs to model random effects makes a lot of sense if there is some interest in modeling correlation between temporally or spatially organised data. Given that switching out the normally distributed random effects for a GP is trivial in modern probabilistic programming languages such as Stan, it makes sense to default to using them, especially as it includes involves just one extra parameter, the length-scale \\(\\rho\\)."
  },
  {
    "objectID": "blog/gaussian-processes.html#footnotes",
    "href": "blog/gaussian-processes.html#footnotes",
    "title": "Why you should default to Gaussian processes for random spatial and temporal effects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDan Simpson’s blog is a great place to learn more about GPs.↩︎\nWhen our input is multidimensional, such as coordinates for spatial data, we are still working with the Euclidean distance between vectors.↩︎\nI also did other things with those tadpoles, but I won’t go into that here.↩︎\nThis also happens to the penalised complexity (PC) prior for this parameter.↩︎\nThe case study is a great place to learn more about GPs more generally.↩︎"
  },
  {
    "objectID": "blog/hmm-in-stan.html",
    "href": "blog/hmm-in-stan.html",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "",
    "text": "The development of Bayesian statistical software had considerable influence on statistical ecology because many ecological models are complex and require custom likelihoods that were not necessarily available in off-the-shelf software such as R or Program Mark (Cooch and White 2008, Kéry and Royle 2015, Kéry and Royle 2020). One of the reasons why ecological models are so complex is because the data are often messy. For instance, in something like a randomised control trial, careful design means you can make stronger assumptions such as that measurements are obtained without error. However, ecological data are noisy and we frequently require complicated observation models that account for imperfect detection of species or individuals. As a result, a large class of ecological models can be formulated as hidden Markov models (HMMs), where time-series data are modeled with a (partially or completely) unobserved ecological model and an observation model conditioned on it. Examples of ecological HMMs include mark-recapture, occupancy, and \\(N\\)-mixture models, including their more complex multistate variants.\nHMMs are straightforward to model with non-gradient based Markov chain Monte Carlo (MCMC) methods because the latent ecological states can be sampled like parameters. Formulating these models became accessible to ecologists because of the simplicity afforded by statistical software like BUGS, JAGS, and NIMBLE (de Valpine et al. 2017). Consider the simple mark-recapture model where individuals \\(i \\in 1 : I\\) first captured at time \\(t = f_i\\) survive between times \\(t \\in f_i: T\\) with survival probability \\(\\phi\\) and are recaptured on each occasion with detection probability \\(p\\). The state-space formulation of this model from \\(t_{(f_i + 1):T}\\) with vague priors for \\(\\phi\\) and \\(p\\) is as follows:\n\\[\n\\begin{aligned}\n  z_{i,t} &\\sim \\textrm{Bernoulli} \\left( z_{i,t-1} \\times \\phi \\right) \\\\\n  y_{i,t} &\\sim \\textrm{Bernoulli} \\left( z_{i,t} \\times p \\right) \\\\\n  \\phi, p &\\sim \\textrm{Beta} \\left( 1, 1 \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(z_{i,t}\\) are the partially observed ecological states (where \\(z=1\\) is an alive individual and \\(z=0\\) is a dead individual) and \\(y_{i,t}\\) are the observed data (where \\(y=1\\) is an observed individual and \\(y=0\\) is an unobserved individual). Using NIMBLE 1.1.0 in R 4.3.2 (R Core Team 2023), coding this model is straightforward and follows the algebra above closely.\n\n\nCode\nlibrary(nimble)\ncmr_code &lt;- nimbleCode({\n  # likelihood\n  for (i in 1:n_ind) {\n    # initial state is known\n    z[i, first[i]] &lt;- y[i, first[i]]\n    # subsequent surveys\n    for (t in (first[i] + 1):n_surv) {\n      z[i, t] ~ dbern(z[i, t - 1] * phi)\n      y[i, t] ~ dbern(z[i, t] * p)\n    } # t\n  } # i\n  # priors\n  phi ~ dbeta(1, 1)\n  p ~ dbeta(1, 1)\n})\n\n\nAfter simulating some data, creating a NIMBLE model from the code, and configuring the MCMC, we can see that each unknown z[i, t] gets its own binary MCMC sampler.\n\n\nCode\n# metadata\nn_ind &lt;- 100\nn_surv &lt;- 8\n\n# parameters\nphi &lt;- 0.6\np &lt;- 0.7\n\n# containers\nz &lt;- y &lt;- z_known &lt;- matrix(NA, n_ind, n_surv)\nfirst &lt;- sample(1:(n_surv - 1), n_ind, replace = T) |&gt; sort()\nlast &lt;- numeric(n_ind)\n\n# simulation\nfor (i in 1:n_ind) {\n  z[i, first[i]] &lt;- y[i, first[i]] &lt;- 1\n  for (t in (first[i] + 1):n_surv) {\n    z[i, t] &lt;- rbinom(1, 1, z[i, t - 1] * phi)\n    y[i, t] &lt;- rbinom(1, 1, z[i, t] * p)\n  } # t\n  last[i] &lt;- max(which(y[i, ] == 1))\n  z_known[i, first[i]:last[i]] &lt;- 1\n} # i\n\n# create NIMBLE model object\nRmodel &lt;- nimbleModel(cmr_code, \n                      constants = list(n_ind = n_ind, n_surv = n_surv, first = first),\n                      data = list(y = y, z = z_known))\n\n# configure MCMC\nconf &lt;- configureMCMC(Rmodel)\n\n\n===== Monitors =====\nthin = 1: p, phi\n===== Samplers =====\nRW sampler (2)\n  - phi\n  - p\nbinary sampler (263)\n  - z[]  (263 elements)\n\n\nNote that because we know with certainty that individuals were alive between the first and last survey they were captured, we can actually ease computation by supplying those z[i, first[i]:last[i]] as known. We’ll run this model to confirm that it’s able to recover our input parameters using MCMCvis (Youngflesh 2018).\n\n\nCode\n# compile and build MCMC\nCmodel &lt;- compileNimble(Rmodel)\nCmcmc &lt;- buildMCMC(conf) |&gt; compileNimble(project = Cmodel, resetFunctions = T)\n\n# fit model\nn_iter &lt;- 1e3 ; n_chains &lt;- 4\nfit_cmr_nimble &lt;- runMCMC(Cmcmc, niter = n_iter * 2, nburnin = n_iter, nchains = n_chains)\n\n\n\n\nCode\n# summarise\nlibrary(MCMCvis)\nsummary_cmr_nimble &lt;- MCMCsummary(fit_cmr_nimble, params = c(\"phi\", \"p\")) |&gt;\n  as_tibble(rownames = \"variable\") |&gt;\n  mutate(truth = c(phi, p))\nsummary_cmr_nimble\n\n\n# A tibble: 2 × 9\n  variable  mean     sd `2.5%` `50%` `97.5%`  Rhat n.eff truth\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 phi      0.660 0.0360  0.586 0.660   0.727  1.02   633   0.6\n2 p        0.764 0.0487  0.669 0.765   0.861  1.02   429   0.7\n\n\nAs flexible as Bayesian software like BUGS and JAGS are, most of them do not use Hamiltonian Monte Carlo (HMC, Neal 1994) to generate samples from the posterior distribution, but see nimbleHMC for a recent implementation in NIMBLE. Without going into detail myself (but see Betancourt (2018)), HMC is generally considered a superior algorithm and moreover gives warnings when something goes wrong in the sampling, alerting the user to potential issues in the estimation. As a result, HMC should generally be preferred by practitioners but it has one trait that can be challenging to ecologists in particular: HMC requires that all parameters are continuous, meaning that our typical state-space formulations of HMMs cannot be fit with HMC due to the presence of latent discrete parameters like our ecological states z[i, t] above. The remainder of this post will focus on dealing with fitting complex ecological models using Stan (Carpenter et al. 2017), a probabilistic programming language which implements a state-of-the-art No U-Turn Sampler (NUTS, Hoﬀman and Gelman 2014). I present a unified approach that is applicable to simple and complex models alike, highlighted with three examples of increasing complexity."
  },
  {
    "objectID": "blog/hmm-in-stan.html#introduction",
    "href": "blog/hmm-in-stan.html#introduction",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "",
    "text": "The development of Bayesian statistical software had considerable influence on statistical ecology because many ecological models are complex and require custom likelihoods that were not necessarily available in off-the-shelf software such as R or Program Mark (Cooch and White 2008, Kéry and Royle 2015, Kéry and Royle 2020). One of the reasons why ecological models are so complex is because the data are often messy. For instance, in something like a randomised control trial, careful design means you can make stronger assumptions such as that measurements are obtained without error. However, ecological data are noisy and we frequently require complicated observation models that account for imperfect detection of species or individuals. As a result, a large class of ecological models can be formulated as hidden Markov models (HMMs), where time-series data are modeled with a (partially or completely) unobserved ecological model and an observation model conditioned on it. Examples of ecological HMMs include mark-recapture, occupancy, and \\(N\\)-mixture models, including their more complex multistate variants.\nHMMs are straightforward to model with non-gradient based Markov chain Monte Carlo (MCMC) methods because the latent ecological states can be sampled like parameters. Formulating these models became accessible to ecologists because of the simplicity afforded by statistical software like BUGS, JAGS, and NIMBLE (de Valpine et al. 2017). Consider the simple mark-recapture model where individuals \\(i \\in 1 : I\\) first captured at time \\(t = f_i\\) survive between times \\(t \\in f_i: T\\) with survival probability \\(\\phi\\) and are recaptured on each occasion with detection probability \\(p\\). The state-space formulation of this model from \\(t_{(f_i + 1):T}\\) with vague priors for \\(\\phi\\) and \\(p\\) is as follows:\n\\[\n\\begin{aligned}\n  z_{i,t} &\\sim \\textrm{Bernoulli} \\left( z_{i,t-1} \\times \\phi \\right) \\\\\n  y_{i,t} &\\sim \\textrm{Bernoulli} \\left( z_{i,t} \\times p \\right) \\\\\n  \\phi, p &\\sim \\textrm{Beta} \\left( 1, 1 \\right)\n\\end{aligned}\n\\tag{1}\\]\nwhere \\(z_{i,t}\\) are the partially observed ecological states (where \\(z=1\\) is an alive individual and \\(z=0\\) is a dead individual) and \\(y_{i,t}\\) are the observed data (where \\(y=1\\) is an observed individual and \\(y=0\\) is an unobserved individual). Using NIMBLE 1.1.0 in R 4.3.2 (R Core Team 2023), coding this model is straightforward and follows the algebra above closely.\n\n\nCode\nlibrary(nimble)\ncmr_code &lt;- nimbleCode({\n  # likelihood\n  for (i in 1:n_ind) {\n    # initial state is known\n    z[i, first[i]] &lt;- y[i, first[i]]\n    # subsequent surveys\n    for (t in (first[i] + 1):n_surv) {\n      z[i, t] ~ dbern(z[i, t - 1] * phi)\n      y[i, t] ~ dbern(z[i, t] * p)\n    } # t\n  } # i\n  # priors\n  phi ~ dbeta(1, 1)\n  p ~ dbeta(1, 1)\n})\n\n\nAfter simulating some data, creating a NIMBLE model from the code, and configuring the MCMC, we can see that each unknown z[i, t] gets its own binary MCMC sampler.\n\n\nCode\n# metadata\nn_ind &lt;- 100\nn_surv &lt;- 8\n\n# parameters\nphi &lt;- 0.6\np &lt;- 0.7\n\n# containers\nz &lt;- y &lt;- z_known &lt;- matrix(NA, n_ind, n_surv)\nfirst &lt;- sample(1:(n_surv - 1), n_ind, replace = T) |&gt; sort()\nlast &lt;- numeric(n_ind)\n\n# simulation\nfor (i in 1:n_ind) {\n  z[i, first[i]] &lt;- y[i, first[i]] &lt;- 1\n  for (t in (first[i] + 1):n_surv) {\n    z[i, t] &lt;- rbinom(1, 1, z[i, t - 1] * phi)\n    y[i, t] &lt;- rbinom(1, 1, z[i, t] * p)\n  } # t\n  last[i] &lt;- max(which(y[i, ] == 1))\n  z_known[i, first[i]:last[i]] &lt;- 1\n} # i\n\n# create NIMBLE model object\nRmodel &lt;- nimbleModel(cmr_code, \n                      constants = list(n_ind = n_ind, n_surv = n_surv, first = first),\n                      data = list(y = y, z = z_known))\n\n# configure MCMC\nconf &lt;- configureMCMC(Rmodel)\n\n\n===== Monitors =====\nthin = 1: p, phi\n===== Samplers =====\nRW sampler (2)\n  - phi\n  - p\nbinary sampler (263)\n  - z[]  (263 elements)\n\n\nNote that because we know with certainty that individuals were alive between the first and last survey they were captured, we can actually ease computation by supplying those z[i, first[i]:last[i]] as known. We’ll run this model to confirm that it’s able to recover our input parameters using MCMCvis (Youngflesh 2018).\n\n\nCode\n# compile and build MCMC\nCmodel &lt;- compileNimble(Rmodel)\nCmcmc &lt;- buildMCMC(conf) |&gt; compileNimble(project = Cmodel, resetFunctions = T)\n\n# fit model\nn_iter &lt;- 1e3 ; n_chains &lt;- 4\nfit_cmr_nimble &lt;- runMCMC(Cmcmc, niter = n_iter * 2, nburnin = n_iter, nchains = n_chains)\n\n\n\n\nCode\n# summarise\nlibrary(MCMCvis)\nsummary_cmr_nimble &lt;- MCMCsummary(fit_cmr_nimble, params = c(\"phi\", \"p\")) |&gt;\n  as_tibble(rownames = \"variable\") |&gt;\n  mutate(truth = c(phi, p))\nsummary_cmr_nimble\n\n\n# A tibble: 2 × 9\n  variable  mean     sd `2.5%` `50%` `97.5%`  Rhat n.eff truth\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 phi      0.660 0.0360  0.586 0.660   0.727  1.02   633   0.6\n2 p        0.764 0.0487  0.669 0.765   0.861  1.02   429   0.7\n\n\nAs flexible as Bayesian software like BUGS and JAGS are, most of them do not use Hamiltonian Monte Carlo (HMC, Neal 1994) to generate samples from the posterior distribution, but see nimbleHMC for a recent implementation in NIMBLE. Without going into detail myself (but see Betancourt (2018)), HMC is generally considered a superior algorithm and moreover gives warnings when something goes wrong in the sampling, alerting the user to potential issues in the estimation. As a result, HMC should generally be preferred by practitioners but it has one trait that can be challenging to ecologists in particular: HMC requires that all parameters are continuous, meaning that our typical state-space formulations of HMMs cannot be fit with HMC due to the presence of latent discrete parameters like our ecological states z[i, t] above. The remainder of this post will focus on dealing with fitting complex ecological models using Stan (Carpenter et al. 2017), a probabilistic programming language which implements a state-of-the-art No U-Turn Sampler (NUTS, Hoﬀman and Gelman 2014). I present a unified approach that is applicable to simple and complex models alike, highlighted with three examples of increasing complexity."
  },
  {
    "objectID": "blog/hmm-in-stan.html#marginalisation",
    "href": "blog/hmm-in-stan.html#marginalisation",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Marginalisation",
    "text": "Marginalisation\nSince HMC cannot sample discrete parameters, we have to reformulate our models without the latent states through a process called marginalisation. Marginalisation essentially just counts the mutually exclusive ways an observation can be made and sums these probabilities, as \\(\\Pr(A \\cup B) = \\Pr(A) + \\Pr(B)\\). For instance, in the above mark-recapture example, consider an individual was captured on occasion \\(t_1\\) and recaptured on occasion \\(t_2\\). The only way this could have happened is that the individual survived the interval \\(t_{1:2}\\) with probability \\(\\phi\\) and was recaptured on occasion \\(t_2\\) with probability \\(p\\), making the marginal likelihood of that datapoint \\(\\phi \\times p\\). However, if it was not observed at time \\(t_{2}\\), two things are possible: either the animal survived but was not recaptured with probability \\(\\phi \\times (1 - p)\\) or the individual died with probability \\(1 - \\phi\\) and was thus not recaptured. Summing these probabilities gives us the marginal likelihood for that datapoint, which is \\(\\phi \\times (1 - p) + (1 - \\phi)\\). Models can be marginalised in many different ways, but in this post I’m going to focus on the forward algorithm. Although it takes some getting used to, the forward algorithm facilitates fitting a wide variety of (increasingly complex) HMMs.\n\nForward algorithm\nFirst, I’m going to restructure our mark-recapture model with the introduction of transition probability matrices (TPMs). Formulating ecological models this way means we can apply the same strategy for a wide variety of models. In the mark-recapture example, we still have two latent ecological states: (1) dead and (2) alive. The transitions from one state to the other is given by the following TPM, where the states of departure (time \\(t - 1\\)) are in the rows and states of arrival (time \\(t\\)) are in the columns (note that the rows of TPMs must sum to 1):\n\\[\n\\mathrm{TPM}_z = \\begin{bmatrix}\n  1 & 0 \\\\\n  1 - \\phi & \\phi          \n\\end{bmatrix}\n\\tag{2}\\]\nHere, the dead state is an absorbing state and individuals remain in the alive states with probability \\(\\phi\\). The observation process still deals with two observed states, (1) not detected and (2) detected, and is also formulated as a \\(2 \\times 2\\) TPM, where the ecological states are in the rows (states of departure) and the observed states in the columns (states of arrival):\n\\[\n\\mathrm{TPM}_y = \\begin{bmatrix}\n  1 & 0 \\\\\n  1 - p & p\n\\end{bmatrix}\n\\tag{3}\\]\nThe forward algorithm works by computing the marginal likelihood of an individual being in each state \\(s \\in 1 : S\\) at time \\(t\\) given the observations up to time \\(t\\), stored in a \\(T \\times S\\) matrix we’ll call \\(\\mathrm{MPS}\\) (Marginal Probability per State). From here on, the notation for subsetting row \\(i\\) of a matrix \\(M\\) is \\(M_i\\) and \\(M_{:j}\\) for subsetting column \\(j\\). The forward algorithm starts from initial state probabilities, which in the mark-recapture example are known, given that we know with certainty that every individual is alive at first capture:\n\\[\n\\mathrm{MPS}_{f_i} = \\left[ 0, 1 \\right]\n\\]\nFor subsequent occasions after an individual’s first capture \\(t_{(f_i+1):T}\\), the marginal probabilities of being in state \\(s\\) up to time \\(t\\) given the observations that came before \\(t\\) are computed as:\n\\[\n\\mathrm{MPS}_{t} = \\mathrm{MPS}_{t-1} \\times \\mathrm{TPM}_z \\odot {\\mathrm{TPM}_y}_{[:y_{i, t}]}^\\intercal\n\\]\nThat is, matrix multiplying the marginal probabilities at time \\(t-1\\) with the ecological TPM and then element-wise multiplying (where \\(\\odot\\) denotes the Hadamard or element-wise product) the resulting row-vector with the transposed relevant column of the observation TPM, where the first column is used if the individual was not observed (\\(y_{i,t} = 1\\)) and the second column if it was observed (\\(y_{i,t} = 2\\)). The forward algorithm continues until survey \\(T\\) after which the log density is incremented with the log of the sum of \\(\\mathrm{MPS}_T\\), the probabilities of being in each state at the end of the study period."
  },
  {
    "objectID": "blog/hmm-in-stan.html#example-1-basic-mark-recapture-as-a-hmm",
    "href": "blog/hmm-in-stan.html#example-1-basic-mark-recapture-as-a-hmm",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Example 1: Basic mark-recapture as a HMM",
    "text": "Example 1: Basic mark-recapture as a HMM\nAs a first example of structuring ecological models as HMMs with matrix multiplication, I have translated the initial mark-recapture model for use in Stan. I wrote two versions, one with the probabilities which most closely resembles the above algebra and one with log probabilities, which is what Stan naturally works with. Doing so involves swapping the multiplication of probabilities with summing the log probabilities, changing log(sum()) to log_sum_exp(), and performing matrix multiplication on the log scale with a custom function log_product_exp(), which I found on Stack Overflow (note that the function is overloaded to account for the dimensions of your two matrices). I have a few comments:\n\nFor computational efficiency with column-major ordering in Stan (also true for R, BUGS, and NIMBLE, for that matter), the above linear algebra operations are happening in reverse with a transposed \\(\\mathrm{TPM}_z\\) and \\(\\mathrm{MPS}\\). The ecological TPM is still constructed as above but transposed afterwards.\nI am getting into the habit of initialising \\(\\mathrm{MPS}_t\\) with the probabilities associated with the observation process (that is, the lowest hierarchical level of the HMM), as this is the only part of the model that is directly conditioned on the observed data. These marginal detection probabilities can then be incremented with the ecological process, where the .*= operator in Stan element-wise multiplies mps[:, t] with the expression on the right of the operator. Structuring even the simplest model this way makes understanding the more complicated ones coming up more manageable.\nJust like in the NIMBLE version, we don’t have to worry about the probabilities associated with being dead between the first and last capture. Note that for the occasions between first and last capture, I only compute the marginal probabilities of the alive state, fixing those of the dead state to 0 (or negative_infinity() for log).\nEven though these models don’t have latent ecological states as discrete parameters, they can still be derived using the Viterbi algorithm. This is performed in the generated quantities block, and for details I recommend studying the Stan User’s Guide. Note that in the simple mark-recapture example we already know that individuals were alive in state 2 between the first and last occasion of capture, so the Viterbi algorithm is only implemented from (last[i] + 1):n_surv.\nIn all models, I also compute the log likelihood in the vector log_lik for potential use in model checking using the loo package (Vehtari et al. 2023).\nMost parts of the model block are repeated in generated quantities. Although this makes the whole program more verbose, it is more efficient to compute derived quantities in the generated quantities block as they are only computed once per HMC iteration, whereas the transformed parameters and the model blocks get updated with each gradient evaluation, of which there can be many per iteration.\n\n\nStan programs\n\nProbabilitiesLog Probabilities\n\n\n\n\nCode\ndata {\n  int&lt;lower=1&gt; n_ind, n_surv;\n  array[n_ind] int&lt;lower=1, upper=n_surv-1&gt; first;\n  array[n_ind] int&lt;lower=first, upper=n_surv&gt; last;\n  array[n_ind, n_surv] int&lt;lower=1, upper=2&gt; y;\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; phi, p;\n}\n\nmodel {\n  // TPM containers\n  matrix[2, 2] tpm_z, tpm_y;\n  \n  // ecological TPM (transpose once)\n  tpm_z[1, 1] = 1;\n  tpm_z[1, 2] = 0;\n  tpm_z[2, 1] = 1 - phi;\n  tpm_z[2, 2] = phi;\n  tpm_z = tpm_z';\n  \n  // observation TPM\n  tpm_y[1, 1] = 1;\n  tpm_y[1, 2] = 0;\n  tpm_y[2, 1] = 1 - p;\n  tpm_y[2, 2] = p;\n  \n  // marginal probabilities per state\n  matrix[2, n_surv] mps;\n  \n  // likelihood\n  for (i in 1:n_ind) {\n    \n    // initial state probabilities\n    mps[:, first[i]] = [ 0, 1 ]';\n    \n    // from first to last capture, only alive states conditioned on data\n    if (first[i] &lt; last[i]) {\n      for (t in (first[i] + 1):last[i]) {\n        \n        // initialise marginal probability with observation process\n        mps[2, t] = tpm_y[2, y[i, t]];\n        \n        // increment with ecological process\n        mps[2, t] *= tpm_z[2, 2] * mps[2, t - 1];\n      } // t\n    }\n    \n    // ensure dead state at last capture is impossible\n    mps[1, last[i]] = 0;\n    \n    // if individual was not captured on last survey\n    if (last[i] &lt; n_surv) {\n      \n      // after last capture, condition both states on data\n      for (t in (last[i] + 1):n_surv) {\n        \n        // initialise marginal probabilities with observation process\n        mps[:, t] = tpm_y[:, y[i, t]];\n        \n        // increment with ecological process (matrix multiplication)\n        mps[:, t] .*= tpm_z * mps[:, t - 1];\n      } // t\n    }\n    \n    // increment log density\n    target += log(sum(mps[:, n_surv]));\n    \n  } // i\n  \n  // priors\n  target += beta_lupdf(phi | 1, 1);\n  target += beta_lupdf(p | 1, 1);\n}\n\ngenerated quantities {\n  array[n_ind, n_surv] int z;\n  vector[n_ind] log_lik;\n  {\n    matrix[2, 2] tpm_z, tpm_y;\n    tpm_z[1, 1] = 1;\n    tpm_z[1, 2] = 0;\n    tpm_z[2, 1] = 1 - phi;\n    tpm_z[2, 2] = phi;\n    tpm_z = tpm_z';\n    tpm_y[1, 1] = 1;\n    tpm_y[1, 2] = 0;\n    tpm_y[2, 1] = 1 - p;\n    tpm_y[2, 2] = p;\n    array[2, n_surv] int back_ptr;\n    matrix[2, n_surv] mps, best_p;\n    real tmp;\n    int tt;\n    \n    for (i in 1:n_ind) {\n    \n      // log-lik\n      mps[:, first[i]] = [ 0, 1 ]';\n      if (first[i] &lt; last[i]) {\n        for (t in (first[i] + 1):last[i]) {\n          mps[2, t] = tpm_y[2, y[i, t]];\n          mps[2, t] *= tpm_z[2, 2] * mps[2, t - 1];\n        } // t\n      }\n      mps[1, last[i]] = 0;\n      \n      // Viterbi\n      best_p[:, last[i]] = [ 0, 1 ]';\n      if (last[i] &lt; n_surv) {\n        best_p[:, (last[i] + 1):n_surv] = rep_matrix(0, 2, n_surv - last[i]);\n        for (t in (last[i] + 1):n_surv) {\n          // marginal observation probabilities\n          mps[:, t] = tpm_y[:, y[i, t]];\n          for (s_a in 1:2) {  // state of arrival\n            for (s_d in 1:2) { // state of departure\n              tmp = best_p[s_d, t - 1] * tpm_z[s_a, s_d] * mps[s_a, t];\n              if (tmp &gt; best_p[s_a, t]) {\n                back_ptr[s_a, t] = s_d;\n                best_p[s_a, t] = tmp;\n              }\n            } // s_d\n          } // s_a\n          \n          // increment with ecological process for log-lik\n          mps[:, t] .*= tpm_z * mps[:, t - 1];\n        } // t\n      }\n      log_lik[i] = log(sum(mps[:, n_surv]));\n      \n      // ecological states\n      z[i, first[i]:last[i]] = rep_array(2, last[i] - first[i] + 1);\n      if (last[i] &lt; n_surv) {\n        tmp = max(best_p[:, n_surv]);\n        for (s_a in 1:2) {\n          if (best_p[s_a, n_surv] == tmp) {\n            z[i, n_surv] = s_a;\n          }\n        } // s_a\n        if (last[i] &lt; (n_surv - 1)) {\n          for (t in (last[i] + 1):(n_surv - 1)) {\n            tt = n_surv - t + last[i] + 1;\n            z[i, tt - 1] = back_ptr[z[i, tt], tt];\n          } // t\n        }\n      }\n    } // i\n  }\n}\n\n\n\n\n\n\nCode\nfunctions{\n  /**\n   * Return the natural logarithm of the product of the element-wise exponentiation of the specified matrices\n   *\n   * @param a  First matrix or (row_)vector\n   * @param b  Second matrix or (row_)vector\n   *\n   * @return   log(exp(a) * exp(b))\n   */\n  matrix log_product_exp(matrix a, matrix b) {\n    int x = rows(a);\n    int y = cols(b);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    matrix[x, y] c;\n    for (j in 1:y) {\n      for (i in 1:x) {\n        c[i, j] = log_sum_exp(a_tr[:, i] + b[:, j]);\n      }\n    }\n    return c;\n  }\n  vector log_product_exp(matrix a, vector b) {\n    int x = rows(a);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    vector[x] c;\n    for (i in 1:x) {\n      c[i] = log_sum_exp(a_tr[:, i] + b);\n    }\n    return c;\n  }\n  row_vector log_product_exp(row_vector a, matrix b) {\n    int y = cols(b);\n    vector[size(a)] a_tr = a';\n    row_vector[y] c;\n    for (j in 1:y) {\n      c[j] = log_sum_exp(a_tr + b[:, j]);\n    }\n    return c;\n  }\n  real log_product_exp(row_vector a, vector b) {\n    real c = log_sum_exp(a' + b);\n    return c;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; n_ind, n_surv;\n  array[n_ind] int&lt;lower=1, upper=n_surv-1&gt; first;\n  array[n_ind] int&lt;lower=first, upper=n_surv&gt; last;\n  array[n_ind, n_surv] int&lt;lower=1, upper=2&gt; y;\n}\n\nparameters {\n  real&lt;lower=0, upper=1&gt; phi, p;\n}\n\nmodel {\n  // log TPM containers\n  matrix[2, 2] ltpm_z, ltpm_y;\n  \n  // ecological ltpm\n  ltpm_z[1, 1] = 0;\n  ltpm_z[1, 2] = negative_infinity();\n  ltpm_z[2, 1] = log1m(phi);\n  ltpm_z[2, 2] = log(phi);\n  ltpm_z = ltpm_z';\n  \n  // observation ltpm\n  ltpm_y[1, 1] = 0;\n  ltpm_y[1, 2] = negative_infinity();\n  ltpm_y[2, 1] = log1m(p);\n  ltpm_y[2, 2] = log(p);\n  \n  // marginal log probabilities per state\n  matrix[2, n_surv] lmps;\n  \n  // likelihood\n  for (i in 1:n_ind) {\n    \n    // initial state log probabilities\n    lmps[:, first[i]] = [ negative_infinity(), 0 ]';\n    \n    // from first to last capture, only alive states conditioned on data\n    if (first[i] &lt; last[i]) {\n      for (t in (first[i] + 1):last[i]) {\n        \n        // initialise marginal log probability with observation process\n        lmps[2, t] = ltpm_y[2, y[i, t]];\n        \n        // increment with ecological process\n        lmps[2, t] += ltpm_z[2, 2] + lmps[2, t - 1];\n      } // t\n    }\n    \n    // ensure dead state at last capture is impossible\n    lmps[1, last[i]] = negative_infinity();\n    \n    // if individual was not captured on last survey\n    if (last[i] &lt; n_surv) {\n      \n      // after last capture, condition both states on data\n      for (t in (last[i] + 1):n_surv) {\n        \n        // initialise marginal log probabilities with observation process\n        lmps[:, t] = ltpm_y[:, y[i, t]];\n        \n        // increment with ecological process (log matrix multiplication)\n        lmps[:, t] += log_product_exp(ltpm_z, lmps[:, t - 1]);\n      } // t\n    }\n    \n    // increment log density\n    target += log_sum_exp(lmps[:, n_surv]);\n    \n  } // i\n  \n  // priors\n  target += beta_lupdf(phi | 1, 1);\n  target += beta_lupdf(p | 1, 1);\n}\n\ngenerated quantities {\n  array[n_ind, n_surv] int z;\n  vector[n_ind] log_lik;\n  {\n    matrix[2, 2] ltpm_z, ltpm_y, trm;\n    ltpm_z[1, 1] = 0;\n    ltpm_z[1, 2] = negative_infinity();\n    ltpm_z[2, 1] = log1m(phi);\n    ltpm_z[2, 2] = log(phi);\n    ltpm_z = ltpm_z';\n    ltpm_y[1, 1] = 0;\n    ltpm_y[1, 2] = negative_infinity();\n    ltpm_y[2, 1] = log1m(p);\n    ltpm_y[2, 2] = log(p);\n    array[2, n_surv] int back_ptr;\n    matrix[2, n_surv] lmps, best_lp;\n    real tmp;\n    int tt;\n    \n    for (i in 1:n_ind) {\n      \n      // log-lik\n      lmps[:, first[i]] = [ negative_infinity(), 0 ]';\n      if (first[i] &lt; last[i]) {\n        for (t in (first[i] + 1):last[i]) {\n          lmps[2, t] = ltpm_y[2, y[i, t]];\n          lmps[2, t] += ltpm_z[2, 2] + lmps[2, t - 1];\n        } // t\n      }\n      lmps[1, last[i]] = negative_infinity();\n      \n      // Viterbi\n      best_lp[:, last[i]] = [ negative_infinity(), 0 ]';\n      if (last[i] &lt; n_surv) {\n        best_lp[:, (last[i] + 1):n_surv] = rep_matrix(negative_infinity(), 2, n_surv - last[i]);\n        for (t in (last[i] + 1):n_surv) {\n          // marginal observation log probabilities\n          lmps[:, t] = ltpm_y[:, y[i, t]];\n          for (s_a in 1:2) {  // state of arrival\n            for (s_d in 1:2) { // state of departure\n              tmp = best_lp[s_d, t - 1] + ltpm_z[s_a, s_d] + lmps[s_a, t];\n              if (tmp &gt; best_lp[s_a, t]) {\n                back_ptr[s_a, t] = s_d;\n                best_lp[s_a, t] = tmp;\n              }\n            } // s_d\n          } // s_a\n          \n          // increment with ecological process for log-lik\n          lmps[:, t] += log_product_exp(ltpm_z, lmps[:, t - 1]);\n        } // t\n      }\n      log_lik[i] = log_sum_exp(lmps[:, n_surv]);\n      \n      // ecological states\n      z[i, first[i]:last[i]] = rep_array(2, last[i] - first[i] + 1);\n      if (last[i] &lt; n_surv) {\n        tmp = max(best_lp[:, n_surv]);\n        for (s_a in 1:2) {\n          if (best_lp[s_a, n_surv] == tmp) {\n            z[i, n_surv] = s_a;\n          }\n        } // s_a\n        if (last[i] &lt; (n_surv - 1)) {\n          for (t in (last[i] + 1):(n_surv - 1)) {\n            tt = n_surv - t + last[i] + 1;\n            z[i, tt - 1] = back_ptr[z[i, tt], tt];\n          } // t\n        }\n      }\n    } // i\n  }\n}\n\n\n\n\n\n\n\nWhy we like HMC\nUsing CmdStanR 0.7.1 to call CmdStan 2.34.1 (Gabry et al. 2023), we’ll just run the log probability model for the same number of iterations as the NIMBLE version, and we see that there were no issues with sampling (Stan would tell us otherwise) and the effective sample sizes (ESS) were several times higher than for the NIMBLE model. This is why we want to use HMC.\n\n\nCode\n# data for Stan\ncmr_data &lt;- list(n_ind = n_ind, n_surv = n_surv, \n                 first = first, last = last, \n                 y = y + 1) |&gt;\n  # no NAs in Stan\n  sapply(\\(x) replace(x, is.na(x), 1))\n\n# run HMC\nfit_cmr &lt;- cmr_lp$sample(data = cmr_data, refresh = 0,\n                         chains = n_chains, parallel_chains = n_chains, iter_warmup = n_iter, iter_sampling = n_iter)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 2.3 seconds.\nChain 2 finished in 2.5 seconds.\nChain 3 finished in 2.5 seconds.\nChain 4 finished in 2.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.4 seconds.\nTotal execution time: 2.6 seconds.\n\n\nCode\n# summary\nlibrary(tidyverse)\nfit_cmr$summary(c(\"phi\", \"p\")) |&gt; \n  select(variable, median, contains(\"q\"), contains(\"ess\")) |&gt;\n  mutate(ess_nimble = summary_cmr_nimble$n.eff,\n         truth = c(phi, p))\n\n\n# A tibble: 2 × 8\n  variable median    q5   q95 ess_bulk ess_tail ess_nimble truth\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 phi       0.662 0.599 0.720    2325.    2402.        633   0.6\n2 p         0.763 0.673 0.840    2163.    2446.        429   0.7\n\n\nAnd just to confirm, we check if the Viterbi algorithm was able to closely recover the latent alive states.\n\n\nCode\n# generate estimated z matrix\nz_est &lt;- fit_cmr$summary(\"z\")$median |&gt;\n  matrix(n_ind, n_surv) |&gt; \n  {\\(z) ifelse(z &lt; 1, NA, ifelse(z == 1, 0, 1))}()\n\n# what proportion of latent z states correspond with the truth?\nn_unknown &lt;- n_surv - last\ncorrect &lt;- numeric(n_ind)\nfor (i in 1:n_ind) {\n  if (n_unknown[i] &gt; 0) {\n    unknowns &lt;- (last[i] + 1):n_surv\n    correct[i] &lt;- sum((z_est == z)[i, unknowns])\n  }\n}\nsum(correct) / sum(n_unknown)\n\n\n[1] 0.966"
  },
  {
    "objectID": "blog/hmm-in-stan.html#example-2-dynamic-multistate-occupancy-model",
    "href": "blog/hmm-in-stan.html#example-2-dynamic-multistate-occupancy-model",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Example 2: Dynamic multistate occupancy model",
    "text": "Example 2: Dynamic multistate occupancy model\nIn order to highlight the generality of the above modeling approach, we’re going to switch gears to a dynamic occupancy model. These models are also just HMMs, with some trivial differences to the basic mark-recapture model that can seem daunting at first:\n\nThere can be more than two states.\nThe initial state probabilities here are unknown, that is, we don’t know the occupancy state of a site when it’s first surveyed.\nState transitions can happen in more than one direction, whereas dead individuals in mark-recapture can never transition to alive.\n\nConsider a study area containing sites \\(i \\in 1 : I\\) surveyed across \\(t \\in 1 : T\\) years. We are investigating whether sites are occupied by tiger snakes (Notechis scutatus) in each year, but want to differentiate between sites that simply support tiger snakes or those that support breeding. Our ecological process thus has three possible states: (1) unoccupied by tiger snakes, (2) occupied by tiger snakes but not for breeding, or (3) occupied by breeding tiger snakes. The observation process also has three possible states: (1) no snakes found, (2) snakes found but no evidence of breeding, and (3) snakes found with evidence of breeding, as determined by the discovery of gravid females. Instead of directly formulating the ecological TPM here, we’ll do it in continuous time instead using a transition rate matrix (TRM).\n\n\n\nA rainforest tiger snake (Notechis scutatus) from northern New South Wales, Australia.\n\n\n\n\n\n\n\n\nEcology occurs in continuous time\n\n\n\nTPMs assume equal intervals between surveys, but this is the exception rather than the norm for ecological data. Although things like survival probabilities can easily be exponentiated to the time interval, this doesn’t work for processes that are not absorbing states, i.e., snakes moving in and out of sites. Unlike individual survival, sites are free to transition between states from year to year. Even with simpler models, however, it often makes more sense to model the survival process with mortality hazard rates (where the survival probability \\(S\\) is related to the mortality hazard rate \\(h\\) as \\(S = \\exp(-h)\\)) for a number of reasons (Ergon et al. 2018). The multistate extension of this is straightforward, where we instead take the matrix exponential of a TRM (Glennie et al. 2022). By constructing our transition matrices as TRMs, where the off-diagonals contain instantaneous hazard rates and the diagonals contain negative sums of these off-diagonals (ensuring rows sum to 0, not 1), we can generate time-specific TPMs by taking the matrix exponential of the TRM multiplied by the time interval between occasions (\\(\\tau_t\\)), thereby essentially marginalising over possible state transitions that occurred during survey intervals:\n\\[\n\\mathrm{TPM}_t = \\exp \\left( \\mathrm{TRM} \\times \\tau_t \\right)\n\\tag{4}\\]\n\n\nA TRM for the tiger snake example might look something like this, with again states of departure (year \\(t-1\\)) in the rows and states of arrival (year \\(t\\)) in the columns:\n\\[\n\\mathrm{TRM}_z = \\begin{bmatrix}\n  -(\\gamma_1 + \\gamma_2) & \\gamma_1 & \\gamma_2 \\\\\n  \\epsilon_1 & -(\\epsilon_1 + \\psi_1) & \\psi_1 \\\\\n  \\epsilon_2 & \\psi_2 & -(\\epsilon_2 + \\psi_2)\n\\end{bmatrix}\n\\tag{5}\\] with the following parameters:\n\n\\(\\gamma_1\\): the “non-breeding” colonisation rate, or the rate at which a site becomes occupied with non-breeding snakes at time \\(t\\) when the site was not occupied at time \\(t-1\\).\n\\(\\gamma_2\\): the “breeding” colonisation rate, or the rate at which a site becomes occupied with breeding snakes at time \\(t\\) when the site was not occupied at time \\(t-1\\).\n\\(\\epsilon_1\\): the “non-breeding” emigration rate, or the rate at which a site becomes onoccupied at time \\(t\\) when it was occupied with non-breeding snakes at time \\(t-1\\).\n\\(\\epsilon_2\\): the “breeding” emigration rate, or the rate at which a site becomes onoccupied at time \\(t\\) when it was occupied with breeding snakes at time \\(t-1\\).\n\\(\\psi_1\\): the breeding rate, or the rate at which a site becomes used for breeding at time \\(t\\) when the site was occupied but not used for breeding at time \\(t-1\\).\n\\(\\psi_2\\): the breeding cessation rate, or the rate at which a site stops being used for breeding at time \\(t\\) when the site was used for breeding at time \\(t-1\\).\n\nNote that even if these parameters aren’t exactly the quantities of interest, derivatives of these parameters are easily derived as generated quantities after parameter estimation. Of course, these parameters can be modeled more complexly with linear or non-linear effects at the level of years, sites, or both, with the only changes being that the TRMs are constructed at those levels (for instance, by creating site- and year-level TRMs where the parameters populating the TRM are allowed to vary at that level—see Example 3 for an application).\nIn addition to the TRM/TPM, the ecological process is initiated on the first occasion with an initial state vector, giving the probabilities of being in each state at \\(t_1\\), which have to sum to 1:\n\\[\n  \\boldsymbol{\\eta} = \\left[ \\eta_1 , \\eta_2 , \\eta_3 \\right]\n\\tag{6}\\]\n\nSimulating continuous time\nBelow, I’ll simulate the ecological data by specifying a TRM and using the expm package to take the matrix exponential to generate the TPM (Goulet et al. 2021). In order to simulate ecological states z[i, t] we use the categorical distribution, which is just the Bernoulli distribution generalised to multiple outcomes. Note that because we’re simulating the ecological process from year to year, the time intervals \\(\\tau\\) are just 1; however, in the Stan program below, we will account for missing years using tau.\n\n\nCode\n# metadata\nn_site &lt;- 100\nn_year &lt;- 10\nn_sec &lt;- 4\n\n# parameters\neta &lt;- c(NA, 0.3, 0.4)       # unoccupied/non-breeding/breeding initial probabilities\neta[1] &lt;- 1 - sum(eta[2:3])  # ensure probabilities sum to 1 (simplex) \ngamma &lt;- c(0.3, 0.2)         # non-breeding/breeding colonisation rates\nepsilon &lt;- c(0.4, 0.1)       # non-breeding/breeding emigration rates\npsi &lt;- c(0.4, 0.3)           # breeding/breeding cessation rates\n\n# function for random categorical draw\nrcat &lt;- function(n, prob) {\n  return(\n    rmultinom(n, size = 1, prob) |&gt; \n      apply(2, \\(x) which(x == 1))\n  )\n}\n\n# TRM\ntrm &lt;- matrix(c(-(gamma[1] + gamma[2]), gamma[1], gamma[2],\n                epsilon[1], -(epsilon[1] + psi[1]), psi[1],\n                epsilon[2], psi[2], -(epsilon[2] + psi[2])),\n              3, byrow = T)\ntrm\n\n\n     [,1] [,2] [,3]\n[1,] -0.5  0.3  0.2\n[2,]  0.4 -0.8  0.4\n[3,]  0.1  0.3 -0.4\n\n\nCode\n# here's the TPM\nlibrary(expm)\ntpm_z &lt;- expm(trm)\ntpm_z\n\n\n      [,1]  [,2]  [,3]\n[1,] 0.650 0.182 0.168\n[2,] 0.231 0.515 0.254\n[3,] 0.101 0.182 0.717\n\n\nCode\n# containers\nz &lt;- matrix(NA, n_site, n_year)\n\n# ecological process simulation\nfor (i in 1:n_site) {\n  \n  # initial states\n  z[i, 1] &lt;- rcat(1, eta)\n  \n  # subsequent years\n  for (t in 2:n_year) {\n    z[i, t] &lt;- rcat(1, tpm_z[z[i, t - 1], ])\n  } # t\n} # i\n\n# some ecological states\nhead(z, 10)\n\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    1    1    3    1    2    2    3    1    1     1\n [2,]    3    2    3    3    3    3    3    3    2     2\n [3,]    2    2    2    2    3    3    3    3    3     3\n [4,]    2    2    2    2    2    3    1    1    1     1\n [5,]    1    1    1    1    1    1    2    3    3     3\n [6,]    1    2    2    3    1    1    2    3    1     1\n [7,]    2    2    3    3    3    3    1    2    2     1\n [8,]    3    3    3    3    3    3    3    2    3     3\n [9,]    1    2    2    3    3    2    2    2    1     3\n[10,]    3    2    2    1    1    1    2    1    1     1\n\n\n\n\nObservation process\nAlthough possible to construct in continuous time, often our observation model makes more sense in discrete time, especially if specific surveys were conducted instead of with something like an automated recording device. For instance, with dynamic occupancy models, an investigator would conduct multiple “secondary” surveys within a season or year in order to disentangle the ecological process from the observation process.\n\n\n\n\n\n\nRobust Design\n\n\n\nWith multistate models where individuals or sites can transition between ecological states, in order to reliably estimate transitions rates and detection probabilities we need to conduct multiple consecutive surveys within periods of assumed closure, yielding secondary surveys nested within primary occasions. In the absence of this, there are parameter identifiability issues for the state transition rates and state-specific detection probabilities. This survey strategy was originally called the “robust design” (Pollock 1982) and the necessity of applying it for multistate models more generally has gone largely unappreciated by ecologists.\n\n\nAs in the mark-recapture example, we construct an observation TPM where the ecological states are in the rows (states of departure) and observed states in the columns (states of arrival), recognising that there’s uncertainty in being able to detect breeding:\n\\[\n\\mathrm{TPM}_y = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  1 - p_1 & p_1 & 0 \\\\\n  1 - p_2 & p_2 \\times (1 - \\delta) & p_2 \\times \\delta\n\\end{bmatrix}\n\\tag{7}\\] with the following parameters:\n\n\\(p_{1:2}\\): the state-specific detection probabilities, or the probabilities of detecting tiger snakes if they are present but not breeding (\\(p_1\\)) or present and breeding (\\(p_2\\)).\n\\(\\delta\\): the breeding detection probability, or the probability of detecting tiger snake breeding conditional on it occurring at a site. Note that in this example, breeding is detected when gravid females are found. Note that false-positive detections of any kind (erroneous snake identification, etc.) are not considered, but will be in Example 3.\n\nI’ll simulate some observational data below where the investigators aimed to conduct 4 secondary surveys per year (primary occasion), all conducted during the tiger snake breeding season, with some secondary surveys missed, and some years missed altogether.\n\n\nCode\n# parameters\np &lt;- c(0.4, 0.5)  # non-breeding/breeding detection probabilities\ndelta &lt;- 0.6      # breeding detection probability\n\n# TPM\ntpm_y &lt;- matrix(c(1, 0, 0,\n                  1 - p[1], p[1], 0,\n                  1 - p[2], p[2] * (1 - delta), p[2] * delta),\n                3, byrow = T)\n\n# containers\nn_year_i &lt;- numeric(n_site)\nyears &lt;- n_sec_it &lt;- matrix(NA, n_site, n_year)\nsecs &lt;- array(NA, c(n_site, n_year, n_sec))\ny &lt;- array(NA, c(n_site, n_year, n_sec))\n\n# simulation\nfor (i in 1:n_site) {\n  \n  # number of years and sequence of years surveyed (always surveyed in first year)\n  n_year_i[i] &lt;- 1 + rbinom(1, n_year - 1, 0.8)\n  years[i, 1:n_year_i[i]] &lt;- c(1, sample(2:n_year, n_year_i[i] - 1) |&gt; sort())\n  \n  for (t in years[i, 1:n_year_i[i]]) {\n    \n    # number of secondaries per site and year and sequence of secondaries surveyed\n    n_sec_it[i, t] &lt;- 1 + rbinom(1, n_sec - 1, 0.8)\n    secs[i, t, 1:n_sec_it[i, t]] &lt;- sample(1:n_sec, n_sec_it[i, t]) |&gt; sort()\n    \n    # observation process\n    y[i, t, secs[i, t, 1:n_sec_it[i, t]]] &lt;- rcat(n_sec_it[i, t], tpm_y[z[i, t], ])\n  } # t\n} # i\n\n\nI recognise that some of the indexing here is daunting but it just makes sure that our data is contained in the right parts of the detection arrays and that Stan isn’t going to have to deal with NAs in the data, which is less straightfoward than it is in JAGS and NIMBLE.\n\n\n\n\n\n\nIndexing\n\n\n\nIn JAGS and NIMBLE we don’t really have to worry about NAs because MCMC samplers are assigned to unobserved data, but that is inefficient. Things aren’t so simple in Stan and we’re better off just pointing the model to where the observations sit in data arrays. In the above example, n_year_i is an n_site length vector holding the number of surveyed years per site. years is an n_site * n_year matrix with the sequence of years observed, which for n_year_i[i] = 6 might looks like years[i, ] = [1, 3, 4, 6, 7, 8, NA, NA, NA, NA]. The NAs will have to be replaced with some other value by the time they get fed to Stan, but you don’t have to worry about them because they won’t get used anyway. n_sec is the maximum number of secondary surveys, and n_sec_it[i, t] holds the number of secondary surveys conducted per site and year. Just like years, secs holds the sequence of specific secondaries that were surveyed for that site and year. Putting it together, in each value t of years[i, 1:n_year_i[i]], y[i, t, secs[i, t, 1:n_sec_it[i, t]]] selects the observations of a site only for the secondaries in the years that were surveyed. In the Stan program below, this subset of y is used to select the observations to extract the associated probabilities out of the observation log TPM.\n\n\n\n\nStan program\nBelow is the Stan program for the above model written with log probabilities. In many ways it is similar to the mark-recapture example, with the following noteworthy changes:\n\nThe site- and year-specific time intervals tau[i, t] between surveyed years are computed in the transformed data block. Additionally, a logical q is computed indicating whether or not sites were surveyed in each year.\nAll entries in the \\(3 \\times T\\) log MPS lmps are initialised with the observation process. Note that to account for the replicate secondary surveys, the relevant log probabilities of each secondary are subsetted from the observation TPM for each possible ecological state. For example, consider a detection vector of site \\(i\\), year \\(t\\), and secondaries \\(1:K_{i,t}\\) of y[i, t, secs[i, t, 1:n_sec_it[i, t]]] = [2, 3, 1, 2], showing that snakes were only observed during secondary 1, 2, and 4, and that a gravid female was only observed on the second occasion. The detection vector corresponding to ecological state 3 (present and breeding) would be \\(\\mathrm{TPM}_{3,[2, 3, 1, 3]} = \\left[ p_2 \\times (1 - \\delta), p_2 \\times \\delta, p_2 \\times (1 - \\delta), 1 - p_2 \\right]^\\intercal\\). The marginal probability of the observations is the product of these probabilities, or the sum of the log probabilities. Note that the detection vectors corresponding to the other states will be 0, because it’s impossible to observe breeding snakes for both unoccupied sites and sites occupied by non-breeders.\nBecause the TPM varies by site and year due to the unequal time intervals tau[i, t], the log TPM of the ecological process gets overwritten in each site- and year-level loop in the model block. Note that in the generated quantities block a different approach is used, where the same yearly TRM is used but lmps is only incremented with observation log probabilities in years that are surveyed for each site.\nAs in Example 1, the log likelihood and Viterbi algorithm are performed in the generated quantities block, but the Viterbi algorithm has been modified for this specific model.\n\n\n\nCode\nfunctions{\n  /**\n   * Return the natural logarithm of the product of the element-wise exponentiation of the specified matrices\n   *\n   * @param a  First matrix or (row_)vector\n   * @param b  Second matrix or (row_)vector\n   *\n   * @return   log(exp(a) * exp(b))\n   */\n  matrix log_product_exp(matrix a, matrix b) {\n    int x = rows(a);\n    int y = cols(b);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    matrix[x, y] c;\n    for (j in 1:y) {\n      for (i in 1:x) {\n        c[i, j] = log_sum_exp(a_tr[:, i] + b[:, j]);\n      }\n    }\n    return c;\n  }\n  vector log_product_exp(matrix a, vector b) {\n    int x = rows(a);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    vector[x] c;\n    for (i in 1:x) {\n      c[i] = log_sum_exp(a_tr[:, i] + b);\n    }\n    return c;\n  }\n  row_vector log_product_exp(row_vector a, matrix b) {\n    int y = cols(b);\n    vector[size(a)] a_tr = a';\n    row_vector[y] c;\n    for (j in 1:y) {\n      c[j] = log_sum_exp(a_tr + b[:, j]);\n    }\n    return c;\n  }\n  real log_product_exp(row_vector a, vector b) {\n    real c = log_sum_exp(a' + b);\n    return c;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; n_site, n_year, n_sec;\n  array[n_site] int&lt;lower=1, upper=n_year&gt; n_year_i;\n  array[n_site, n_year] int&lt;lower=1, upper=n_year&gt; years;\n  array[n_site, n_year] int&lt;lower=1, upper=n_sec&gt; n_sec_it;\n  array[n_site, n_year, n_sec] int&lt;lower=1, upper=n_sec&gt; secs;\n  array[n_site, n_year, n_sec] int&lt;lower=1, upper=3&gt; y;\n}\n\ntransformed data {\n  array[n_site, n_year - 1] int tau;\n  array[n_site, n_year] int&lt;lower=0, upper=1&gt; q = rep_array(0, n_site, n_year);\n  for (i in 1:n_site) {\n    for (t in 2:n_year_i[i]) {\n      tau[i, t - 1] = years[i, t] - years[i, t - 1];\n    }\n    // surveys in that year?\n    for (t in 1:n_year) {\n      for (t_i in years[i, 1:n_year_i[i]]) {\n        if (t == t_i) {\n          q[i, t] = 1;\n        }\n      } // t_i\n    } // t\n  } // i\n}\n\nparameters {\n  simplex[3] eta;\n  vector&lt;lower=0&gt;[2] gamma, epsilon, psi;\n  vector&lt;lower=0, upper=1&gt;[2] p;\n  real&lt;lower=0, upper=1&gt; delta;\n}\n\nmodel {\n  // TRM and log TPM containers\n  matrix[3, 3] trm, ltpm_z, ltpm_y;\n  \n  // pre-compute log initial state vector\n  vector[3] log_eta = log(eta);\n  \n  // ecological TRM (transpose once)\n  trm[1, 1] = -(gamma[1] + gamma[2]);\n  trm[1, 2] = gamma[1];\n  trm[1, 3] = gamma[2];\n  trm[2, 1] = epsilon[1];\n  trm[2, 2] = -(epsilon[1] + psi[1]);\n  trm[2, 3] = psi[1];\n  trm[3, 1] = epsilon[2];\n  trm[3, 2] = psi[2];\n  trm[3, 3] = -(epsilon[2] + psi[2]);\n  trm = trm';\n  \n  // pre-compute log detection probabilities\n  vector[2] log_p = log(p);\n  vector[2] log1m_p = log1m(p);\n  \n  // observation log TPM\n  ltpm_y[1, 1] = 0;\n  ltpm_y[1, 2] = negative_infinity();\n  ltpm_y[1, 3] = negative_infinity();\n  ltpm_y[2, 1] = log1m_p[1];\n  ltpm_y[2, 2] = log_p[1];\n  ltpm_y[2, 3] = negative_infinity();\n  ltpm_y[3, 1] = log1m_p[2];\n  ltpm_y[3, 2] = log_p[2] + log1m(delta);\n  ltpm_y[3, 3] = log_p[2] + log(delta);\n  \n  // marginal log probabilities per state\n  matrix[3, n_year] lmps;\n  int tt;\n  \n  // likelihood\n  for (i in 1:n_site) {\n    \n    // initialise marginal log probabilities with observation model in each year\n    for (t in 1:n_year_i[i]) {\n      tt = years[i, t];  // map to correct year\n      for (s in 1:3) {\n        lmps[s, t] = sum(ltpm_y[s, y[i, tt, secs[i, tt, 1:n_sec_it[i, tt]]]]);\n      } // s\n    } // t\n    \n    // increment initial state log probabilities\n    lmps[:, 1] += log_eta;\n    \n    // for subsequent years\n    for (t in 2:n_year_i[i]) {\n      \n      // compute log TPM from TRM and tau\n      ltpm_z = log(matrix_exp(trm * tau[i, t - 1]));\n      \n      // increment marginal log detection probabilities with ecological process\n      lmps[:, t] += log_product_exp(ltpm_z, lmps[:, t - 1]);\n    }\n    \n    // increment log density\n    target += log_sum_exp(lmps[:, n_year_i[i]]);\n    \n  } // i\n  \n  // priors\n  target += dirichlet_lupdf(eta | ones_vector(3));\n  target += exponential_lupdf(gamma | 1);\n  target += exponential_lupdf(epsilon | 1);\n  target += exponential_lupdf(psi | 1);\n  target += beta_lupdf(p | 1, 1);\n  target += beta_lupdf(delta | 1, 1);\n}\n\ngenerated quantities {\n  array[n_site, n_year] int z;\n  vector[n_site] log_lik;\n  {\n    matrix[3, 3] trm, ltpm_z, ltpm_y;\n    vector[3] log_eta = log(eta);\n    trm[1, 1] = -(gamma[1] + gamma[2]);\n    trm[1, 2] = gamma[1];\n    trm[1, 3] = gamma[2];\n    trm[2, 1] = epsilon[1];\n    trm[2, 2] = -(epsilon[1] + psi[1]);\n    trm[2, 3] = psi[1];\n    trm[3, 1] = epsilon[2];\n    trm[3, 2] = psi[2];\n    trm[3, 3] = -(epsilon[2] + psi[2]);\n    trm = trm';\n    ltpm_z = matrix_exp(trm); // only one needed\n    vector[2] log_p = log(p);\n    vector[2] log1m_p = log1m(p);\n    ltpm_y[1, 1] = 0;\n    ltpm_y[1, 2] = negative_infinity();\n    ltpm_y[1, 3] = negative_infinity();\n    ltpm_y[2, 1] = log1m_p[1];\n    ltpm_y[2, 2] = log_p[1];\n    ltpm_y[2, 3] = negative_infinity();\n    ltpm_y[3, 1] = log1m_p[2];\n    ltpm_y[3, 2] = log_p[2] + log1m(delta);\n    ltpm_y[3, 3] = log_p[2] + log(delta);\n    array[3, n_year] int back_ptr;\n    matrix[3, n_year] lmps, best_lp;\n    real tmp;\n    int tt;\n    \n    for (i in 1:n_site) {\n      \n      // initialise probabilities\n      lmps = rep_matrix(0, 3, n_year);\n      best_lp = rep_matrix(negative_infinity(), 3, n_year);\n      \n      // first year and observation log probabilities\n      lmps[:, 1] = log_eta;\n      for (t in 1:n_year) {\n        if (q[i, t]) {\n          for (s in 1:3) {\n            lmps[s, t] += sum(ltpm_y[s, y[i, t, secs[i, t, 1:n_sec_it[i, t]]]]);\n          } // s\n        }\n      } // t\n      \n      // Viterbi\n      best_lp[:, 1] = lmps[:, 1];\n      for (t in 2:n_year) {\n        for (s_a in 1:3) {  // state of arrival\n          for (s_d in 1:3) { // state of departure\n            if (q[i, t]) {\n              tmp = best_lp[s_d, t - 1] + ltpm_z[s_a, s_d] + lmps[s_a, t];\n            } else {\n              // ignore observation process if not surveyed\n              tmp = best_lp[s_d, t - 1] + ltpm_z[s_a, s_d];\n            }\n            if (tmp &gt; best_lp[s_a, t]) {\n              back_ptr[s_a, t] = s_d;\n              best_lp[s_a, t] = tmp;\n            }\n          } // s_d\n        } // s_a\n        \n        // increment with ecological process for log-lik\n        lmps[:, t] += log_product_exp(ltpm_z, lmps[:, t - 1]);\n      } // t\n      log_lik[i] += log_sum_exp(lmps[:, n_year_i[i]]);\n      \n      // ecological states\n      tmp = max(best_lp[:, n_year]);\n      for (s_a in 1:3) {\n        if (best_lp[s_a, n_year] == tmp) {\n          z[i, n_year] = s_a;\n        }\n      } // s_a\n      for (t in 1:(n_year - 1)) {\n        tt = n_year - t + 1;\n        z[i, tt - 1] = back_ptr[z[i, tt], tt];\n      } // t\n    } // i\n  }\n}\n\n\nWe’ll run Stan and visually summarise the parameter estimates, with the input plotted alongside in green. It looks like we did alright, but there is a lot of uncertainty, largely because sites can move between 3 states between each year. For this reason, dynamic occupancy models are notoriously data-hungry. Often the initial state parameters are particularly difficult to estimate and are sometimes best parameterised as the steady state vector of the TPM. I haven’t quite worked out how to implement this efficiently in Stan.\n\n\nCode\n# data for Stan\nocc_data &lt;- list(n_site = n_site, n_year = n_year, n_sec = n_sec,\n                 n_year_i = n_year_i, years = years, n_sec_it = n_sec_it,\n                 secs = secs, y = y) |&gt; \n  sapply(\\(x) replace(x, is.na(x), 1))\n\n# run HMC\nfit_dyn &lt;- occ_dyn_ms$sample(data = occ_data, refresh = 0, init = 0.1,\n                             chains = n_chains, parallel_chains = n_chains, iter_sampling = n_iter)\n\n\nRunning MCMC with 4 parallel chains...\n\nChain 4 finished in 211.4 seconds.\nChain 2 finished in 211.6 seconds.\nChain 1 finished in 221.6 seconds.\nChain 3 finished in 227.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 217.9 seconds.\nTotal execution time: 227.4 seconds.\n\n\n\n\nResults\n\n\nCode\n# visualise estimates\nfit_dyn$summary(c(\"eta\", \"gamma\", \"epsilon\", \"psi\", \"p\", \"delta\")) |&gt;\n  mutate(truth = c(eta, gamma, epsilon, psi, p, delta),\n         parameter = c(rep(\"eta\", 3), \n                       rep(c(\"gamma\", \"epsilon\", \"psi\", \"p\"), each = 2), \n                       \"delta\") |&gt;\n           factor(levels = c(\"eta\", \"gamma\", \"epsilon\", \"psi\", \"p\", \"delta\")),\n         variable = factor(variable) |&gt; fct_reorder(as.numeric(parameter)),\n         process = c(rep(\"Ecological Process\", 3 + 3 * 2), \n                     rep(\"Observation Process\", 3)) |&gt;\n           factor()) |&gt;\n  ggplot(aes(median, fct_rev(variable))) +\n  facet_wrap(~ process, scales = \"free_y\", ncol = 1) +\n  geom_pointrange(aes(xmin = q5, xmax = q95), position = position_nudge(y = 1/10)) +\n  geom_point(aes(truth), colour = green, position = position_nudge(y = -1/10)) +\n  scale_x_continuous(breaks = seq(0.2, 1, 0.2), expand = c(0, 0)) +\n  scale_y_discrete(labels = ggplot2:::parse_safe) +\n  labs(x = \"Posterior\", y = \"Parameter\")\n\n\n\n\n\nLet’s also check how many of our ecological ecological states were correctly recovered. Since only state 3 (snakes present and breeding) can be directly observed with certainty, we’ll only check the ecological states for those times where no breeding was observed. We certainly didn’t do as well as in the mark-recapture example, which isn’t too surprising with a small number of individuals and the complexity of the model.\n\n\nCode\n# maximum observed state\nz_obs &lt;- apply(y, 1:2, \\(y) ifelse(sum(is.na(y)) == n_sec, \n                                   NA, max(y, na.rm = T)))\n\n# estimated ecological states\nz_est &lt;- fit_dyn$summary(\"z\")$median |&gt;\n  matrix(n_site, n_year)\n\n# proportion of ecological states correctly estimated\nunknowns &lt;- z_obs &lt; 3 | is.na(z_obs)\nsum((z_est == z)[unknowns]) / sum(unknowns)\n\n\n[1] 0.688\n\n\n\n\n\n\n\n\nSteady state distribution\n\n\n\nChatGPT tells me that to get the steady state distribution from a TPM, you perform the eigendecomposition of the TPM and the eigenvector corresponding to the eigenvalue of 1 is the steady state distribution. Stan does have eigendecomposition, so I’m sure there’s a way to parameterise the initial states nicely, but it gets more complicated with time- and/or individual-varying parameters."
  },
  {
    "objectID": "blog/hmm-in-stan.html#example-3-disease-structured-mark-recapture-with-state-misclassification",
    "href": "blog/hmm-in-stan.html#example-3-disease-structured-mark-recapture-with-state-misclassification",
    "title": "A unified approach to ecological modeling in Stan",
    "section": "Example 3: Disease-structured mark-recapture with state misclassification",
    "text": "Example 3: Disease-structured mark-recapture with state misclassification\nThe main reason I dove so deep into HMMs is because during my PhD I ran into a problem with my mark-recapture study on Fleay’s barred frogs (Mixophyes fleayi). For two years I conducted mark-recapture surveys along Gondwana rainforest streams in northern New South Wales, Australia, swabbing every frog I found to test for the presence of the pathogenic amphibian chytrid fungus (Batrachochytrium dendrobatidis, Bd) (Scheele et al. 2019) to hopefully infer something about (1) the effect of Bd on frog mortality rates and (2) the infection dynamics.\n\n\n\nAdult male Fleay’s or silverblue-eyed barred frog (Mixophyes fleayi) from Border Ranges National Park, New South Wales, Australia.\n\n\nThe ecological TRM of such a disease-structured multistate model is pretty straightforward, with three possible states: (1) alive and uninfected, (2) alive and infected, and (3) dead, which is an absorbing state:\n\\[\n\\mathrm{TRM}_z = \\begin{bmatrix}\n  -(\\psi_1 + \\phi_1) & \\psi_1 & \\phi_1 \\\\\n  \\psi_2 & -(\\psi_2 + \\phi_2) & \\phi_2 \\\\\n  0 & 0 & 0\n\\end{bmatrix}\n\\tag{8}\\]\nHere, \\(\\phi_{1:2}\\) are the mortality hazard rates of uninfected and infected frogs, respectively, and \\(\\psi_{1:2}\\) are the rates of gaining and clearing infections, respectively.\n\nNothing is certain\nWhile I was running qPCRs in the lab on some swabs collected during high frequency surveys (1 week intervals) with large numbers of recaptures, I noticed some frogs were apparently frequently clearing and re-gaining infections. For instance, a frog captured on 5 successive weeks may have looked something like this: \\(\\left[ 2, 2, 1, 2, 1 \\right]\\), implying the infection was lost twice and gained once across 4 weeks. This seemed unlikely, and it struck me as more probable that I was simply getting false negatives. At the same time, I couldn’t rule out false positives, either, given that contamination is always a risk. The best way to account for this uncertainty is to model it.\nI realised that if you conduct robust design sampling and collect a swab with each capture, you could essentially model the swabbing process like an occupancy model within a mark-recapture model. That is, imagine surveying frogs \\(i \\in 1:I\\) during \\(t \\in 1:T\\) primary occasions (say every 2 months). During each primary occasion, you conduct multiple secondary surveys \\(k \\in 1:K\\) within a short amount of time where you’re willing to assume the population is closed, that is, no animals dying, coming in, changing their infection state, etc. Every time you capture a frog, including those captured multiple times within a primary, you collect a swab. There are 3 possible observed states (\\(o\\)) per secondary: (1) frog captured with swab Bd–, (2) frog captured with swab Bd+, and (3) frog not captured. The TPM of this observation process for repeat secondaries with false negatives and false positives is as follows, with ecological states in the rows and observed states in the columns:\n\\[\n\\mathrm{TPM}_o = \\begin{bmatrix}\n  p_1 \\times ( 1 - \\lambda_1) & p_2 \\times \\lambda_1 & 1 - p_1 \\\\\n  p_2 \\times (1 - \\delta_1) & p_2 \\times \\delta_1 & 1 - p_2 \\\\\n  0 & 0 & 1\n\\end{bmatrix}\n\\tag{9}\\] Where \\(p_{1:2}\\) are the detection probabilities of uninfected and infected frogs, respectively, \\(\\lambda_1\\) is the probability of getting a Bd+ swab from an uninfected frog (false positive), and \\(\\delta_2\\) is the probability of detecting Bd on a swab from an infected frog (where \\(1 - \\delta_1\\) is the false negative probability). The Bd detection parameters \\(\\delta_1\\) and \\(\\lambda_1\\) are identified if at least some frogs are recaptured multiple times within a primary occasion.\n\n\nDouble-decking the HMM\nThis is already a lot, but it’s really no different from Example 2: a continuous time ecological process, simpler this time because the dead state is absorbing, and a discrete time observation process, albeit with an extra false positive parameter. However, the reality of the data was slightly more complex. It is conventional to run qPCR several times per swab to accurately quantify the DNA load present in a sample. But this is analogous to the swabbing process described above. Presumably, it’s possible to get false negatives and false positives in the qPCR diagnostic process as well. There are thus three diagnostic states (\\(y\\), data): (1) qPCR replicate Bd–, (2) qPCR replicate Bd+, or (3) no qPCR performed, because no frog was captured and thus no swab was collected. Typically, people will just average their measured DNA loads across runs and apply an inclusion criterion, such as that samples are considered infected when 2/3 qPCR replicates return Bd DNA. But we can do better, we can just model it. Here’s the TPM of the diagnostic process, with observed states in the rows and diagnostic states in the columns:\n\\[\n\\mathrm{TPM}_y = \\begin{bmatrix}\n  1 - \\lambda_2 & \\lambda_2 & 0 \\\\\n  1 - \\delta_2 & \\delta_2 & 0 \\\\\n  0 & 0 & 1\n\\end{bmatrix}\n\\tag{10}\\] where \\(\\lambda_2\\) is another false positive parameter and \\(\\delta_2\\) is the detection probability in the qPCR process.\n\n\nAlways try to use all of the data\nAnother element of this model involved incorporating infection intensity into the model. qPCR doesn’t just tell you whether a sample is infected, but it tells you how many DNA copies are present in the sample. So with multiple qPCR runs (\\(x\\)) \\(l \\in 1:L\\) collected from multiple samples (\\(n\\)), we can actually estimate the latent infection intensity on the individual \\(m\\) and samples:\n\\[\n\\begin{aligned}\n  m_{i,t} &\\sim \\mathrm{Lognormal} \\left( \\mu, \\sigma_1 \\right) \\\\\n  n_{i,t,k} &\\sim \\mathrm{Lognormal} \\left( m_{i,t}, \\sigma_2 \\right) \\\\\n  x_{i,t,k,l} &\\sim \\mathrm{Lognormal} \\left( n_{i,t,k}, \\sigma_3 \\right)\n\\end{aligned}\n\\tag{11}\\] where lognormals are used to ensure the infection intensities are positive, \\(\\mu\\) is the log population average and \\(\\sigma_{1:3}\\) are the population standard deviation and the errors of the swabbing and qPCR processes, respectively. We can incorporate the infection intensities to the ecological and observation models by modeling \\(\\phi_2\\), the mortality rates of infected frogs, and \\(\\delta_{1:2}\\), the Bd detection probabilities, as functions of the relevant infection intensities, for example like this:\n\\[\n\\begin{aligned}\n  {\\phi_2}_{i, t} &= \\exp \\left( \\log {\\phi_2}_\\alpha + {\\phi_2}_\\beta \\times m_{i,t} \\right) \\\\\n  {\\delta_1}_{i,t} &= 1 - (1 - r_1)^{m_{i,t}} \\\\\n  {\\delta_2}_{i,t,k} &= 1 - (1 - r_2)^{n_{i,t,k}}\n\\end{aligned}\n\\tag{12}\\] Here, the first equation is a simple GLM of the mortality hazard rate as a function of time-varying individual infection intensity, and the Bd detection probabilities are modeled as a function of \\(r_{1:2}\\), or the probabilities of detecting one log gene copy of DNA on a swab or qPCR replicate.\nDr. Andy Royle and myself published this model in Methods in Ecology and Evolution where we initially implemented the model in NIMBLE (Hollanders and Royle 2022). But of course, I was keen to try to marginalise out the latent ecological and observed states to fit this model in Stan (note that the ecological states are completely unobserved, as there’s no way of knowing whether an infected qPCR run actually came from an infected sample or frog!).\n\n\nStan implementation\nThis model took me a lot of time to get going in Stan, but I got there in the end. I won’t go into detail about explaining everything, but I’ll mention a few key things here:\n\nI implemented this model with threading (within-chain parallelisation), where the work from the different HMC chains gets partitioned across (potentially) all of the cores on your computer. With this particular individual-level likelihood, this should have considerable speed benefits. Basically, I put as much of the model block inside the partial_sum_lpmf() function which then gets called by reduce_sum(). See the Stan User’s Guide for more information. In this example, a lot of things could have been pre-computed instead of within each individual loop, but the current formulation is ready more more individual-level complexity.\nOne thing I got hung up on for a while was how the observation and diagnostic processes should be coded. The trick is to start with the diagnostic process, that is, the lowest level of the hierarchy that is directly conditioned on the data (as we have been doing). For each secondary survey \\(k\\) in primary \\(t\\) that an individual was captured (using the indicator q created in the transformed data block), you sum the diagnostic probability vectors associated with the observed data (diagnostic states) for each possible (partially latent) observed state. Because these probabilities differ for each observed state, we need another object to hold the intermediate quantities, which I’ve called omega. The diagnostic log probability vectors are then (log) matrix multiplied with the observation (log) TPM. If the frog was not captured during that secondary, lmps is simply incremented with the observation TPM probabilities associated with not being detected. From here, lmps is incremented with the ecological process as in the previous examples.\nBecause we only use the rows of the observation and diagnostic TPMs that are associated with alive and observed frogs, respectively, I don’t write out the full TPMs in the code.\nStan has some issue related to evaluating the gradient of 0 * log(0). I don’t know what that means exactly, but I know I was having errors with the model unless I formulated the model in a particular way (I think that’s why I had to subset \\(\\mathrm{TPM}_z\\) (ltpm_z) to not include the dead state, but I’m not sure). Specifically, as in Example 3, I only increment lmps with the alive states until the last primary of capture. After that occasion, the dead state is incremented for the first time. For subsequent occasions, the dead state is incremented separately from the alive states. In the generated quantities for the Viterbi algorithm, I do need the full ltpm_z, as I’m interested in propagating the probabilities of the dead state.\nThe inclusion of false positives means the observation and diagnostic processes are examples of finite mixtures that may have multimodality in certain model constructions (Royle and Link 2006). Here, the true pathogen detection probabilities \\(\\delta\\) are modeled as a function of infection intensity, which may be enough to avoid the multimodality. Just to be sure, however, I’ve constrained \\(\\lambda_{1:2}\\) to be less than \\(r_{1:2}\\) in the program and further given fairly strong \\(\\mathrm{Beta} (1, 10)\\) priors for \\(\\lambda\\).\nThe individual infection intensities m[i, t] are parameterised as centered lognormal distribution and the sample infection intensities n[i, t, k] as non-centered lognormal, as this seemed to give the best HMC performance.\n\n\nSimulation\nBelow I simulate some data where the rates of gaining and clearing infection are affected by temperature (negative and positive, respectively), which seems to be the case for the frog-Bd system. To account for the varying infection dynamics in the state probabilities at first capture, I’ll use the steady state distribution for each primary which is computed using the infection dynamics probabilities, given by \\({\\psi_p}_{1:2} = 1 - \\exp(-\\psi_{1:2})\\). Then, the expected probability of being infected at first capture in each primary is \\(\\eta = \\frac{{\\psi_p}_1}{{\\psi_p}_1 + {\\psi_p}_2}\\).\n\n\nCode\n# metadata\nn_ind &lt;- 100\nn_prim &lt;- 8\nn_sec &lt;- 3\nn_diag &lt;- 3\ntau &lt;- rlnorm(n_prim - 1, log(1), 0.5)             # unequal primary occasion intervals\ntemp &lt;- rnorm(n_prim - 1) |&gt; sort(decreasing = T)  # it's getting colder\n\n# parameters\nphi_a &lt;- c(0.1, 0.1)       # mortality rates of uninfected/infected frogs (intercepts)\nphi_b &lt;- 0.3               # effect of one log Bd gene copy on mortality\npsi_a &lt;- c(0.7, 0.4)       # rates of gaining/clearing infections (intercepts)\npsi_b &lt;- c(-0.4, 0.3)      # effect of temperature on rates of gaining/clearing infections\np_a &lt;- c(0.7, 0.6)         # detection probabilities of uninfected/infected frogs\nr &lt;- c(0.4, 0.6)           # pathogen detection probabilities (per log Bd gene copy)\nlambda &lt;- c(0.05, 0.10)    # sampling/diagnostic false positive probabilities\nmu &lt;- 1.0                  # population log average infection intensity\nsigma &lt;- c(0.3, 0.2, 0.1)  # population and sampling/diagnostic SDs\n\n# TPM containers\ntrm_z &lt;- tpm_z &lt;- array(NA, c(3, 3, n_ind, n_prim - 1))\ntpm_o &lt;- array(NA, c(3, 3, n_ind, n_prim, n_sec))\ntpm_y &lt;- array(NA, c(3, 3, n_ind, n_prim, n_sec))\n\n# parameter containers\nphi &lt;- array(phi_a, c(2, n_ind, n_prim - 1))\npsi &lt;- exp(matrix(log(psi_a), 2, n_prim - 1) + matrix(psi_b, 2, n_prim - 1) * matrix(temp, 2, n_prim - 1, byrow = T))\npsi_p &lt;- 1 - exp(-psi)\neta &lt;- psi_p[1, ] / apply(psi_p, 2, sum)\np &lt;- array(p_a, c(2, n_ind, n_prim, n_sec))\nm &lt;- delta1 &lt;- array(NA, c(n_ind, n_prim))\nn &lt;- delta2 &lt;- array(NA, c(n_ind, n_prim, n_sec))\n\n# primary occasions of first capture\nfirst &lt;- sort(sample(1:(n_prim - 1), n_ind, replace = T))\n\n# TPMs\nfor (i in 1:n_ind) {\n  \n  # fix detection probabilities in first secondary of first primary to ensure capture\n  p[, i, first[i], 1] &lt;- 1\n  \n  for (t in first[i]:n_prim) {\n    \n    # individual infection intensity\n    m[i, t] &lt;- rlnorm(1, mu, sigma[1])\n    \n    # sample pathogen detection\n    delta1[i, t] &lt;- 1 - (1 - r[1]) ^ m[i, t]\n    \n    # sample infection intensities\n    n[i, t, ] &lt;- rlnorm(n_sec, log(m[i, t]), sigma[2])\n    \n    # diagnostic pathogen detection\n    delta2[i, t, ] &lt;- 1 - (1 - r[2]) ^ n[i, t, ]\n    \n    for (k in 1:n_sec) {\n      \n      # observation TPM\n      tpm_o[, , i, t, k] &lt;- matrix(c(p[1, i, t, k] * (1 - lambda[1]), p[1, i, t, k] * lambda[1], 1 - p[1, i, t, k],\n                                     p[2, i, t, k] * (1 - delta1[i, t]), p[2, i, t, k] * delta1[i, t], 1 - p[2, i, t, k], \n                                     0, 0, 1), \n                                   3, byrow = T)\n      \n      # diagnostic TPM\n      tpm_y[, , i, t, k] &lt;- matrix(c(1 - lambda[2], lambda[2], 0, \n                                     1 - delta2[i, t, k], delta2[i, t, k], 0, \n                                     0, 0, 1), \n                                   3, byrow = T)\n\n    } # k\n  } # t\n  \n  for (t in first[i]:(n_prim - 1)) {\n    \n    # mortality rate as a function of infection intensity\n    phi[2, i, t] &lt;- exp(log(phi_a[2]) + phi_b * m[i, t])\n    \n    # ecological TRM/TPM\n    trm_z[, , i, t] &lt;- matrix(c(-(psi[1, t] + phi[1, i, t]), psi[1, t], phi[1, i, t], \n                                psi[2, t], -(psi[2, t] + phi[2, i, t]), phi[2, i, t], \n                                0, 0, 0), \n                              3, byrow = T)\n    tpm_z[, , i, t] &lt;- expm(trm_z[, , i, t] * tau[t])\n    \n  } # t\n} # i\n\n# containers\nz &lt;- array(NA, c(n_ind, n_prim))\no &lt;- array(NA, c(n_ind, n_prim, n_sec))\ny &lt;- x &lt;- array(NA, c(n_ind, n_prim, n_sec, n_diag))\n\n# simulation\nfor (i in 1:n_ind) {\n  \n  # ecological process\n  z[i, first[i]] &lt;- rcat(1, c(1 - eta[first[i]], eta[first[i]]))\n  for (t in (first[i] + 1):n_prim) {\n    z[i, t] &lt;- rcat(1, tpm_z[z[i, t - 1], , i, t - 1])\n  } # t\n  \n  for (t in first[i]:n_prim) {\n    for (k in 1:n_sec) {\n      \n      # observation process\n      o[i, t, k] &lt;- rcat(1, tpm_o[z[i, t], , i, t, k])\n      \n      # diagnostic process\n      y[i, t, k, ] &lt;- rcat(n_diag, tpm_y[o[i, t, k], , i, t, k])\n      \n      # diagnostic run infection intensity\n      for (l in 1:n_diag) {\n        if (y[i, t, k, l] == 2) {\n          x[i, t, k, l] &lt;- rlnorm(1, log(n[i, t, k]), sigma[3])\n        }\n      } # l\n    } # k\n  } # t\n} # i\n\n\n\n\nStan program\nHere’s the Stan program.\n\n\nCode\nfunctions {\n  /**\n   * Return partial sum over individuals applying the forward algorithm for multievent mark-recapture model formulated as hidden Markov model\n   *\n   * @param seq_ind    Sequence of individuals from 1 to n_ind\n   * @param start      Start of seq_ind for partial sum\n   * @param end        End of seq_ind for partial sum\n   * @param y          Detection history\n   * @param q          Logical indicating whether individual was detected in each secondary\n   * @param n_prim     Number of primary occasions\n   * @param n_sec      Number of secondary occasions per primary\n   * @param first      Primary occasion of first capture per individual\n   * @param last       Primary occasion of last capture per individual\n   * @param first_sec  Secondary of first capture in primary of first capture\n   * @param tau        Time intervals between primary occasions\n   * @param temp       Average temperatures between primary occasions\n   * @param phi_a      Mortality rates of uninfected and infected individuals (intercepts)\n   * @param phi_b      Effect of one log gene copy of pathogen on mortality rate\n   * @param psi_a      Rates of gaining and clearing infections (intercepts)\n   * @param psi_b      Effects of temperature on rates of gaining and clearing infections\n   * @param p_a        Detection probabilities of uninfected and infected individuals\n   * @param r          Probabilities of detecting one log gene copy of pathogen on individuals (sampling process) and samples (diagnostic process)\n   * @param lambda     False positive probabilities in sampling and diagnostic processes\n   * @param m          Individual infection intensities per primary\n   * @param n_z        z-scores of sample infection intensities per secondary\n   * @param sigma      Vector of infection intensity SDs\n   *\n   * @return           Log probability for subset of individuals\n   */\n  real partial_sum_lpmf(\n    data array[] int seq_ind, data int start, data int end, data array[,,,] int y, data array[,,] int q, \n    data int n_prim, data int n_sec, data array[] int first, data array[] int last, data array[] int first_sec, data vector tau, data vector temp, \n    vector phi_a, real phi_b, vector psi_a, vector psi_b, vector p_a, vector r, vector lambda, matrix m, array[] matrix n_z, vector sigma) {\n      \n      // number of individuals in partial sum and initialise partial target\n      int n_ind = end - start + 1;\n      real ptarget = 0;\n      \n      // containers\n      array[2] vector[n_prim - 1] phi, psi, psi_p;\n      vector[n_prim - 1] eta, log_eta, log1m_eta;\n      array[2] matrix[n_sec, n_prim] p;\n      matrix[n_ind, n_prim] log_m = log(m[:, start:end])';\n      array[n_ind] matrix[n_sec, n_prim] n;\n      tuple(vector[n_prim], matrix[n_sec, n_prim]) delta;\n      \n      // transform intercepts\n      vector[2] log_phi_a = log(phi_a);\n      vector[2] log_psi_a = log(psi_a);\n      \n      // transition rate/log probability matrices (TRM/LTPM)\n      array[n_prim - 1] matrix[3, 3] trm;\n      array[n_prim - 1] matrix[3, 2] ltpm_z;\n      array[n_prim, n_sec] matrix[2, 3] ltpm_o;\n      array[n_prim, n_sec] matrix[2, 2] ltpm_y;\n      \n      // log probabilities\n      array[n_prim] matrix[2, n_sec] omega;\n      matrix[3, n_prim] lmps;\n      \n      // for each individual\n      for (i in 1:n_ind) {\n        \n        // index that maps to original n_ind\n        int ii = i + start - 1;\n        \n        // ecological process parameters\n        phi[1] = rep_vector(phi_a[1], n_prim - 1);\n        phi[2] = exp(log_phi_a[2] + phi_b * m[1:(n_prim - 1), ii]);\n        for (s in 1:2) {\n          psi[s] = exp(log_psi_a[s] + psi_b[s] * temp);\n          psi_p[s] = 1 - exp(-psi[s]);\n        }\n        eta = psi_p[1] ./ (psi_p[1] + psi_p[2]);\n        log_eta = log(eta);\n        log1m_eta = log1m(eta);\n        \n        // primary occasion intervals\n        for (t in first[ii]:(n_prim - 1)) {\n          \n          // ecological TRM\n          trm[t][1, 1] = -(psi[1][t] + phi[1][t]);\n          trm[t][1, 2] = psi[1][t];\n          trm[t][1, 3] = phi[1][t];\n          trm[t][2, 1] = psi[2][t];\n          trm[t][2, 2] = -(psi[2][t] + phi[2][t]);\n          trm[t][2, 3] = phi[2][t];\n          trm[t][3] = zeros_row_vector(3);\n          trm[t] = trm[t]';\n          \n          // ecological log TPM\n          ltpm_z[t] = log(matrix_exp(trm[t] * tau[t])[:, 1:2]);\n          \n        } // t\n        \n        // sample infection intensities\n        ptarget += std_normal_lupdf(to_vector(n_z[ii]));\n        n[i] = exp(rep_matrix(log_m[i], n_sec) + n_z[ii] * sigma[2]);\n        \n        // individual and sample infection detection probabilities\n        delta.1 = 1 - pow(1 - r[1], m[:, ii]);\n        delta.2 = 1 - pow(1 - r[2], n[i]);\n        \n        // detection probabilities (fixed at 1 for secondary of first capture)\n        for (s in 1:2) {\n          p[s] = rep_matrix(p_a[s], n_sec, n_prim);\n          p[s][first_sec[ii], first[ii]] = 1;\n        } // s\n        \n        // initialise log MPS\n        lmps = rep_matrix(0, 3, n_prim);\n        \n        // for each primary occasion\n        for (t in first[ii]:n_prim) {\n          \n          // for each secondary occasion\n          for (k in 1:n_sec) {\n            \n            // observation log TPM\n            ltpm_o[t, k][1, 1] = log(p[1][k, t]) + log1m(lambda[1]);\n            ltpm_o[t, k][1, 2] = log(p[1][k, t]) + log(lambda[1]);\n            ltpm_o[t, k][1, 3] = log1m(p[1][k, t]);\n            ltpm_o[t, k][2, 1] = log(p[2][k, t]) + log1m(delta.1[t]);\n            ltpm_o[t, k][2, 2] = log(p[2][k, t]) + log(delta.1[t]);\n            ltpm_o[t, k][2, 3] = log1m(p[2][k, t]);\n            \n            // diagnostic log TPM\n            ltpm_y[t, k][1, 1] = log1m(lambda[2]);\n            ltpm_y[t, k][1, 2] = log(lambda[2]);\n            ltpm_y[t, k][2, 1] = log1m(delta.2[k, t]);\n            ltpm_y[t, k][2, 2] = log(delta.2[k, t]);\n            \n            // if captured during secondary\n            if (q[ii, t, k]) {\n              \n              // diagnostic probabilities (y[ii, t, k] is an array of n_diag integers)\n              for (o in 1:2) {\n                omega[t][o, k] = sum(ltpm_y[t, k][o, y[ii, t, k]]);\n              }\n              \n              // accrue log marginal probabilities with observation process\n              lmps[1:2, t] += log_product_exp(ltpm_o[t, k][:, 1:2], omega[t][:, k]);\n              \n              // if not captured, observation process only\n            } else {\n              lmps[1:2, t] += ltpm_o[t, k][:, 3];\n            }\n          } // k\n        } // t\n        \n        // marginal log probabilities of alive states at first capture\n        lmps[1:2, first[ii]] += [ log1m_eta[first[i]], log_eta[first[i]] ]';\n        \n        // marginal log probabilities of alive states between first and last primary with captures\n        if (first[ii] &lt; last[ii]) {\n          for (t in (first[ii] + 1):last[ii]) {\n            lmps[1:2, t] += log_product_exp(ltpm_z[t - 1][1:2], lmps[1:2, t - 1]);\n          } // t\n        }\n        \n        // if last captured in the last primary\n        if (last[ii] == n_prim) {\n          \n          // increment target with marginal log probabilities of alive states\n          ptarget += log_sum_exp(lmps[1:2, n_prim]);\n        } else {\n          \n          // marginal log probabilities of all ecological states in primary after last capture\n          lmps[1:2, last[ii] + 1] += log_product_exp(ltpm_z[last[ii]][1:2], lmps[1:2, last[ii]]);\n          lmps[3, last[ii] + 1] = log_product_exp(ltpm_z[last[ii]][3], lmps[1:2, last[ii]]);\n          \n          // if there are more primaries\n          if ((last[ii] + 1) &lt; n_prim) {\n            \n            // marginal log probabilities of all ecological states until last primary\n            for (t in (last[ii] + 2):n_prim) {\n              lmps[1:2, t] += log_product_exp(ltpm_z[t - 1][1:2], lmps[1:2, t - 1]);\n              lmps[3, t] = log_sum_exp(log_product_exp(ltpm_z[t - 1][3], lmps[1:2, t - 1]), lmps[3, t - 1]);\n            } // t\n          }\n          \n          // increment target with marginal log probabilities of all ecological states\n          ptarget += log_sum_exp(lmps[:, n_prim]);\n        }\n      } // i\n      \n      return ptarget;\n    }\n    \n  /**\n   * Return the natural logarithm of the product of the element-wise exponentiation of the specified matrices\n   *\n   * @param a  First matrix or (row_)vector\n   * @param b  Second matrix or (row_)vector\n   *\n   * @return   log(exp(a) * exp(b))\n   */\n  matrix log_product_exp(matrix a, matrix b) {\n    int x = rows(a);\n    int y = cols(b);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    matrix[x, y] c;\n    for (j in 1:y) {\n      for (i in 1:x) {\n        c[i, j] = log_sum_exp(a_tr[:, i] + b[:, j]);\n      }\n    }\n    return c;\n  }\n  vector log_product_exp(matrix a, vector b) {\n    int x = rows(a);\n    int z = cols(a);\n    matrix[z, x] a_tr = a';\n    vector[x] c;\n    for (i in 1:x) {\n      c[i] = log_sum_exp(a_tr[:, i] + b);\n    }\n    return c;\n  }\n  row_vector log_product_exp(row_vector a, matrix b) {\n    int y = cols(b);\n    vector[size(a)] a_tr = a';\n    row_vector[y] c;\n    for (j in 1:y) {\n      c[j] = log_sum_exp(a_tr + b[:, j]);\n    }\n    return c;\n  }\n  real log_product_exp(row_vector a, vector b) {\n    real c = log_sum_exp(a' + b);\n    return c;\n  }\n}\n\ndata {\n  int grainsize, n_ind, n_prim, n_sec, n_diag, n_x;\n  array[n_ind] int&lt;lower=1, upper=n_prim&gt; first, last, first_sec;\n  vector&lt;lower=0&gt;[n_prim - 1] tau;\n  vector[n_prim - 1] temp;\n  array[n_ind, n_prim, n_sec, n_diag] int&lt;lower=1, upper=3&gt; y;\n  array[n_x] int&lt;lower=1&gt; ind, prim, sec;\n  vector[n_x] x;\n}\n\ntransformed data {\n  array[n_ind] int seq_ind = linspaced_int_array(n_ind, 1, n_ind);\n  array[n_ind, n_prim, n_sec] int&lt;lower=0, upper=1&gt; q = rep_array(0, n_ind, n_prim, n_sec);\n  for (i in 1:n_ind) {\n    for (t in first[i]:n_prim) {\n      for (k in 1:n_sec) {\n        if (min(y[i, t, k]) &lt; 3) {\n          q[i, t, k] = 1;\n        }\n      }\n    }\n  }\n}\n\nparameters {\n  vector&lt;lower=0&gt;[2] phi_a, psi_a;\n  real phi_b;\n  vector[2] psi_b;\n  vector&lt;lower=0, upper=1&gt;[2] p_a, r;\n  vector&lt;lower=0, upper=r&gt;[2] lambda;\n  real mu;\n  matrix&lt;lower=0&gt;[n_prim, n_ind] m;\n  array[n_ind] matrix[n_sec, n_prim] n_z;\n  vector&lt;lower=0&gt;[3] sigma;            \n}\n\nmodel {\n  // log sample infection intensities\n  array[n_ind] matrix[n_sec, n_prim] log_n;\n  matrix[n_ind, n_prim] log_m = log(m)';\n  for (i in 1:n_ind) {\n    log_n[i] = rep_matrix(log_m[i], n_sec) + n_z[i] * sigma[2];\n  }\n  \n  // likelihood of diagnostic infection intensities\n  for (j in 1:n_x) {\n    target += lognormal_lupdf(x[j] | log_n[ind[j]][sec[j], prim[j]], sigma[3]);\n  }\n  \n  // likelihood of CMR\n  target += reduce_sum(partial_sum_lupmf, seq_ind, grainsize, y, q, \n                       n_prim, n_sec, first, last, first_sec, tau, temp, \n                       phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, m, n_z, sigma);\n                       \n  // priors\n  target += exponential_lupdf(phi_a | 1);\n  target += std_normal_lupdf(phi_b);\n  target += exponential_lupdf(psi_a | 1);\n  target += std_normal_lupdf(psi_b);\n  target += beta_lupdf(p_a | 1, 1);\n  target += beta_lupdf(r | 1, 1);\n  target += beta_lupdf(lambda | 1, 10);\n  target += normal_lupdf(mu | 0, 1);\n  target += exponential_lupdf(sigma | 1);\n  target += lognormal_lupdf(to_vector(m) | mu, sigma[1]);\n}\n\ngenerated quantities {\n  // containers\n  array[n_ind, n_prim] int z;\n  vector[n_ind] log_lik;\n  {\n    array[2] vector[n_prim - 1] phi, psi, psi_p;\n    vector[n_prim - 1] eta, log_eta, log1m_eta;\n    array[2] matrix[n_sec, n_prim] p;\n    matrix[n_ind, n_prim] log_m = log(m)';\n    array[n_ind] matrix[n_sec, n_prim] n;\n    tuple(vector[n_prim], matrix[n_sec, n_prim]) delta;\n    vector[2] log_phi_a = log(phi_a);\n    vector[2] log_psi_a = log(psi_a);\n    array[n_prim - 1] matrix[3, 3] trm;\n    array[n_prim - 1] matrix[3, 3] ltpm_z;\n    array[n_prim, n_sec] matrix[2, 3] ltpm_o;\n    array[n_prim, n_sec] matrix[2, 2] ltpm_y;\n    array[n_prim] matrix[2, n_sec] omega;\n    array[3, n_prim] int back_ptr;\n    matrix[3, n_prim] lmps, best_lp;\n    real tmp;\n    int tt;\n    \n    for (i in 1:n_ind) {\n    \n      phi[1] = rep_vector(phi_a[1], n_prim - 1);\n      phi[2] = exp(log_phi_a[2] + phi_b * m[1:(n_prim - 1), i]);\n      for (s in 1:2) {\n        psi[s] = exp(log_psi_a[s] + psi_b[s] * temp);\n        psi_p[s] = 1 - exp(-psi[s]);\n      }\n      eta = psi_p[1] ./ (psi_p[1] + psi_p[2]);\n      log_eta = log(eta);\n      log1m_eta = log1m(eta);\n      for (t in first[i]:(n_prim - 1)) {\n        trm[t][1, 1] = -(psi[1][t] + phi[1][t]);\n        trm[t][1, 2] = psi[1][t];\n        trm[t][1, 3] = phi[1][t];\n        trm[t][2, 1] = psi[2][t];\n        trm[t][2, 2] = -(psi[2][t] + phi[2][t]);\n        trm[t][2, 3] = phi[2][t];\n        trm[t][3] = zeros_row_vector(3);\n        trm[t] = trm[t]';\n        // dead state remains for Viterbi algorithm\n        ltpm_z[t] = log(matrix_exp(trm[t] * tau[t]));\n      } // t\n      n[i] = exp(rep_matrix(log_m[i], n_sec) + n_z[i] * sigma[2]);\n      delta.1 = 1 - pow(1 - r[1], m[:, i]);\n      delta.2 = 1 - pow(1 - r[2], n[i]);\n      for (s in 1:2) {\n        p[s] = rep_matrix(p_a[s], n_sec, n_prim);\n        p[s][first_sec[i], first[i]] = 1;\n      } // s\n      lmps = rep_matrix(0, 3, n_prim);\n      best_lp = rep_matrix(negative_infinity(), 3, n_prim);\n      \n      for (t in first[i]:n_prim) {\n        for (k in 1:n_sec) {\n          ltpm_o[t, k][1, 1] = log(p[1][k, t]) + log1m(lambda[1]);\n          ltpm_o[t, k][1, 2] = log(p[1][k, t]) + log(lambda[1]);\n          ltpm_o[t, k][1, 3] = log1m(p[1][k, t]);\n          ltpm_o[t, k][2, 1] = log(p[2][k, t]) + log1m(delta.1[t]);\n          ltpm_o[t, k][2, 2] = log(p[2][k, t]) + log(delta.1[t]);\n          ltpm_o[t, k][2, 3] = log1m(p[2][k, t]);\n          ltpm_y[t, k][1, 1] = log1m(lambda[2]);\n          ltpm_y[t, k][1, 2] = log(lambda[2]);\n          ltpm_y[t, k][2, 1] = log1m(delta.2[k, t]);\n          ltpm_y[t, k][2, 2] = log(delta.2[k, t]);\n          // observation log probabilities\n          if (q[i, t, k]) {\n            for (o in 1:2) {\n              omega[t][o, k] = sum(ltpm_y[t, k][o, y[i, t, k]]);\n            }\n            lmps[1:2, t] += log_product_exp(ltpm_o[t, k][:, 1:2], omega[t][:, k]);\n          } else {\n            lmps[1:2, t] += ltpm_o[t, k][:, 3];\n          }\n        } // k\n      } // t\n      \n      // first primary\n      lmps[1:2, first[i]] = [ log1m_eta[first[i]], log_eta[first[i]] ]' + lmps[1:2, first[i]];\n      best_lp[1:2, first[i]] = lmps[1:2, first[i]];\n      \n      // Viterbi\n      if (first[i] &lt; last[i]) {\n        for (t in (first[i] + 1):last[i]) {\n          for (s_a in 1:2) {  // state of arrival\n            for (s_d in 1:2) {  // state of departure\n              tmp = best_lp[s_d, t - 1] + ltpm_z[t - 1][s_a, s_d] + lmps[s_a, t];\n              if (tmp &gt; best_lp[s_a, t]) {\n                back_ptr[s_a, t] = s_d;\n                best_lp[s_a, t] = tmp;\n              }\n            } // s_d\n          } // s_a\n          // increment with ecological process for log-lik\n          lmps[1:2, t] += log_product_exp(ltpm_z[t - 1][1:2, 1:2], lmps[1:2, t - 1]);\n        } // t\n      }\n      if (last[i] &lt; n_prim) {\n        for (t in (last[i] + 1):n_prim) {\n          for (s_a in 1:3) {  // state of arrival\n            for (s_d in 1:3) {  // state of departure\n              tmp = best_lp[s_d, t - 1] + ltpm_z[t - 1][s_a, s_d] + lmps[s_a, t];\n              if (tmp &gt; best_lp[s_a, t]) {\n                back_ptr[s_a, t] = s_d;\n                best_lp[s_a, t] = tmp;\n              }\n            } // s_d\n          } // s_a\n          // increment with ecological process for log-lik\n          lmps[:, t] += log_product_exp(ltpm_z[t - 1], lmps[:, t - 1]);\n        } // t\n        log_lik[i] = log_sum_exp(lmps[:, n_prim]);\n      }\n      \n      // ecological states\n      tmp = max(best_lp[:, n_prim]);\n      for (s_a in 1:3) { // state of arrival\n        if (best_lp[s_a, n_prim] == tmp) {\n          z[i, n_prim] = s_a;\n        }\n      } // s_a\n      for (t in first[i]:(n_prim - 1)) {\n        tt = n_prim - t + first[i];\n        z[i, tt - 1] = back_ptr[z[i, tt], tt];\n      } // t\n    } // i\n  }\n}\n\n\nNext I prepare the data for Stan and run HMC with threading enabled for more computational power. On this M2 MacBook Pro I have 10 cores, meaning if I just use 2 HMC chains I can spread the work across 5 cores each. The grainsize arguments recommends how to partition the work in the partial sum function across cores.\n\n\nCode\n# convert diagnostic infection intensities to long format\nx_long &lt;- reshape2::melt(x, value.name = \"x\", varnames = c(\"ind\", \"prim\", \"sec\", \"diag\")) |&gt; \n  as_tibble() |&gt;\n  drop_na() |&gt;\n  arrange(ind)\n\n# data for Stan\nn_chains &lt;- 2\nn_cores &lt;- parallel::detectCores()\nn_threads &lt;- floor(n_cores / n_chains)\nfirst_sec &lt;- rep(1, n_ind)  # simulated to be first captured in first secondary\nlast &lt;- apply(y, 1:2, min) |&gt; apply(1, \\(y) max(which(y &lt; 3)))\ngoat_data &lt;- list(grainsize = floor(n_ind / n_threads / 4),\n                  n_ind = n_ind, n_prim = n_prim, n_sec = n_sec, n_diag = n_diag,\n                  first = first, last = last, first_sec = first_sec, tau = tau, temp = temp,\n                  y = ifelse(is.na(y), 3, y),\n                  n_x = nrow(x_long), ind = x_long$ind, prim = x_long$prim, sec = x_long$sec,\n                  x = x_long$x)\n\n# run HMC\nfit_goat &lt;- hmm_goat$sample(data = goat_data, refresh = 0, init = 0.1, \n                            threads_per_chain = n_threads, chains = n_chains, parallel_chains = n_chains, iter_sampling = n_iter)\n\n\nRunning MCMC with 2 parallel chains...\n\nChain 2 finished in 712.9 seconds.\nChain 1 finished in 719.9 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 716.4 seconds.\nTotal execution time: 720.1 seconds.\n\n\n\n\n\nResults\nWe’ll visually check the parameter estimates with the simulation input and see that it was recovered well. For what it’s worth, in the paper we showed that the infection dynamics get overestimated significantly (5-fold in our application) when not accounting for state misclassification.\n\n\nCode\n# check estimates\nfit_goat$summary(c(\"phi_a\", \"phi_b\", \"psi_a\", \"psi_b\", \"p_a\", \"r\", \"lambda\", \"mu\", \"sigma\")) |&gt;\n  mutate(truth = c(phi_a, phi_b, psi_a, psi_b, p_a, r, lambda, mu, sigma),\n         parameter = c(rep(\"phi\", 3), \n                       rep(c(\"psi\", \"psi\", \"p\", \"r\", \"lambda\"), each = 2), \n                       \"mu\", rep(\"sigma\", 3)) |&gt;\n           factor(levels = c(\"phi\", \"psi\", \"p\", \"r\", \"lambda\", \"mu\", \"sigma\")),\n         variable = factor(variable) |&gt;\n           str_remove(c(\"_a\")) |&gt; \n           str_replace(\"phi\\\\[2\\\\]\", \"phi\\\\[2\\\\[alpha\\\\]\\\\]\") |&gt;\n           str_replace(\"phi_b\", \"phi\\\\[2\\\\[beta\\\\]\\\\]\") |&gt;\n           str_replace(\"psi\\\\[1\\\\]\", \"psi\\\\[1\\\\[alpha\\\\]\\\\]\") |&gt;\n           str_replace(\"psi\\\\[2\\\\]\", \"psi\\\\[2\\\\[alpha\\\\]\\\\]\") |&gt;\n           str_replace(\"psi_b\\\\[1\\\\]\", \"psi\\\\[1\\\\[beta\\\\]\\\\]\") |&gt;\n           str_replace(\"psi_b\\\\[2\\\\]\", \"psi\\\\[2\\\\[beta\\\\]\\\\]\") |&gt;\n           fct_reorder(as.numeric(parameter)),\n         process = c(rep(\"Ecological Process\", 3 + 2 + 2),\n                     rep(\"Observation and Diagnostic Processes\", 2 * 3),\n                     rep(\"Infection Intensities\", 1 + 3)) |&gt;\n           factor(levels = c(\"Ecological Process\", \"Observation and Diagnostic Processes\", \"Infection Intensities\"))) |&gt;\n  ggplot(aes(median, fct_rev(variable))) +\n  facet_wrap(~ process, scales = \"free_y\", ncol = 1) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", colour = \"#333333\")  +\n  geom_pointrange(aes(xmin = q5, xmax = q95), position = position_nudge(y = 1/10)) +\n  geom_point(aes(truth), colour = green, position = position_nudge(y = -1/10)) +\n  scale_x_continuous(breaks = seq(-1, 1, 0.5), expand = c(0, 0)) +\n  scale_y_discrete(labels = ggplot2:::parse_safe) +\n  labs(x = \"Posterior\", y = \"Parameter\")\n\n\n\n\n\nAgain, we might check to see how well we recovered our latent ecological states, which were completely unobservable from the data.\n\n\nCode\nz_est &lt;- fit_goat$summary(\"z\")$median |&gt;\n  matrix(n_ind, n_prim) |&gt;\n  {\\(z) ifelse(z &lt; 1, NA, z)}()\nsum(z_est == z, na.rm = T) / sum(!is.na(z))\n\n\n[1] 0.94\n\n\nOne of the main reasons to account for imperfect pathogen detection is to get more realistic estimates of infection prevalence in the population. We can use the full posterior distribution of the latent ecological states from the Viterbi algorithm to estimate the infection prevalence per primary occasion, which here was simulated to be increasing due to dropping temperatures going into austral winter plotted for some fictional dates to show the unequal primary occasion intervals.\n\n\nCode\n# dates\nstart &lt;- ymd(\"2020-12-01\")\ntime_unit &lt;- 21\ndates &lt;- seq.Date(start, start + days(1 + round(sum(tau) * time_unit)), by = 1)\nprim &lt;- dates[c(1, sapply(1:(n_prim - 1), \\(t) round(sum(tau[1:t] * time_unit))))]\n\n# plot\nfit_goat$draws(\"z\", format = \"draws_df\") |&gt;\n  pivot_longer(contains(\"z\"), values_to = \"z\") |&gt;\n  mutate(z = if_else(z &lt; 1, NA, z),\n         ind = rep(1:n_ind, n() / n_ind) |&gt; factor(),\n         prim = rep(prim, each = n_ind) |&gt; rep(n() / n_ind / n_prim)) |&gt;\n  mutate(alive = sum(z == 1 | z == 2, na.rm = T),\n         infected = sum(z == 2, na.rm = T),\n         prevalence = infected / alive,\n         .by = c(.draw, prim)) |&gt;\n  summarise(mean = mean(prevalence),\n            q5 = quantile(prevalence, 0.05),\n            q95 = quantile(prevalence, 0.95),\n            .by = prim) |&gt;\n  ggplot(aes(prim, mean)) +\n  geom_pointrange(aes(ymin = q5, ymax = q95), position = position_nudge(x = 2)) +\n  geom_point(aes(y = infected / alive),\n             data = tibble(prim = prim,\n                           alive = apply(z, 2, \\(z) sum(z == 1 | z == 2, na.rm = T)),\n                           infected = apply(z, 2, \\(z) sum(z == 2, na.rm = T))),\n             colour = green, position = position_nudge(x = -2)) +\n  scale_y_continuous(breaks = seq(0.2, 1, 0.2), limits = c(0, 1.001), expand = c(0, 0)) +\n  labs(x = \"Primary Occasion\", y = \"Infection Prevalence\")\n\n\n\n\n\nThanks for reading if you made it this far. I hope this guide will be useful for ecologists (and others) wanting to transition to gradient-based sampling methods for complex ecological models. I’m sure I’ve some errors somewhere, so if someone finds any or has any questions, please don’t hesitate to reach out."
  },
  {
    "objectID": "portfolio/dmso.html",
    "href": "portfolio/dmso.html",
    "title": "Controlling for DMSO",
    "section": "",
    "text": "Kate Summer et al. recently published an open access manuscript in Biofilm, for which Dr. Matthijs Hollanders from Quantecol conducted the statistical analysis.\nThe authors make a strong case for controlling for the effects of the solvent dimethyl-sulfoxide (DMSO) in biofilm studies. In addition to a literature review of 76 published studies, Summer conducted experiments on the bacteria Streptococcus pneumoniae and Pseudomonas aeruginosa to determine to what extent DMSO affected biofilm formation. We analysed the microbial activity and biofilm inhibition in these experiments using hormetic dose-response models."
  },
  {
    "objectID": "portfolio/subtropical-cats.html",
    "href": "portfolio/subtropical-cats.html",
    "title": "Estimating feral cat densities in subtropical rainforest",
    "section": "",
    "text": "Dr. Darren McHugh led an extensive field project employing camera traps in Border Ranges National Park—a large World Heritage listed subtropical rainforest reserve in northeast New South Wales, Australia—to estimate the density of feral cats (Felis catus). Feral cats are a leading cause of species decline throughout Australia, particularly for mammals which in Australia experience the highest extinction rates globally. Unfortunately, the research revealed that cat densities were more than triple the national average, a surprising result in light of popular beliefs about rainforest cat populations.\nDr. Matthijs Hollanders from Quantecol worked alongside Dr. Ben Augustine to fit his state-of-the-art spatial mark-recapture model that incorporates detections that could not be identified to individuals. Since the majority of cats detected on cameras could not be individually assigned due to their similar pelage types, being able to still incorporate these observations into the analysis increases the statistical power and yields improved estimates of abundance.\nThe work was published today as an Open Access article in Wildlife Research and presents the most thorough assessment of cat densities in Australian rainforests to date."
  }
]